<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【深層強化学習 | PyTorch】走る作曲家のAIカフェ</title>
    <link rel="stylesheet" href="style.css"> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1050827580219099"
     crossorigin="anonymous"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
            <h2>目次</h2>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#nn-q">NN & Q-Learning</a></li>
                <li><a href="#dqn">DQN</a></li>
                <li><a href="#pgm">Policy Gradient Method</a></li>
            </ul>
        </section>
        <section id="overview">
            <h2>Overview</h2>
            深層強化学習とは、強化学習と深層学習を組み合わせた手法です。ニューラルネットワークを使用して、エージェントが環境から得られる観測値をもとに価値関数や方策を近似します。<br>
            このページでは、深層強化学習について基礎から学んでいきます。
        </section>
        <section id="nn-q">
          <h2>NN & Q-Learning</h2>
            <p>「3×4のグリッドワールド」程度の問題なら、Q関数をテーブルとして保持できる。しかし、現実はより複雑で、状態も行動も膨大になる可能性がある。テーブルとして保持するのが難しくなるほか、莫大な数をすべて経験することも現実的ではない。
            この問題を解決するために、Q関数をコンパクトな関数で近似することを考える。そのための最も有力な手法が、ディープラーニングである。</p>
            <p>ニューラルネットワークの代表的な構造は2通り考えられる。1つ目は、状態と行動の2つを入力とするネットワークである。この場合、出力はQ関数の値を1つだけ出力する。
            もう1つの構造は、状態だけを入力として、行動の候補の数の分だけQ関数の値を出力するネットワークである。
            ただし、1つ目のネットワーク構造には計算コストの点で問題がある。具体的には、ある状態においてQ関数の最大値を求める計算コスト（\(\max_a Q(s, a)\)の計算コスト）が大きくなる。よって、2つ目のネットワーク構造を用いる。</p>
            <p>Q学習では次の式によってQ関数を更新する。
            <div class="scroll">
                \begin{align}
                Q'(S_t, A_t) = Q(S_t, A_t) + \alpha \{R_t + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)\}
                \end{align}
            </div>
            この式によって、\(Q(S_t, A_t)\)の値は、ターゲットである\(R_t + \gamma \max_a Q(S_{t+1}, a)\)の方向へと更新される。このとき、\(\alpha\)によってどれだけターゲットの方向へ進むかが調整される。
            ここで、ターゲットである\(R_t + \gamma \max_a Q(S_{t+1}, a)\)を\(T\)で表すことにする。そうすると、上式は以下のように表される。
            <div class="scroll">
                \begin{align}
                Q'(S_t, A_t) = Q(S_t, A_t) + \alpha \{T - Q(S_t, A_t)\}
                \end{align}
            </div>
            この式は、入力が\(S_t, A_t\)のとき出力が\(T\)になるようにQ関数を更新すると解釈できる。これをニューラルネットワークの文脈に当てはめると、入力が\(S_t, A_t\)で、出力が\(T\)となるように学習させることと同じである。
            つまり、\(T\)は正解ラベルとみなせる。また、\(T\)はスカラ値なので、これは回帰問題と考えることができる。</p>
        </section>
        <section id="dqn">
            <h2>DQN</h2>
            <p>Q学習では、推定値を使って推定値を更新する（ブートストラッピング）。まだ正確ではない推定値を使って、今ある推定値を更新するため、Q学習は不安定になりやすい性質がある。
            そこにニューラルネットワークのような表現能力の高い関数近似手法が加わると、結果はさらに不安定になる。ニューラルネットワークの学習を安定させるために、経験再生とターゲットネットワークという技術が使われている。</p>
            <h3>経験再生</h3>
            <p>教師あり学習を復習する。ここでは、MNISTを例に扱う。MNISTは主に、クラス分類の問題に使われ、データセットの中身は、画像データと正解ラベルがペアで与えられる。MNISTを使ってニューラルネットワークで学習する流れは次の通りである。
            <ol>
                <li>訓練用のデータセットから一部のデータをランダムに取り出す（この取り出したデータはミニバッチと呼ぶ）。</li>
                <li>そのミニバッチを使ってニューラルネットワークのパラメータを更新する。</li>
            </ol>
            ここで、ミニバッチを作るときには、データに偏りがないように気をつける必要がある。ニューラルネットワークの学習では、データの偏りを防ぐために、データセットからランダムに取り出すのが一般的である。</p>
            <p>次に、Q学習である。Q学習では、エージェントが環境に対して行動するたびにデータが生成される。具体的には、ある時間\(t\)において得られた\(E_t = (S_t, A_t, R_t, S_{t+1})\)を使ってQ関数を更新する。
            ここでは\(E_t\)を「経験データ」と呼ぶ。この経験データは時間\(t\)が進むに従い得られるが、経験データ間には強い相関がある。つまりQ学習では、相関の強い（偏りのある）データを使って学習を行なっていることになる。
            これが教師あり学習とQ学習の1つ目の違いである。この違いを埋めるテクニックに経験再生がある。まずエージェントが経験したデータ\(E_t = (S_t, A_t, R_t, S_{t+1})\)を一度「バッファ」に保存する（バッファとは、一時的にデータを蓄えておく記憶装置のこと）。
            そして、Q関数を更新する際には、そのバッファから経験データをランダムに取り出して使う。経験再生によって、経験データ間の相関が弱まり、偏りの少ないデータが得られる。さらに、経験データを繰り返し使うことができるため、データ効率が良くなる。
            経験再生は、Q学習に限らず、他の強化学習のアルゴリズムでも使うことができるが、経験再生を使えるのは方策オフ型のアルゴリズムに限られる。方策オン型は、現時点の方策から得たデータだけしか使えないため（過去に集めた経験データを使えないため）、経験再生を使うことはできない。</p>
            <h3>ターゲットネットワーク</h3>
            <p>教師あり学習では、学習データに正解ラベルが付与される。このとき、入力に対する正解ラベルは不変である。Q学習では、\(Q(S_t, A_t)\)の値が\(R_t + \gamma \max_a Q(S_{t + 1}, a)\)（TDターゲット）となるようにQ関数を更新する。
            TDターゲットは、教師あり学習における正解ラベルに相当する。しかしTDターゲットの値は、Q関数が更新されると変動する。ここが教師あり学習とQ学習の違いである。この違いを埋めるために、ターゲットネットワークという、TDターゲットを固定するテクニックを使う。
            まず、Q関数を表すオリジナルのネットワーク（これをqnetと呼ぶ）を用意する。そして、それとは別に、もう一つ同じ構造のネットワーク（これをqnet_targetと呼ぶ）を用意する。qnetは通常のQ学習によって更新を行う。
            一方、qnet_targetは定期的にqnetの重みと同期するようにして、それ以外の重みパラメータを固定したままにする。あとは、qnet_targetを使ってTDターゲットの値を計算すれば、教師ラベルであるTDターゲットの変動が抑えられる。
            これによって教師ラベルであるTDターゲットが（常には）変動しないので、ニューラルネットワークの学習が安定することが期待できる。つまり、ターゲットネットワークはTDターゲットの値を固定するためのテクニックである。
            ただし、TDターゲットが全く更新されないとQ関数の学習は進まないため、間をあけて定期的に（例えば100エピソードごとに）ターゲットネットワークの更新を行うようにしている。</p>
            <h3>POMDP</h3>
            <p>Atariの「Pong」では、MDPを満たさない。なぜなら、ゲーム画面の画像1枚だけを見ても、ボールがどの方向に進んでいるかがわからないからである。もちろん、ボールの進行方向が不明の状態では最適な行動は行えない。このような問題はPOMDP（部分観測マルコフ決定過程）という。
            「Pong」のようなテレビゲームの場合、POMDPをMDPに変換することは簡単である。その方法は、連続するフレームを使うことである。DQNの論文では、4フレームの連続する画像を重ね合わせ、それを１つの「状態」として扱う。
            連続する画像を使うことで、状態の遷移が分かる。これで今まで通りのMDPとして扱うことができる。POMDPでは、現在の観測だけでは不十分であるため、過去の観測も考慮して行動を決める必要がある。POMDPで有力な手法は、RNNを使った方法である。RNNを用いると、過去に入力されたデータを引き継いで計算することができる。</p>
            <h3>その他の工夫</h3>
            <b>εの調整</b>
            <p>強化学習では、活用と探索のバランスが重要である。エージェントの経験が増えるにしたがって価値関数の信頼性が上がることを考えると、エピソード数に比例して探索の割合を減らすことは理にかなっている。
            つまり、初期の段階ではエージェントに多く探索を行わせ、学習が進むにつれて探索を減らす（活用を増やす）ということである。ε-greedy法でこのアイデアを実現するには、エージェントが行動を重ねるにしたがってεを減らすことが考えられる。
            DQNの論文では、最初の100万ステップはεを1.0から0.1まで線形に減少させ、それ以降はε=0.1で固定するという方式が採用されている。</p>
            <b>報酬のクリッピング</b>
            <p>DQNの論文では、報酬を-1.0から1.0の範囲に収まるように調整し、報酬のスケールをそろえることで学習を円滑化している。</p>
            <h3>Double DQN</h3>
            <p>DQNでは、「ターゲットネットワーク」というテクニックが使われる。これは、メインとなるネットワークの他に、パラメータが異なるネットワーク（＝ターゲットネットワーク）を使う手法である。
            ここでは２つのネットワークのパラメータをそれぞれ\(\theta\)と\(\theta '\)で表すことにして、その２つのネットワークによって表現されるQ関数を\(Q_{\theta} (s,a)\)、\(Q_{\theta '} (s,a)\)で表すことにする。
            このとき、Q関数の更新で用いるターゲットは次の式で表される。
            <div class="scroll">
                \begin{align}
                R_t + \gamma \max_a Q_{\theta '} (S_{t+1}, a)
                \end{align}
            </div>
            DQNでは、\(Q_{\theta} (s,a)\)の値を上式の値（TDターゲット）に近づけるように学習する。ここで問題になるのが、\(\max_a Q_{\theta '} (S_{t+1}, a)\)である。
            具体的には、誤差が含まれる推定値\(Q_{\theta '}\)に対してmax演算子を使うと、真のQ関数を使って計算する場合に比べて過大に評価されてしまう。この問題を解決したのがDouble DQNである。Double DQNでは、次の式をTDターゲットとする。
            <div class="scroll">
                \begin{align}
                R_t + \gamma Q_{\theta '} (S_{t+1}, argmax_a Q_{\theta} (S_{t+1}, a))
                \end{align}
            </div>
            ポイントは、\(Q_{\theta}(s,a)\)を使って最大となる行動を選び、実際の値は\(Q_{\theta '}(s,a)\)から取得することである。このように２つのQ関数を使いわけることで、過大評価が解消され、学習がより安定する。</p>
            <h3>優先度付き経験再生</h3>
            <p>DQNで使われる経験再生では、経験\(E_t = (S_t, A_t, R_t, S_{t+1})\)をバッファに保存し、学習時にはバッファから経験データをランダムに取り出して使う。これをさらに進化させたのが、優先度付き経験再生である。
            その言葉どおり、経験データをランダムに選ぶのではなく、優先度に応じて選ばれやすくする。優先度の決め方として、自然に考えられるのは次の式である。
            <div class="scroll">
                \begin{align}
                \delta_t = |R_t + \gamma \max_a Q_{\theta '} (S_{t+1}, a) - Q_{\theta}(S_t, A_t)|
                \end{align}
            </div>
            上式のとおり、TDターゲットである\(R_t + \gamma \max_a Q_{\theta '} (S_{t+1}, a)\)と\(Q_{\theta}(S_t, A_t)\)の差分をとり、その絶対値を求めて\(\delta_t\)とする。この値が大きければ、それだけ修正すべきことが大きい、つまりは、学ぶべきことが大きいということである。
            逆にこの値が小さければ、すでに良いパラメータであり、学ぶべきことは少ないということになる。優先度付き経験再生では、経験データをバッファに保存するときに\(\delta_t\)も計算する。そして、\(\delta_t\)も経験データに含めた\(S_t, A_t, R_t, S_{t+1}, \delta_t\)をバッファに追加する。
            バッファから経験データを取り出すときは、\(\delta_t\)を使って各経験データが選ばれる確率を計算する。仮に\(N\)個の経験データがバッファに含まれる場合、\(i\)番目の経験データが選ばれる確率は次の式で表される。
            <div class="scroll">
                \begin{align}
                p_i = \frac{\delta_i}{\sum_{k=0}^{N} \delta_k}
                \end{align}
            </div>
            この確率\(p_i\)に応じて、バッファから経験データを選び出す。優先度付き経験再生を使うことで、学ぶべきことが多いデータほど優先して使用されるため、学習がより早く進むことが期待できる。</p>
            <h3>Dueling DQN</h3>
            <p>Dueling DQNは、ニューラルネットワークの構造を工夫した手法である。この手法でキーとなるのが、アドバンテージ関数である。アドバンテージ関数は、Q関数と価値関数の差分で定義される。つまり、次の式で表される。
            <div class="scroll">
                \begin{align}
                A_{\pi}(s,a) = Q_{\pi} (s, a) - V_{\pi} (s)
                \end{align}
            </div>
            アドバンテージ関数は、\(a\)という行動が方策\(\pi\)に比べてどれだけ良いか（もしくは悪いか）を表す。その理由は次の点から明らかである。</p>
            <ul>
                <li>\(Q_{\pi}(s, a)\)は、状態\(s\)において「特定の行動\(a\)」を行い、それ以降は\(\pi\)に従って行動した時に得られる収益の期待値。</li>
                <li>\(V_{\pi} (s)\)は、状態\(s\)において以降すべての方策\(\pi\)に従って行動したときに得られる収益の期待値。</li>
            </ul>
            <p>つまり、\(Q_{\pi}(s,a)\)と\(V_{\pi})(s)\)の違いは、状態\(s\)において行動\(a\)を行うか、それとも方策\(\pi\)に従って行動を選ぶかの違いである。
            そのため、アドバンテージ関数は、「\(a\)という行動」が「方策\(\pi\)で選ばれる行動」に比べてどれだけ良いかを示す指標として解釈できる。また、上式のアドバンテージ関数をもとに、Q関数を求めることもできる。
            <div class="scroll">
                \begin{align}
                Q_{\pi} (s, a) = A_{\pi}(s,a) + V_{\pi} (s)
                \end{align}
            </div>
            Dueling DQNは、この式をニューラルネットワークで実現する。ネットワークとして途中まで計算を共有して、アドバンテージ関数と価値関数に枝分かれするように分離する。そして最後にその2つを加算して\(Q(s,a)\)を出力する。この構造がDueling DQNの特徴である。
            これは主に、どのような行動を行っても結果がほとんど変わらないような状態において利点があると考えられる。例えば、「Pong」において、どのような行動を取っても結果は負け（マイナスの報酬）になる場面を考える。
            DQNの場合、ある状態\(s\)で実際に行った行動\(a\)に対して\(Q(s, a)\)を学習する。行動に関わらず結果が決まっている状態であっても、すべての行動を試さなければ\(Q(s,a)\)は学習されない。
            一方のDueling DQNでは、価値関数\(V(s)\)を経由する。価値関数はある状態\(s\)における価値である。そのため、今回の場面を経験すれば\(V(s)\)が学習され、それによって他の行動を試さなくても\(Q(s, a)\)の近似性能が改善する。これにより、学習が促進することが期待できる。</p>
        </section>
        <section id="pgm">
            <h2>Policy Gradient Method</h2>
            <p>Q学習やSARSA、モンテカルロ法などは、大別すれば価値ベースの手法に分類される。価値ベースの手法は、価値関数をモデル化し、価値関数を学習する。そして、価値関数を経由して方策を得る。
            価値ベースの手法では、一般化方策反復というアイデアに基づいて最適方策を見つけることが多く行われる。具体的には、価値関数の評価と方策を改善するというプロセスを繰り返すことで、徐々に最適方策に近づく。
            価値ベースの手法の他に、価値関数を経由せずに方策を直接表す手法も考えられる。これが方策ベースの手法である。中でも、方策をニューラルネットワークなどでモデル化し、勾配を使って方策を最適化する手法は方策勾配法と呼ばれる。
            方策勾配法に基づくアルゴリズムは様々な手法が提案されている。</p>
            <p>確率的な方策は数式で\(\pi (a|s)\)と表される。\(\pi (a|s)\)は、状態\(s\)において、\(a\)という行動を取る確率である。ここでは、方策をニューラルネットワークでモデル化する。
            それにあたり、ニューラルネットワークのすべての重みパラメータを\(\theta\)で表す（\(\theta\)は、すべてのパラメータの要素を一列に並べたベクトルとする）。そして、ニューラルネットワークによる方策を\(\pi_{\theta} (a|s)\)で表す。</p>
            <p>方策\(\pi_{\theta}\)を使って目的関数を設定する。まず、次のような「状態、行動、報酬」からなる時系列データが得られたと仮定する。
            <div class="scroll">
                \begin{align}
                \tau = (S_0, A_0, R_0, S_1, A_1, R_1, \cdots, S_{T+1})
                \end{align}
            </div>
            \(\tau\)は軌道と呼ばれる。このとき、収益は以下のように設定できる。
            <div class="scroll">
                \begin{align}
                G(\tau) = R_0 + \gamma R_1 + \gamma^2 R_2 + \cdots + \gamma^T R_T
                \end{align}
            </div>
            このとき、目的関数は次の式で表される。
            <div class="scroll">
                \begin{align}
                J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [G(\tau)]
                \end{align}
            </div>
            収益は確率的に変動するため、その期待値が目的関数となる。\(\tau \sim \pi_\theta\)は、\(\tau\)が\(\pi_\theta\)によって生成されることを示している。
            目的関数が決まれば、次はその勾配を求める。ここではパラメータ\(\theta\)に関する勾配を\(\nabla_{\theta}\)で表す。ここでの目標は、\(\nabla_{\theta} J(\theta)\)を求めることである。この結果は以下のようになる。
            <div class="scroll">
                \begin{align}
                \nabla_{\theta} J(\theta) &= \nabla_{\theta} \mathbb{E}_{\tau \sim \pi_\theta} [G(\tau)] \\
                &= \mathbb{E}_{\tau~\pi_\theta} [\sum_{t=0}^{T} G(\tau) \nabla_{\theta} \log \pi_{\theta} (A_t | S_t)]
                \end{align}
            </div>
            この式で注目したい点は、\(\nabla_{\theta}\)が\(\mathbb{E}\)の中にあることである。これが求まれば、続いてニューラルネットワークのパラメータを更新する。単純な最適化の方法は次の式で表される。
            <div class="scroll">
                \begin{align}
                \theta \leftarrow \theta + \alpha \nabla_{\theta} J(\theta)
                \end{align}
            </div></p>
            <p>\(\nabla_{\theta} J(\theta)\)は上で示したように、期待値として表される。この期待値はモンテカルロ法によって求めることができる。モンテカルロ法は、サンプリングをいくつか行って、その平均を取る。
                今回の場合、方策\(\pi_{\theta}\)のエージェントに実際に行動させ、軌道\(\tau\)を\(n\)個得たとする。その場合、各\(\tau\)において上式における期待値の中身を計算して、その平均を求めることで\(\nabla_{\theta} J(\theta)\)を近似できる。</p>
            <h3>REINFORCE</h3>
        </section>
    </main>
</body>
</html>
