<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【深層強化学習 | PyTorch】走る作曲家のAIカフェ</title>
    <link rel="stylesheet" href="style.css"> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1050827580219099"
     crossorigin="anonymous"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
            <h2>目次</h2>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#nn-q">NN & Q-Learning</a></li>
                <li><a href="#dqn">DQN</a></li>
            </ul>
        </section>
        <section id="overview">
            <h2>Overview</h2>
            深層強化学習とは、強化学習と深層学習を組み合わせた手法です。ニューラルネットワークを使用して、エージェントが環境から得られる観測値をもとに価値関数や方策を近似します。<br>
            このページでは、深層強化学習について基礎から学んでいきます。
        </section>
        <section id="nn-q">
          <h2>NN & Q-Learning</h2>
            <p>「3×4のグリッドワールド」程度の問題なら、Q関数をテーブルとして保持できる。しかし、現実はより複雑で、状態も行動も膨大になる可能性がある。テーブルとして保持するのが難しくなるほか、莫大な数をすべて経験することも現実的ではない。
            この問題を解決するために、Q関数をコンパクトな関数で近似することを考える。そのための最も有力な手法が、ディープラーニングである。</p>
            <p>ニューラルネットワークの代表的な構造は2通り考えられる。1つ目は、状態と行動の2つを入力とするネットワークである。この場合、出力はQ関数の値を1つだけ出力する。
            もう1つの構造は、状態だけを入力として、行動の候補の数の分だけQ関数の値を出力するネットワークである。
            ただし、1つ目のネットワーク構造には計算コストの点で問題がある。具体的には、ある状態においてQ関数の最大値を求める計算コスト（\(\max_a Q(s, a)\)の計算コスト）が大きくなる。よって、2つ目のネットワーク構造を用いる。</p>
            <p>Q学習では次の式によってQ関数を更新する。
            <div class="scroll">
                \begin{align}
                Q'(S_t, A_t) = Q(S_t, A_t) + \alpha {R_t + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)}
                \end{align}
            </div>
            この式によって、\(Q(S_t, A_t)\)の値は、ターゲットである\(R_t + \gamma \max_a Q(S_{t+1}, a)\)の方向へと更新される。このとき、\(\alpha\)によってどれだけターゲットの方向へ進むかが調整される。
            ここで、ターゲットである\(R_t + \gamma \max_a Q(S_{t+1}, a)\)を\(T\)で表すことにする。そうすると、上式は以下のように表される。
            <div class="scroll">
                \begin{align}
                Q'(S_t, A_t) = Q(S_t, A_t) + \alpha {T - Q(S_t, A_t)}
                \end{align}
            </div>
            この式は、入力が\(S_t, A_t\)のとき出力が\(T\)になるようにQ関数を更新すると解釈できる。これをニューラルネットワークの文脈に当てはめると、入力が\(S_t, A_t\)で、出力が\(T\)となるように学習させることと同じである。
            つまり、\(T\)は正解ラベルとみなせる。また、\(T\)はスカラ値なので、これは回帰問題と考えることができる。</p>
        </section>
        <section id="dqn">
            <h2>DQN</h2>
            
        </section>
