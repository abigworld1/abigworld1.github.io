<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【PyTorchで学ぶ深層学習】走る作曲家のAIカフェ</title>
    <link rel="stylesheet" href="style.css"> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1050827580219099"
     crossorigin="anonymous"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
            <h2>目次</h2>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#source">Source</a></li>
                <li><a href="#python">ディープラーニングのためのPython</a></li>
                <li><a href="#pytorch">PyTorchの基本</a></li>
            </ul>
        </section>
        <section id="overview">
            <h2>Overview</h2>
            深層学習とは、人工ニューラルネットワークを多層に積み重ねて学習させる手法で、複雑なデータのパターンや特徴を自動的に抽出することができる機械学習の一分野です。<br>
            このページでは、深層学習のライブラリであるPyTorchを用いて、深層学習の基礎を学んでいきます。
        </section>
        <section id="source">
            <h2>Source</h2>
            <p>以下の講義・書籍を参考にしました。</p>
            <ul>
                <li><a href="https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI">MIT 6.S191: Introduction to Deep Learning</a></li>
                <li><a href="https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html">Quickstart — PyTorch Tutorials 2.4.0+cu121 documentation</a></li>
                <li><a href="https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/af0caf6d7af0dda755f4c9d7af9ccc2c/quickstart_tutorial.ipynb">quickstart_tutorial.ipynb</a></li>
                <li><a href="https://yutaroogawa.github.io/pytorch_tutorials_jp/">PyTorchチュートリアル（日本語翻訳版）</a></li>
                <li><a href="https://github.com/makaishi2/pytorch_book_info">makaishi2_pytorch_book_info_ 書籍「最短コースでわかるPyTorch深層学習プログラミング」用サポートサイト</a></li>
                <li><a href="https://www.amazon.co.jp/%E6%9C%80%E7%9F%AD%E3%82%B3%E3%83%BC%E3%82%B9%E3%81%A7%E3%82%8F%E3%81%8B%E3%82%8B-PyTorch-%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-%E8%B5%A4%E7%9F%B3-%E9%9B%85%E5%85%B8/dp/4296110322#customerReviews">最短コースでわかる PyTorch &深層学習プログラミング</a></li>
            </ul>     
        </section>
        <section id="python">
            <h2>ディープラーニングのためのPython</h2>
            <h3>コンテナデータ型</h3>
            <p>コンテナデータ型：「リスト」やNumPy配列のように、名前から実際のデータにアクセスするのにインデックスを経由する必要があるデータの型。</p>
            <p>
                <pre><code>
x = np.array([5, 7, 9])
y = x
x[1] = -1
print(y)
                </code></pre>
            この出力は、<code2>[5, -1, 9]</code2>となる。xの要素の変更がyに影響しないようにするには、<code2>y = x.copy()</code2>を使う。</p>
            <p>PyTorchで扱うデータは「テンソル」(Tensor)というクラスのインスタンスに保存される。</p>
            <p>テンソルの場合も同様にcopyしておけば他からの影響を気にしないで済む。</p>
            <h3>カスタムクラス定義</h3>
            <b>オブジェクト指向の基礎概念</b>
            <p>クラス：「型」。</p>
            <p>インスタンス：「型」から生成された個別の実体。</p>
            <p>クラスは、「属性」と呼ばれるクラス内の変数を持っている。</p>
            <p>「関数」あるいは「メソッド」と呼ばれる処理機能も持っている。</p>
            <p>属性と呼ばれるクラス内の変数の値は、インスタンスごとに異なる。</p>
            <b>最初のクラス定義</b>
            <p>例）Pointというクラスを定義してみる。</p>
            <p>Pointクラスの属性のxとyは、点のx座標とy座標である。</p>
            <p>また、関数drawは、自分を点としてグラフに表示する関数である。</p>
            <p>以下のように実装される。</p>
            <pre><code>
import matplotlib.pyplot as plt

# 円描画に必要なライブラリ
import matplotlib.patches as patches

# クラスPointの定義
Class Point:
    # インスタンス生成時にxとyの２つの引数を持つ
    def __init__(self, x, y):
        # インスタンスの属性xに第１引数をセットする
        self.x = x
        # インスタンスの属性yに第２引数をセットする
        self.y = y
    # 描画関数drawの定義（引数なし）
        def draw(self):
            # (x, y)に点を描画する
            plt.plot(self.x, self.y, marker='o', markersize=10, c='k')
            </code></pre>
            <p><code2>__init__</code2>は初期化処理。</p>
            <p><code2>self</code2>はクラスからインスタンスが生成された際、インスタンス自身を指す。</p>
            <b>最初のインスタンス生成</b>
            <pre><code>
# クラスPointからインスタンス変数p1とp2を生成する
p1 = Point(2, 3)
p2 = Point(-1, -2)
            </code></pre>
            <p>引数リストの引数は、__init__関数の定義の引数からselfを取り除いたもの。</p>
            <b>インスタンス属性へのアクセス</b>
            <pre><code>
# p1とp2の属性x、yの参照
print(p1.x, p1.y)
print(p2.x, p2.y)
            </code></pre>
            <b>draw関数の呼び出し</b>
            <pre><code>
# p1とp2のdraw関数を呼び出し、2つの点を描画する
p1.draw()
p2.draw()
plt.xlim(-4, 4)
plt.ylim(-4, 4)
plt.show()
            </code></pre>
            <b>Circleクラスの定義</b>
            <p>中心点のx座標とy座標の他に半径を意味する属性rを持つ。</p>
            <p>xとyについてはPointクラスの定義を再利用できる（「クラスの継承」）。</p>
            <pre><code>
# Pointの子クラスCircleの定義
class Circle(Point)
    # Circleはインスタンス生成時に引数x、y、rを持つ
    def __init__(self, x, y, r):
        # xとyは、親クラスの属性として設定
        super().__init__(x, y)
        # rは、Circleの属性として設定
        self.r = r

    # Circleのdraw関数は、親の関数呼び出しのあとで、円の描画も独自に行う
    def draw(self):
        # 親クラスのdraw関数呼び出し
        super().draw()
                
        # 円の描画
        c = patches.Circle(xy=(self.x, self.y), radius=self.r, fc='b', ec='k')
        ax.add_patch(c)
            </code></pre>
            <p>CircleクラスはPointクラスの子クラスとして定義している。</p>
            <b>Circleインスタンスの生成とdraw関数の呼び出し</b>
            <pre><code>
# クラスCircleからインスタンス変数c1を生成する
c1 = Circle(1, 0, 2)

# p1, p2, c1のそれぞれのdraw関数を呼び出す
ax = plt.subplot()
p1.draw()
p2.draw()
c1.draw()
plt.xlim(-4, 4)
plt.ylim(-4, 4)
plt.show()
            </code></pre>
            <p>親クラスと同じ名前の関数を子クラスでも定義して振る舞いを変更させることを「オーバーライド」と呼ぶ。</p>
            <h3>インスタンスを関数として扱う</h3>
            <p>クラスから生成したインスタンスを呼び出し可能な関数にする。</p>
            <pre><code>
# 関数クラスHの定義
class H:
    def __call__(self, x):
        return 2*x**2 + 2
            </code></pre>
            <pre><code>
# hが関数として動作することを確認する

# NumPy配列としてxを定義
x = np.arange(-2, 2.1, 0.25)
print(x)

# Hクラスのインスタンスとしてhを生成
h = H()

# 関数hの呼び出し
y = h(x)
print(y)
            </code></pre>
        </section>
        <section id="pytorch">
            <h2>PyTorchの基本</h2>
            <h3>重要な概念</h3>
            <p>PyTorchでは、テンソルという独自のクラスでデータを表現する。</p>
            <p>PyTorchの最大の特徴は、自動微分機能。</p>
            <h3>テンソル</h3>
            <b>ライブラリインポート</b>
            <pre><code>
import torch
            </code></pre>
            <b>色々な階層のテンソル</b>
            <pre><code>
# 0階テンソル（スカラー）
r0 = torch.tensor(1.0).float()
            </code></pre>
            <p>テンソル変数を作るのに一番簡単なのは、torch.tensor関数を使う方法。</p>
            <p>テンソル変数の生成時には、必ず後ろにfloat関数の呼び出しをつけて、dtype（テンソル変数の要素のデータ型）を強制的にfloat32に変換する。</p>
            <pre><code>
# 1階テンソル（ベクトル）

# 1階のNumPy変数作成
r1_np = np.array([1, 2, 3, 4, 5])

# NumPyからテンソルに変換
r1 = torch.tensor(r1_np).float()

# dtypeを調べる
print(r1.dtype)

# shapeを調べる
print(r1.shape)

# データを調べる
print(r1.data)
            </code></pre>
            <pre><code>
# 2階テンソル（行列）

# 2階のNumPy変数作成
r2_np = np.array([1, 5, 6], [4, 3, 2])

# NumPyからテンソルに変換
r2 = torch.tensor(r2_np).float()
            </code></pre>
            <pre><code>
# 3階テンソル

# 乱数seedの初期化
torch.manual_seed(123)

# shape=[3,2,2]の正規分布変数テンソルを作る
r3 = torch.randn((3, 2, 2))
            </code></pre>
            <b>整数値のテンソル</b>
            <p>PyTorchによる計算では、そのほとんどの場合、数値型としてdtype=float32を利用する。</p>
            <p>しかし、「多値分類」用の損失関数である、nn.CrossEntropyLossとnn.NLLLossは、損失関数呼び出し時に、第2引数に整数型を指定する必要がある。</p>
            <p>その際は、以下のように整数型へ変換する。</p>
            <pre><code>
r5 = r1.long()
            </code></pre>
            <p>このようにlong関数をかけると、dtype=torch.int64になる。</p>
            <b>view関数</b>
            <p>view関数は、NumPyのreshape関数に相当し、変数の階数を変換することができる。</p>
            <pre><code>
# 2階化（r3は[3,2,2]の3階テンソル）
# 要素数に-1を指定すると、この数を自動調整する
r6 = r3.view(3, -1)
            </code></pre>
            <p>結果として、[3, 4]の2階テンソルが得られる。</p>
            <pre><code>
# 1階化
r7 - r3.view(-1)
            </code></pre>
            <b>それ以外の属性</b>
            <p>テンソルはデータとして扱うことも、クラスとして扱うこともできる。</p>
            <pre><code>
# requires_grad属性
print('requires_grad: ', r1.requires_grad)

# device属性
print('device: ', r1.device)
            </code></pre>
            <b>item関数</b>
            <p>スカラー（0階テンソル）に対しては、テンソルからPython本来のクラスの数値（floatまたはint）を取り出すのにitem関数が使える。</p>
            <p>計算結果テンソルとしてのlossから、データ記録用に値だけを抽出する場合によく用いる。</p>
            <pre><code>
item = r0.item()
            </code></pre>
            <p>この関数は、1階以上のテンソルを対象にできない。</p>
            <b>max関数</b>
            <pre><code>
# max関数を引数無しで呼び出すと、全体の最大値が取得できる
print(r2.max())
            </code></pre>
            <pre><code>
# torch.max関数
# 2つ目の引数はどの軸で集約するかを意味する（軸=1: 行方向、軸=0: 列方向）
print(torch.max(r2, 1))
            </code></pre>
            <p>この呼び出し方をした場合、最大値の値そのものだけでなく、どのindexで最大値を取ったかも返ってくる。</p>
            <p>後ろに[1]をつけると、後者のみを抽出できる。</p>
            <pre><code>
# 何番目の要素が最大値を取るかは、indicesを調べると良い
# 以下の計算は、多値分類で予測ラベルを求めるときによく利用される
print(torch.max(r2, 1)[1])
            </code></pre>
            <p>「複数の予測器の出力のうち、最も大きな値を出した予測器のindexが予測結果ラベルになる」ことを実装している。</p>
            <b>NumPy変数への変換</b>
            <pre><code>
# NumPy化
r2_np = r2.data.numpy()
            </code></pre>
            <p>ただし、この方法だと、テンソル変数とNumPy配列は同じデータを指すため、テンソル側で値を変えるとNumPyも連動して値が変わる。</p>
            <p>連動しないようにするには、r2.data.numpy.copy()として、データのコピーを作る。</p>
            <h3>自動微分機能</h3>
            <p>PyTorchで自動微分を行う場合の処理の流れ：</p>
            <ol>
                <li>勾配計算用変数の定義：requires_grad=Trueとする</li>
                <li>テンソル変数間で計算：裏で計算グラフが自動生成される</li>
                <li>計算グラフの可視化：make_dot関数</li>
                <li>勾配計算：backward関数</li>
                <li>勾配値の取得：grad属性</li>
                <li>勾配値の初期化：zero_関数</li>
            </ol>
            <b>1. 勾配計算用変数の定義</b>
            <p>requires_grad属性をTrueに設定する。</p>
            <b>2. テンソル変数間で計算</b>
            <p>他のテンソル変数との間で演算する。</p>
            <p>このとき、計算式による値の計算が行われるのと同時に、裏で「計算グラフ」が生成される。</p>
            <p>この機能は「Define by Run」と呼ばれる。</p>
            <p>「計算グラフ」とは、データとそれに対する演算の順番を定義するもの。</p>
            <b>3. 計算グラフの可視化</b>
            <p>PyTorch内部の動きを確認するために行う。</p>
            <p>make_dotという関数を使うと、2.で自動生成された計算グラフを可視化できる。</p>
            <b>4. 勾配計算</b>
            <p>勾配計算は、計算結果を保存したテンソル変数（スカラー）に対して、backward関数を呼び出すことにより行われる。</p>
            <b>5. 勾配値の取得</b>
            <p>勾配計算の結果は、PyTorchでは勾配値と呼ばれる。</p>
            <p>勾配値はテンソル変数のgrad属性により取得できる。</p>
            <b>6. 勾配値の初期化</b>
            <p>grad属性に保存されている勾配値は、利用が終わったら値を初期化する必要がある。</p>
            <p>そのための関数がzero_関数である。</p>
            <h3>2次関数の勾配計算</h3>
            <p>例）\(y = 2x^2+2\)に対して自動微分計算を行う。</p>
            <b>1. 勾配計算用変数の定義</b>
            <pre><code>
# xをNumPy配列で定義
x_np = np.arange(-2, 2.1, 0.25)
            </code></pre>
            <pre><code>
# 1. 勾配計算用変数の定義

# requires=Trueとする
x = torch.tensor(x_np, requires_grad=True, dtype=torch.float32)
            </code></pre>
            <b>2. テンソル変数間で計算</b>
            <pre><code>
# 2次関数の計算
# 裏で計算グラフが自動生成される

y = 2 * x**2 + 2
            </code></pre>
            <p>このとき、変数yは自動的にテンソル変数になる。</p>
            <p>requires_grad属性がTrueのテンソル変数は、そのままではMatplotlibで使えないが、data属性を渡すとグラフ表示が可能。</p>
            <pre><code>
# グラフ描画

plt.plot(x.data, y.data)
plt.show()
            </code></pre>
            <p>勾配計算の対象はスカラーである必要があるため、yの値をsum関数ですべて足して、足した結果を新しいテンソル変数zに代入する。</p>
            <p>（「なぜ和をとることで微分計算ができるのか」についての説明は<a href = "https://qiita.com/makaishi2/items/a6cf19add4b6d16b8483">こちら</a>。）</p>
            <pre><code>
# 勾配計算のため、sum 関数で 1階テンソルの関数値をスカラー化する
# (sum 関数を各要素で偏微分した結果は1なので、元の関数の微分結果を取得可能 )
z = y.sum()
            </code></pre>
            <b>3. 計算グラフの可視化</b>
            <p>この変数zを使って、計算グラフの可視化を行う。</p>
            <pre><code>
# 3. 計算グラフの可視化

# 必要なライブラリのインポート
from torchviz import make_dot

# 可視化関数の呼び出し
g = make_dot(z, params={'x': x})
display(g)
            </code></pre>
            <p>make_dot関数の呼び出し時の第1引数として、可視化したい計算グラフの対象となる変数（今回の場合z）、第2引数のparamsとして微分計算対象の変数（今回の場合x）を辞書形式のパラメータリストにして渡す。</p>
            <p>出発点のxをテンソル変数とし、かつrequires_gradフラグをセットしておくと、計算の過程が自動的に記録される。</p>
            <p>値を計算しながら、計算過程を自動的に記録する機能は「Define by Run」と呼ばれる。</p>
            <p>計算グラフにおいて、青色のノードはmake_dot関数呼び出し時にparamsで指定した変数に該当し、リーフノードと呼ばれる。</p>
            <p>勾配値の計算が可能な変数を意味している。</p>
            <p>緑色のノードは出力ノード、灰色のノードは中間処理という意味になっている。</p>
            <p>一番上と一番下のノードに記載される()は、それぞれの変数のshapeを示している。</p>
            <p>(17)であれば、1階17次元のテンソル、()であれば、0階のスカラーということになる。</p>
            <p>"AccumulateGrad"は、末端のリーフノードの直下に配置され、勾配値を蓄積する場所を示している。</p>
            <p>計算グラフがあれば、PyTorch側で\(f(x) = 2x^2 + 2\)という2次関数を、指数関数、乗算関数、加算関数という基本的な関数の合成関数として認識できる。</p>
            <b>4. 勾配計算</b>
            <p>backward関数を呼び出すだけ。</p>
            <pre><code>
# 4. 勾配計算
z.backward()
            </code></pre>
            <b>5. 勾配値の取得</b>
            <pre><code>
print(x.grad)
            </code></pre>
            <b>6. 勾配値の初期化</b>
            <p>x.gradには最新の勾配計算の結果が入るのではなく、それまでの勾配計算結果を加算した値が入る。</p>
            <p>条件を変えて新しく勾配値を取得したい場合、勾配値の初期化が必要になる。</p>
            <pre><code>
# 6. 勾配の初期化は関数zero_()を使う
x.grad.zero_()
            </code></pre>
            <h3>シグモイド関数の勾配計算</h3>
            <b>シグモイド関数の定義</b>
            <pre><code>
# シグモイド関数の定義
sigmoid = torch.nn.Sigmoid()
            </code></pre>
            <b>2. テンソル変数でyの値を計算</b>
            <pre><code>
# 2. yの値の計算
y = sigmoid(x)
            </code></pre>
            <b>グラフ描画</b>
            <pre><code>
plt.plot(x.data, y.data)
plt.show()
            </code></pre>
            <b>最終結果をスカラー化</b>
            <pre><code>
z = y.sum()
            </code></pre>
            <b>3. 計算グラフの可視化</b>
            <pre><code>
3. 計算グラフの可視化
g = make_dot(z, params={'x': x})
            </code></pre>
            <b>4. 勾配計算、5. 勾配値の取得</b>
            <pre><code>
# 4. 勾配計算
z.backward()

# 5. 勾配値の確認
print(x.grad)
            </code></pre>
            <p>グラフに表示</p>
            <pre><code>
# 元の関数と勾配のグラフ化

plt.plot(x.data, y.data, c='b', label='y')
plt.plot(x.data, x.grad.data, c='k', label='x.grad')
plt.legend()
plt.show()
            </code></pre>
        </section>
    </main>
</body>
</html>
