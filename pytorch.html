<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【深層学習 | PyTorchの基本】走る作曲家のAIカフェ</title>
    <link rel="stylesheet" href="style.css"> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1050827580219099"
     crossorigin="anonymous"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
            <h2>目次</h2>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#source">Source</a></li>
                <li><a href="#python">Python For Deep Learning</a></li>
                <li><a href="#pytorch">Basics of PyTorch</a></li>
                <li><a href="#ml">Introduction to Machine Learning</a></li>
                <li><a href="#pf">Definition of the Prediction Function</a></li>
                <li><a href="#lr">Linear Regression</a></li>
                <li><a href="#binary">Binary Classification</a></li>
                <li><a href="#multi">Multiclass Classification</a></li>
                <li><a href="#mnist">MNIST</a></li>
                <li><a href="#cnn">CNN</a></li>
                <li><a href="#tuning">Tuning</a></li>
                <li><a href="#pretrain">Pretrained Model</a></li>
                <li><a href="#custom">Custom Data</a></li>
            </ul>
        </section>
        <section id="overview">
            <h2>Overview</h2>
            深層学習とは、人工ニューラルネットワークを多層に積み重ねて学習させる手法で、複雑なデータのパターンや特徴を自動的に抽出することができる機械学習の一分野です。<br>
            このページでは、深層学習のライブラリであるPyTorchを用いて、深層学習の基礎を学んでいきます。
        </section>
        <section id="source">
            <h2>Source</h2>
            <p>以下の講義・書籍を参考にしました。</p>
            <ul>
                <li><a href="https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI">MIT 6.S191: Introduction to Deep Learning</a></li>
                <li><a href="https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html">Quickstart — PyTorch Tutorials 2.4.0+cu121 documentation</a></li>
                <li><a href="https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/af0caf6d7af0dda755f4c9d7af9ccc2c/quickstart_tutorial.ipynb">quickstart_tutorial.ipynb</a></li>
                <li><a href="https://yutaroogawa.github.io/pytorch_tutorials_jp/">PyTorchチュートリアル（日本語翻訳版）</a></li>
                <li><a href="https://github.com/makaishi2/pytorch_book_info">makaishi2_pytorch_book_info_ 書籍「最短コースでわかるPyTorch深層学習プログラミング」用サポートサイト</a></li>
                <li><a href="https://www.amazon.co.jp/%E6%9C%80%E7%9F%AD%E3%82%B3%E3%83%BC%E3%82%B9%E3%81%A7%E3%82%8F%E3%81%8B%E3%82%8B-PyTorch-%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-%E8%B5%A4%E7%9F%B3-%E9%9B%85%E5%85%B8/dp/4296110322#customerReviews">最短コースでわかる PyTorch &深層学習プログラミング</a></li>
            </ul>     
        </section>
        <section id="python">
            <h2>Python For Deep Learning</h2>
            <h3>コンテナデータ型</h3>
            <p>コンテナデータ型：「リスト」やNumPy配列のように、名前から実際のデータにアクセスするのにインデックスを経由する必要があるデータの型。</p>
            <p>
                <pre><code>
x = np.array([5, 7, 9])
y = x
x[1] = -1
print(y)
                </code></pre>
            この出力は、<code2>[5, -1, 9]</code2>となる。xの要素の変更がyに影響しないようにするには、<code2>y = x.copy()</code2>を使う。</p>
            <p>PyTorchで扱うデータは「テンソル」(Tensor)というクラスのインスタンスに保存される。</p>
            <p>テンソルの場合も同様にcopyしておけば他からの影響を気にしないで済む。</p>
            <h3>カスタムクラス定義</h3>
            <b>オブジェクト指向の基礎概念</b>
            <p>クラス：「型」。</p>
            <p>インスタンス：「型」から生成された個別の実体。</p>
            <p>クラスは、「属性」と呼ばれるクラス内の変数を持っている。</p>
            <p>「関数」あるいは「メソッド」と呼ばれる処理機能も持っている。</p>
            <p>属性と呼ばれるクラス内の変数の値は、インスタンスごとに異なる。</p>
            <b>最初のクラス定義</b>
            <p>例）Pointというクラスを定義してみる。</p>
            <p>Pointクラスの属性のxとyは、点のx座標とy座標である。</p>
            <p>また、関数drawは、自分を点としてグラフに表示する関数である。</p>
            <p>以下のように実装される。</p>
            <pre><code>
import matplotlib.pyplot as plt

# 円描画に必要なライブラリ
import matplotlib.patches as patches

# クラスPointの定義
Class Point:
    # インスタンス生成時にxとyの２つの引数を持つ
    def __init__(self, x, y):
        # インスタンスの属性xに第１引数をセットする
        self.x = x
        # インスタンスの属性yに第２引数をセットする
        self.y = y
    # 描画関数drawの定義（引数なし）
        def draw(self):
            # (x, y)に点を描画する
            plt.plot(self.x, self.y, marker='o', markersize=10, c='k')
            </code></pre>
            <p><code2>__init__</code2>は初期化処理。</p>
            <p><code2>self</code2>はクラスからインスタンスが生成された際、インスタンス自身を指す。</p>
            <b>最初のインスタンス生成</b>
            <pre><code>
# クラスPointからインスタンス変数p1とp2を生成する
p1 = Point(2, 3)
p2 = Point(-1, -2)
            </code></pre>
            <p>引数リストの引数は、__init__関数の定義の引数からselfを取り除いたもの。</p>
            <b>インスタンス属性へのアクセス</b>
            <pre><code>
# p1とp2の属性x、yの参照
print(p1.x, p1.y)
print(p2.x, p2.y)
            </code></pre>
            <b>draw関数の呼び出し</b>
            <pre><code>
# p1とp2のdraw関数を呼び出し、2つの点を描画する
p1.draw()
p2.draw()
plt.xlim(-4, 4)
plt.ylim(-4, 4)
plt.show()
            </code></pre>
            <b>Circleクラスの定義</b>
            <p>中心点のx座標とy座標の他に半径を意味する属性rを持つ。</p>
            <p>xとyについてはPointクラスの定義を再利用できる（「クラスの継承」）。</p>
            <pre><code>
# Pointの子クラスCircleの定義
class Circle(Point)
    # Circleはインスタンス生成時に引数x、y、rを持つ
    def __init__(self, x, y, r):
        # xとyは、親クラスの属性として設定
        super().__init__(x, y)
        # rは、Circleの属性として設定
        self.r = r

    # Circleのdraw関数は、親の関数呼び出しのあとで、円の描画も独自に行う
    def draw(self):
        # 親クラスのdraw関数呼び出し
        super().draw()
                
        # 円の描画
        c = patches.Circle(xy=(self.x, self.y), radius=self.r, fc='b', ec='k')
        ax.add_patch(c)
            </code></pre>
            <p>CircleクラスはPointクラスの子クラスとして定義している。</p>
            <b>Circleインスタンスの生成とdraw関数の呼び出し</b>
            <pre><code>
# クラスCircleからインスタンス変数c1を生成する
c1 = Circle(1, 0, 2)

# p1, p2, c1のそれぞれのdraw関数を呼び出す
ax = plt.subplot()
p1.draw()
p2.draw()
c1.draw()
plt.xlim(-4, 4)
plt.ylim(-4, 4)
plt.show()
            </code></pre>
            <p>親クラスと同じ名前の関数を子クラスでも定義して振る舞いを変更させることを「オーバーライド」と呼ぶ。</p>
            <h3>インスタンスを関数として扱う</h3>
            <p>クラスから生成したインスタンスを呼び出し可能な関数にする。</p>
            <pre><code>
# 関数クラスHの定義
class H:
    def __call__(self, x):
        return 2*x**2 + 2
            </code></pre>
            <pre><code>
# hが関数として動作することを確認する

# NumPy配列としてxを定義
x = np.arange(-2, 2.1, 0.25)
print(x)

# Hクラスのインスタンスとしてhを生成
h = H()

# 関数hの呼び出し
y = h(x)
print(y)
            </code></pre>
        </section>
        <section id="pytorch">
            <h2>Basics of PyTorch</h2>
            <h3>重要な概念</h3>
            <p>PyTorchでは、テンソルという独自のクラスでデータを表現する。</p>
            <p>PyTorchの最大の特徴は、自動微分機能。</p>
            <h3>テンソル</h3>
            <b>ライブラリインポート</b>
            <pre><code>
import torch
            </code></pre>
            <b>色々な階層のテンソル</b>
            <pre><code>
# 0階テンソル（スカラー）
r0 = torch.tensor(1.0).float()
            </code></pre>
            <p>テンソル変数を作るのに一番簡単なのは、torch.tensor関数を使う方法。</p>
            <p>テンソル変数の生成時には、必ず後ろにfloat関数の呼び出しをつけて、dtype（テンソル変数の要素のデータ型）を強制的にfloat32に変換する。</p>
            <pre><code>
# 1階テンソル（ベクトル）

# 1階のNumPy変数作成
r1_np = np.array([1, 2, 3, 4, 5])

# NumPyからテンソルに変換
r1 = torch.tensor(r1_np).float()

# dtypeを調べる
print(r1.dtype)

# shapeを調べる
print(r1.shape)

# データを調べる
print(r1.data)
            </code></pre>
            <pre><code>
# 2階テンソル（行列）

# 2階のNumPy変数作成
r2_np = np.array([1, 5, 6], [4, 3, 2])

# NumPyからテンソルに変換
r2 = torch.tensor(r2_np).float()
            </code></pre>
            <pre><code>
# 3階テンソル

# 乱数seedの初期化
torch.manual_seed(123)

# shape=[3,2,2]の正規分布変数テンソルを作る
r3 = torch.randn((3, 2, 2))
            </code></pre>
            <b>整数値のテンソル</b>
            <p>PyTorchによる計算では、そのほとんどの場合、数値型としてdtype=float32を利用する。</p>
            <p>しかし、「多値分類」用の損失関数である、nn.CrossEntropyLossとnn.NLLLossは、損失関数呼び出し時に、第2引数に整数型を指定する必要がある。</p>
            <p>その際は、以下のように整数型へ変換する。</p>
            <pre><code>
r5 = r1.long()
            </code></pre>
            <p>このようにlong関数をかけると、dtype=torch.int64になる。</p>
            <b>view関数</b>
            <p>view関数は、NumPyのreshape関数に相当し、変数の階数を変換することができる。</p>
            <pre><code>
# 2階化（r3は[3,2,2]の3階テンソル）
# 要素数に-1を指定すると、この数を自動調整する
r6 = r3.view(3, -1)
            </code></pre>
            <p>結果として、[3, 4]の2階テンソルが得られる。</p>
            <pre><code>
# 1階化
r7 - r3.view(-1)
            </code></pre>
            <b>それ以外の属性</b>
            <p>テンソルはデータとして扱うことも、クラスとして扱うこともできる。</p>
            <pre><code>
# requires_grad属性
print('requires_grad: ', r1.requires_grad)

# device属性
print('device: ', r1.device)
            </code></pre>
            <b>item関数</b>
            <p>スカラー（0階テンソル）に対しては、テンソルからPython本来のクラスの数値（floatまたはint）を取り出すのにitem関数が使える。</p>
            <p>計算結果テンソルとしてのlossから、データ記録用に値だけを抽出する場合によく用いる。</p>
            <pre><code>
item = r0.item()
            </code></pre>
            <p>この関数は、1階以上のテンソルを対象にできない。</p>
            <b>max関数</b>
            <pre><code>
# max関数を引数無しで呼び出すと、全体の最大値が取得できる
print(r2.max())
            </code></pre>
            <pre><code>
# torch.max関数
# 2つ目の引数はどの軸で集約するかを意味する（軸=1: 行方向、軸=0: 列方向）
print(torch.max(r2, 1))
            </code></pre>
            <p>この呼び出し方をした場合、最大値の値そのものだけでなく、どのindexで最大値を取ったかも返ってくる。</p>
            <p>後ろに[1]をつけると、後者のみを抽出できる。</p>
            <pre><code>
# 何番目の要素が最大値を取るかは、indicesを調べると良い
# 以下の計算は、多値分類で予測ラベルを求めるときによく利用される
print(torch.max(r2, 1)[1])
            </code></pre>
            <p>「複数の予測器の出力のうち、最も大きな値を出した予測器のindexが予測結果ラベルになる」ことを実装している。</p>
            <b>NumPy変数への変換</b>
            <pre><code>
# NumPy化
r2_np = r2.data.numpy()
            </code></pre>
            <p>ただし、この方法だと、テンソル変数とNumPy配列は同じデータを指すため、テンソル側で値を変えるとNumPyも連動して値が変わる。</p>
            <p>連動しないようにするには、r2.data.numpy.copy()として、データのコピーを作る。</p>
            <h3>自動微分機能</h3>
            <p>PyTorchで自動微分を行う場合の処理の流れ：</p>
            <ol>
                <li>勾配計算用変数の定義：requires_grad=Trueとする</li>
                <li>テンソル変数間で計算：裏で計算グラフが自動生成される</li>
                <li>計算グラフの可視化：make_dot関数</li>
                <li>勾配計算：backward関数</li>
                <li>勾配値の取得：grad属性</li>
                <li>勾配値の初期化：zero_関数</li>
            </ol>
            <b>1. 勾配計算用変数の定義</b>
            <p>requires_grad属性をTrueに設定する。</p>
            <b>2. テンソル変数間で計算</b>
            <p>他のテンソル変数との間で演算する。</p>
            <p>このとき、計算式による値の計算が行われるのと同時に、裏で「計算グラフ」が生成される。</p>
            <p>この機能は「Define by Run」と呼ばれる。</p>
            <p>「計算グラフ」とは、データとそれに対する演算の順番を定義するもの。</p>
            <b>3. 計算グラフの可視化</b>
            <p>PyTorch内部の動きを確認するために行う。</p>
            <p>make_dotという関数を使うと、2.で自動生成された計算グラフを可視化できる。</p>
            <b>4. 勾配計算</b>
            <p>勾配計算は、計算結果を保存したテンソル変数（スカラー）に対して、backward関数を呼び出すことにより行われる。</p>
            <b>5. 勾配値の取得</b>
            <p>勾配計算の結果は、PyTorchでは勾配値と呼ばれる。</p>
            <p>勾配値はテンソル変数のgrad属性により取得できる。</p>
            <b>6. 勾配値の初期化</b>
            <p>grad属性に保存されている勾配値は、利用が終わったら値を初期化する必要がある。</p>
            <p>そのための関数がzero_関数である。</p>
            <h3>2次関数の勾配計算</h3>
            <p>例）\(y = 2x^2+2\)に対して自動微分計算を行う。</p>
            <b>1. 勾配計算用変数の定義</b>
            <pre><code>
# xをNumPy配列で定義
x_np = np.arange(-2, 2.1, 0.25)
            </code></pre>
            <pre><code>
# 1. 勾配計算用変数の定義

# requires=Trueとする
x = torch.tensor(x_np, requires_grad=True, dtype=torch.float32)
            </code></pre>
            <b>2. テンソル変数間で計算</b>
            <pre><code>
# 2次関数の計算
# 裏で計算グラフが自動生成される

y = 2 * x**2 + 2
            </code></pre>
            <p>このとき、変数yは自動的にテンソル変数になる。</p>
            <p>requires_grad属性がTrueのテンソル変数は、そのままではMatplotlibで使えないが、data属性を渡すとグラフ表示が可能。</p>
            <pre><code>
# グラフ描画

plt.plot(x.data, y.data)
plt.show()
            </code></pre>
            <p>勾配計算の対象はスカラーである必要があるため、yの値をsum関数ですべて足して、足した結果を新しいテンソル変数zに代入する。</p>
            <p>（「なぜ和をとることで微分計算ができるのか」についての説明は<a href = "https://qiita.com/makaishi2/items/a6cf19add4b6d16b8483">こちら</a>。）</p>
            <pre><code>
# 勾配計算のため、sum 関数で 1階テンソルの関数値をスカラー化する
# (sum 関数を各要素で偏微分した結果は1なので、元の関数の微分結果を取得可能 )
z = y.sum()
            </code></pre>
            <b>3. 計算グラフの可視化</b>
            <p>この変数zを使って、計算グラフの可視化を行う。</p>
            <pre><code>
# 3. 計算グラフの可視化

# 必要なライブラリのインポート
from torchviz import make_dot

# 可視化関数の呼び出し
g = make_dot(z, params={'x': x})
display(g)
            </code></pre>
            <p>make_dot関数の呼び出し時の第1引数として、可視化したい計算グラフの対象となる変数（今回の場合z）、第2引数のparamsとして微分計算対象の変数（今回の場合x）を辞書形式のパラメータリストにして渡す。</p>
            <p>出発点のxをテンソル変数とし、かつrequires_gradフラグをセットしておくと、計算の過程が自動的に記録される。</p>
            <p>値を計算しながら、計算過程を自動的に記録する機能は「Define by Run」と呼ばれる。</p>
            <p>計算グラフにおいて、青色のノードはmake_dot関数呼び出し時にparamsで指定した変数に該当し、リーフノードと呼ばれる。</p>
            <p>勾配値の計算が可能な変数を意味している。</p>
            <p>緑色のノードは出力ノード、灰色のノードは中間処理という意味になっている。</p>
            <p>一番上と一番下のノードに記載される()は、それぞれの変数のshapeを示している。</p>
            <p>(17)であれば、1階17次元のテンソル、()であれば、0階のスカラーということになる。</p>
            <p>"AccumulateGrad"は、末端のリーフノードの直下に配置され、勾配値を蓄積する場所を示している。</p>
            <p>計算グラフがあれば、PyTorch側で\(f(x) = 2x^2 + 2\)という2次関数を、指数関数、乗算関数、加算関数という基本的な関数の合成関数として認識できる。</p>
            <b>4. 勾配計算</b>
            <p>backward関数を呼び出すだけ。</p>
            <pre><code>
# 4. 勾配計算
z.backward()
            </code></pre>
            <b>5. 勾配値の取得</b>
            <pre><code>
print(x.grad)
            </code></pre>
            <b>6. 勾配値の初期化</b>
            <p>x.gradには最新の勾配計算の結果が入るのではなく、それまでの勾配計算結果を加算した値が入る。</p>
            <p>条件を変えて新しく勾配値を取得したい場合、勾配値の初期化が必要になる。</p>
            <pre><code>
# 6. 勾配の初期化は関数zero_()を使う
x.grad.zero_()
            </code></pre>
            <h3>シグモイド関数の勾配計算</h3>
            <b>シグモイド関数の定義</b>
            <pre><code>
# シグモイド関数の定義
sigmoid = torch.nn.Sigmoid()
            </code></pre>
            <b>2. テンソル変数でyの値を計算</b>
            <pre><code>
# 2. yの値の計算
y = sigmoid(x)
            </code></pre>
            <b>グラフ描画</b>
            <pre><code>
plt.plot(x.data, y.data)
plt.show()
            </code></pre>
            <b>最終結果をスカラー化</b>
            <pre><code>
z = y.sum()
            </code></pre>
            <b>3. 計算グラフの可視化</b>
            <pre><code>
3. 計算グラフの可視化
g = make_dot(z, params={'x': x})
            </code></pre>
            <b>4. 勾配計算、5. 勾配値の取得</b>
            <pre><code>
# 4. 勾配計算
z.backward()

# 5. 勾配値の確認
print(x.grad)
            </code></pre>
            <b>グラフに表示</b>
            <pre><code>
# 元の関数と勾配のグラフ化

plt.plot(x.data, y.data, c='b', label='y')
plt.plot(x.data, x.grad.data, c='k', label='x.grad')
plt.legend()
plt.show()
            </code></pre>
        </section>
        <section id="ml">
            <h2>Introduction to Machine Learning</h2>
            <h3>問題の定義</h3>
            <p>与えられた身長から体重を予測する機械学習モデルを作る（線形回帰）。</p>
            <h3>勾配降下法</h3>
            <p>勾配降下法は、「予測計算」「損失関数」「勾配計算」「パラメータ修正」の４つのステップを繰り返すことで、予測関数の中のパラメータを最適な値に近づけることである。</p>
            <b>①予測計算</b>
            <p>入力テンソルXを入力とし、予測結果は出力テンソルYpに出力されるものとする。</p>
            <p>今回は、予測関数の実体は次のような１次関数である。</p>
            <div class="scroll">
                \begin{align}
                Yp = W * X + B
                \end{align}
            </div>
            <p>予測関数により予測値Ypを求めることを「①予測計算」と呼ぶ。</p>
            <b>②損失計算</b>
            <p>「教師あり学習」における学習データは通常、入力と正解値の両方を含んでいるので、正解値の列を分離して正解テンソルYとする。</p>
            <p>予測計算の結果であるYpと正解であるYの、2つのテンソルを入力とする「損失」lossを定義する。</p>
            <p>損失が最小になるようなWとBを見つけることが目標である。</p>
            <p>この損失を計算する過程が「②損失計算」である。</p>
            <p>損失関数は、予測関数の性質に応じて適したものを選ぶ。</p>
            <p>今回は値を予測する回帰モデルであるため、YとYpの差の2乗を利用することにする。</p>
            <p>正確には、すべてのデータ系列の差を2乗して、その平均をとった関数である「平均２乗誤差」を選択する。</p>
            <b>③勾配計算</b>
            <p>予測関数を構成するパラメータWとBの値を少しだけ変え、その時の損失の変化の度合（勾配）を調べる。これが「③勾配計算」である。</p>
            <p>損失関数の最低値を目指すために、WとBをずらすベストな方向が勾配にあたる。</p>
            <b>④パラメータ修正</b>
            <p>勾配に小さな定数（学習率）lrをかけ、その値だけWとBを同時に減らす。この操作が「④パラメータ修正」である。</p>
            <h3>データ前処理</h3>
            <pre><code>
# サンプルデータの宣言
sampleData1 = np.array([
    [166, 58.7],
    [176.0, 75.7],
    [171.0, 62.1],
    [173.0, 70.4],
    [169.0,60.1]
])
print(sampleData1)
            </code></pre>
            <p>学習データを入力データxと正解データyに分割する。</p>
            <pre><code>
# 機械学習モデルで扱うため、身長だけを抜き出した変数xと
# 体重だけを抜き出した変数yをセットする

x = sampleData1[:,0]
y = sampleData1[:,1]
            </code></pre>
            <p>データの散布図を表示する。</p>
            <pre><code>
# 散布図表示で状況の確認

plt.scatter(x,  y,  c='k',  s=50)
plt.xlabel('$x$: 身長(cm) ')
plt.ylabel('$y$: 体重(kg)')
plt.title('身長と体重の関係')
plt.show()
            </code></pre>
            <b>データの変換</b>
            <p>勾配降下法では、対象となる数値は絶対値が1以内に収まるような比較的小さな値の方が望ましい。</p>
            <p>今回の学習データは身長も体重も大きな数値なので、それぞれ平均値を引くことで勾配降下法がやりやすい条件に変換する。</p>
            <p>変換後のxとyをXとYで表す。</p>
            <pre><code>
X = x - x.mean()
Y = y - y.mean()
            </code></pre>
            <h3>予測計算</h3>
            <p>まず、変換後のXとYをテンソル変数に変換する。</p>
            <pre><code>
# XとYをテンソル変数化する

X = torch.tensor(X).float()
Y = torch.tensor(Y).float()
            </code></pre>
            <p>次に、１次関数の係数にあたる変数Wと定数項にあたる変数Bもテンソル変数として定義する。</p>
            <pre><code>
# 重み変数の定義
# WとBは勾配計算をするので、requires_grad=Trueとする

W = torch.tensor(1.0, requires_grad=True).float()
B = torch.tensor(1.0, requires_grad=True).float()
            </code></pre>
            <p>２つの変数には初期値として1.０の値を設定する。</p>
            <p>また、この２つの変数は勾配降下法の対象となるので、requires_grad属性をTrueに設定して自動微分ができるようにする。</p>
            <p>予測関数を定義し、予測値を計算する。</p>
            <pre><code>
# 予測関数は一次関数

def pred(X):
    return W * X + B

# 予測値の計算

Yp =  pred(X)
            </code></pre>
            <p>計算グラフを表示する。</p>
            <pre><code>
# 予測値の計算グラフ可視化

params = {'W': W, 'B': B}
g = make_dot(Yp, params=params)
display(g)
            </code></pre>
            <h3>損失計算</h3>
            <p>損失関数はMSE（平均２乗誤差）と呼ばれる方法で計算する。</p>
            <pre><code>
# 損失関数は誤差二乗平均

def mse(Yp, Y):
    loss = ((Yp - Y) ** 2).mean()
    return loss

# 損失計算

loss = mse(Yp, Y)
            </code></pre>
            <p>ここで得られた損失(loss)は、１次関数の係数Wと定数項Bの関数になっている。計算グラフで確認する。</p>
            <pre><code>
# 損失の計算グラフ可視化

params = {'W': W, 'B': B}
g = make_dot(loss, params=params)
display(g)
            </code></pre>
            <h3>勾配計算</h3>
            <p>backward関数を呼び出すだけ。</p>
            <pre><code>
# 勾配計算

loss.backward()

# 勾配値確認

print(W.grad)
print(B.grad)
            </code></pre>
            <h3>パラメータ修正</h3>
            <p>勾配計算ができたら、その値に一定の学習率lr（0.01や0.001）を掛けた結果を、もとのパラメータ値から引くのが勾配降下法の基本的な考え方である。</p>
            <pre><code>
# 学習率の定義

lr = 0.001

#  勾配を元にパラメータ修正

W -= lr * W.grad
B -= lr * B.grad
            </code></pre>
            <p>ただし、このコードではエラーが発生する。</p>
            <p>勾配計算をしている最中の変数は他に影響が及んでしまうため、勝手に値を修正できない。</p>
            <p>この場合、with torch.no_grad()というコンテキストを設定すると、そのコンテキストの内部では一時的に計算グラフ生成機能が止まり、変数の修正が可能になる。</p>
            <pre><code>
# 勾配を元にパラメータ修正
# with torch.no_grad() を付ける必要がある

with torch.no_grad():
    W -= lr * W.grad
    B -= lr * B.grad

    # 計算済みの勾配値をリセットする
    W.grad.zero_()
    B.grad.zero_()
            </code></pre>
            <p>勾配値を使ってパラメータ値を更新したあと、次の勾配計算の準備のため、zero_関数で勾配値の初期化もしている。</p>
            <h3>繰り返し計算</h3>
            <p>実際に繰り返し計算を行う。初期化のセルは以下の通り。</p>
            <pre><code>
# 初期化

# WとBを変数として扱う
W = torch.tensor(1.0, requires_grad=True).float()
B = torch.tensor(1.0, requires_grad=True).float()

# 繰り返し回数
num_epochs = 500

# 学習率
lr = 0.001

# 記録用配列初期化
history = np.zeros((0, 2))
            </code></pre>
            <p>ループ処理のセルは以下の通り。</p>
            <pre><code>
# ループ処理

for epoch in range(num_epochs):

    # 予測計算
    Yp = pred(X)

    # 損失計算
    loss = mse(Yp, Y)

    # 勾配計算
    loss.backward()

    with torch.no_grad():
        # パラメータ修正
        W -= lr * W.grad
        B -= lr * B.grad

        # 勾配値の初期化
        W.grad.zero_()
        B.grad.zero_()

    # 損失の記録
    if (epoch %10 == 0):
        item = np.array([epoch, loss.item()])
        history = np.vstack((history, item))
        print(f'epoch = {epoch}  loss = {loss:.4f}')
            </code></pre>
            <h3>結果評価</h3>
            <p>WとBの最終的な値と、損失の開始時、終了時の値を表示する。</p>
            <pre><code>
# パラメータの最終値
print('W = ', W.data.numpy())
print('B = ', B.data.numpy())

#損失の確認
print(f'初期状態: 損失:{history[0,1]:.4f}')
print(f'最終状態: 損失:{history[-1,1]:.4f}')
            </code></pre>
            <p>損失の減り方をグラフで可視化する（学習曲線）。</p>
            <pre><code>
# 学習曲線の表示 (損失)

plt.plot(history[:,0], history[:,1], 'b')
plt.xlabel('繰り返し回数')
plt.ylabel('損失')
plt.title('学習曲線(損失)')
plt.show()
            </code></pre>
            <p>求めたWとBの値から直線の式を算出し、散布図に重ね描きする。</p>
            <pre><code>
# xの範囲を求める(Xrange)
X_max = X.max()
X_min = X.min()
X_range = np.array((X_min, X_max))
X_range = torch.from_numpy(X_range).float()
print(X_range)

# 対応するyの予測値を求める
Y_range = pred(X_range)
print(Y_range.data)

# グラフ描画

plt.scatter(X,  Y,  c='k',  s=50)
plt.xlabel('$X$')
plt.ylabel('$Y$')
plt.plot(X_range.data, Y_range.data, lw=2, c='b')
plt.title('身長と体重の相関直線(加工後)')
plt.show()
            </code></pre>
            <p>最後に、平均値を引き算した(X, Y)から元の(x, y)に戻して同じ散布図表示をする。</p>
            <pre><code>
# y座標値とx座標値の計算

x_range = X_range + x.mean()
yp_range = Y_range + y.mean()

# グラフ描画

plt.scatter(x,  y,  c='k',  s=50)
plt.xlabel('$x$')
plt.ylabel('$y$')
plt.plot(x_range, yp_range.data, lw=2, c='b')
plt.title('身長と体重の相関直線(加工前)')
plt.show()
            </code></pre>
            <h3>最適化関数の利用</h3>
            <p>WとBのパラメータの変更は、「最適化関数」と呼ばれる関数を経由して変更するのが主流。</p>
            <pre><code>
# 初期化

# WとBを変数として扱う
W = torch.tensor(1.0, requires_grad=True).float()
B = torch.tensor(1.0, requires_grad=True).float()

# 繰り返し回数
num_epochs = 500

# 学習率
lr = 0.001

# optimizerとしてSGD(確率的勾配降下法)を指定する
import torch.optim as optim
optimizer = optim.SGD([W, B], lr=lr)

# 記録用配列初期化
history = np.zeros((0, 2))
            </code></pre>
            <p>SGDというクラスのインスタンスを生成し、optimizerという変数に保存している。</p>
            <pre><code>
# ループ処理

for epoch in range(num_epochs):

    # 予測計算
    Yp = pred(X)

    # 損失計算
    loss = mse(Yp, Y)

    # 勾配計算
    loss.backward()

    # パラメータ修正
    optimizer.step()

    #勾配値初期化
    optimizer.zero_grad()

    # 損失値の記録
    if (epoch %10 == 0):
        item = np.array([epoch, loss.item()])
        history = np.vstack((history, item))
        print(f'epoch = {epoch}  loss = {loss:.4f}')
            </code></pre>
            <p>最適化関数を利用してパラメータ値を間接的に変更している。</p>
            <b>最適化関数のチューニング</b>
            <p>最適化関数を導入することで、学習に関して色々なチューニングを簡単にできるようになる。</p>
            <pre><code>
# 初期化

# WとBを変数として扱う
W = torch.tensor(1.0, requires_grad=True).float()
B = torch.tensor(1.0, requires_grad=True).float()

# 繰り返し回数
num_epochs = 500

# 学習率
lr = 0.001

# optimizerとしてSGD(確率的勾配降下法)を指定する
import torch.optim as optim
optimizer = optim.SGD([W, B], lr=lr, momentum=0.9)

# 記録用配列初期化
history2 = np.zeros((0, 2))
            </code></pre>
            <p>momentum=0.9というオプションを、最適化関数のインスタンスoptimizerの生成時に設定している。</p>
            <p>これにより、学習の速度を速くすることができる。</p>
        </section>
        <section id="pf">
            <h2>Definition of the Prediction Function</h2>
            <p>PyTorchでは、予測関数を細かい機能に分け、機能の一つ一つに対応する部品を用意し、その部品を組み合わせることで、複雑な関数を作る（ビルディングブロック）。</p>
            <p>この部品のことを、「レイヤー関数」と呼ぶ。</p>
            <b>レイヤー関数：</b>
            <p>テンソルを入力とし、テンソルを出力とする関数群。機械学習モデルは、レイヤー関数を部品として、これを組み合わせて作る。ReLU関数のような活性化関数もレイヤー関数の一種とする。</p>
            <b>パラメータ：</b>
            <p>レイヤー関数の内部でも持っている、入力テンソル以外のデータ。学習とはレイヤー関数のパラメータ値を調整することを意味する。すべてのレイヤー関数がパラメータを持つわけではない。</p>
            <b>入力テンソル：</b>
            <p>機械学習モデル全体を１つの関数（合成関数）とみなした場合、関数の入力となるテンソル。ニューラルネットワークの概念では、「入力層」に該当する。</p>
            <b>出力テンソル：</b>
            <p>機械学習もでる全体を１つの関数（合成関数）とみなした場合、関数の出力となるテンソル。ニューラルネットワークの概念では、「出力層」に該当する。</p>
            <b>機械学習モデル：</b>
            <p>複数のレイヤー関数を組み合わせて（１つの場合もある）、入力テンソルに対して望ましい（正解データにできるだけ近い）出力テンソルを出力する合成関数。</p>
            <b>学習：</b>
            <p>レイヤー関数内部のパラメータ値を、望ましい出力テンソルが得られるように調整すること。具体的な手段として、勾配降下法などの最適化関数が用いられる。</p>
            <h3>予測関数の内部構造</h3>
            <p>まず、部品となるレイヤー関数をインスタンスとして定義する。</p>
            <pre><code>
# レイヤー関数定義

# 最初の線形関数
# 784 入力数
# 128 出力数
l1 = nn.Linear(784, 128)

# 2番目の線形関数
# 128 入力数
# 10 出力数
l2 = nn.Linear(128, 10)

# 活性化関数
relu = nn.ReLU(inplace=True)
            </code></pre>
            <p>２つの線形関数と、１つの活性化関数（ReLU関数）を定義している。</p>
            <p>最初の線形関数の２つ目のパラメータ値（128）が、２番目の線形関数の最初のパラメータ値と等しい。</p>
            <p>この128という値が、「隠れ層」のノード数に該当することになる。</p>
            <p>この３つの関数を組み合わせて入力から出力を得る。</p>
            <pre><code>
# 入力テンソルから出力テンソルを計算

# ダミー入力データを作成
inputs = torch.randn(100, 784)

# 中間テンソル1の計算
m1 = l1(inputs)

# 中間テンソル2の計算
m2 = relu(m1)

# 出力テンソルの計算
outputs = l2(m2)

# 入力テンソルと出力テンソルのshape確認
print('入力テンソル', inputs.shape)
print('出力テンソル', outputs.shape)
            </code></pre>
            <p>最初に100行、784列の2階テンソル（行列）を作っている。</p>
            <p>機械学習では複数件のデータを同時に扱うことが原則である。</p>
            <p>入力テンソルの最初のインデックスは常に「複数あるデータのうち何番目のデータか」を意味している。</p>
            <p>[100, 784]というshapeは「784個の要素を持つ１階テンソル（ベクトル）のデータが100件ある」と読む。</p>
            <p>直線状につながる合成関数は、nn.Sequentialという部品を利用してより簡単に実装できる。</p>
            <pre><code>
# nn.Sequentialを使って、全体を合成関数として定義

net2 = nn.Sequential(
    l1,
    relu,
    l2
)

outputs2 = net2(inputs)

# 入力テンソルと出力テンソルのshape確認
print('入力テンソル', inputs.shape)
print('出力テンソル', outputs2.shape)
            </code></pre>
            <p>PyTorchによる機械学習プログラムは「予測関数」「損失関数」「最適化関数」の３つのパートに分けることが可能である。</p>
            <p>繰り返し処理の順番という観点でいうと、「予測計算」「損失計算」「勾配計算」「パラメータ修正」を繰り返すという形になる。</p>
            <h3>活性化関数の目的</h3>
            <p>単に線形関数を合成しただけの関数は、結局１階層の線形関数と同じである。</p>
            <p>「非線形関数」と呼ばれる活性化関数を線形関数の間に入れることにより初めて深い階層のディープラーニングモデルが意味を持つ。</p>
            <p>活性化関数にはもう一つ、線形関数の出力を整形するという役割がある。</p>
            <p>具体的には、2値分類モデルではシグモイド関数を、多値分類モデルではsoftmax関数をこの目的で利用し、モデルの出力値を0から1の値を持つ「確率値」にする。</p>
        </section>
        <section id="lr">
            <h2>Linear Regression</h2>
            <h3>問題の定義</h3>
            <p><a href="https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html#:~:text=This%20dataset%20contains%20information">「ボストン・データセット」</a>を利用する。</p>
            <p>ボストン近郊を506の地域に分割し、それぞれの地域で様々な観点の統計情報を取得している。</p>
            <p>その中の１項目に「住宅平均価格」があり、「他の項目から住宅平均価格を予測する」という回帰モデルの題材としてよく利用される。</p>
            <p>まずは、入力項目のうち「平均部屋数」を意味するRMという項目を使って、目的関数にあたる不動産価格を予測する、単回帰と呼ばれるモデルを作る。</p>
            <p>その後、もう一つ「低所得者率」を意味するLSTATという項目も追加し、2入力1出力のモデルを作る。</p>
            <p>このモデルは、「単回帰」に対して「重回帰」と呼ばれる。両者を合わせて「線形回帰」と呼ぶ。</p>
            <p>１次関数はレイヤー関数でいうと、線形関数（nn.Linear）に該当する。</p>
            <p>線形回帰モデルは、他のレイヤー関数との組み合わせなしに、nn.Linearというう単独のレイヤー関数でモデルを実装できる。</p>
            <h3>線形関数</h3>
            <p>インスタンスを生成するコードは以下の通り。</p>
            <pre><code>
# 入力：２、出力：３の線形関数の定義
l3 = nn.Linear(2, 3)
            </code></pre>
            <p>このレイヤー関数はインスタンス生成時、２つの引数をとる。</p>
            <p>最初の引数は入力テンソルの次元数を、次の引数は出力テンソルの次元数を意味する。</p>
            <p>つまり、この例だと、２次元テンソルを入力として３次元テンソルを出力とする関数になる。</p>
            <b>１入力１出力</b>
            <pre><code>
# 乱数の種固定
torch.manual_seed(123)

# 入力:1 出力:1 の線形関数の定義
l1 = nn.Linear(1, 1)

# 線形関数の表示
print(l1)
            </code></pre>
            <p>レイヤー関数には、nn.Linear以外の関数うを含め、統一的にnamed_parametersという関数が組み込まれている。</p>
            <p>この関数を呼び出すと、（パラメータ名、パラメータ参照）のリストを返す。</p>
            <p>変数l1内にどのようなパラメータがあり、どのような値とshapeを持っているか調べる。</p>
            <pre><code>
# パラメータ名、パラメータ値、shapeの表示

for param in l1.named_parameters():
    print('name: ', param[0])
    print('tensor: ', param[1])
    print('shape: ', param[1].shape)
            </code></pre>
            <p>結果から、[1, 1]というshapeを持つ変数weightと、[1]というshapeを持つ変数biasがあることが分かる。</p>
            <p>weightは「重み」（１次関数の係数）を、biasはバイアス（１次関数の定数項）を意味する。</p>
            <p>今回は、入力も出力も１次元なので、weightもbiasも本来ならスカラー（０階テンソル）で問題ないはずである。</p>
            <p>わざわざ[1, 1]という行列、[1]というベクトルになっているのは、入力・出力テンソルの次元数が２以上に増えた時も簡単に拡張できるようにするためである。</p>
            <p>２つのパラメータでは、requires=Trueになっている。通常、レイヤー関数内のパラメータは学習対象である。</p>
            <p>weightとbiasはともにランダムな値に設定されている。</p>
            <p>通常乱数値がセットされるパラメータに明示的な値を設定したい場合、次のコードのように、nn.init.constant_を呼び出す（ここでは、\(y = 2x + 1\)としてみる）。</p>
            <pre><code>
# 初期値設定
nn.init.constant_(l1.weight, 2.0)
nn.init.constant_(l1.bias, 1.0)

# 結果確認
print(l1.weight)
print(l1.bias)
            </code></pre>
            <p>ダミーデータをこの関数にかけて、１次関数として動作していることを確認する。</p>
            <pre><code>
# テスト用データ生成

# x_npをnumpy配列で定義(x_np = [-2. -1. 0. 1. 2.])
x_np = np.arange(-2, 2.1, 1)

# Tensor化
x = torch.tensor(x_np).float()

# サイズを(N,1)に変更
x = x.view(-1,1)

# 結果確認
print(x.shape)
print(x)
            </code></pre>
            <p>weightが[1, 1]の行列であるため、入力変数xは[5, 1]の２次元テンソルに変換している。</p>
            <b>２入力１出力</b>
            <p>線形関数\(y = x_1 + x_2 + 2\)を作る。</p>
            <pre><code>
# 2次元numpy配列
x2_np = np.array([[0, 0], [0, 1], [1, 0], [1,1]])

# Tensor化
x2 =  torch.tensor(x2_np).float()

# 結果確認
print(x2.shape)
print(x2)
            </code></pre>
            <p>以下のコードで４行２列のテストデータを用意する。</p>
            <pre><code>
# 2次元numpy配列
x2_np = np.array([[0, 0], [0, 1], [1, 0], [1,1]])

# Tensor化
x2 =  torch.tensor(x2_np).float()

# 結果確認
print(x2.shape)
print(x2)
            </code></pre>
            <p>以下のように関数呼び出しを行う。</p>
            <pre><code>
# 関数値計算
y2 = l2(x2)

# shape確認
print(y2.shape)

# 値確認
print(y2.data)
            </code></pre>
            <b>２入力３出力</b>
            <pre><code>
# 入力:2 出力:3 の線形関数の定義

l3 = nn.Linear(2, 3)

# 初期値設定
nn.init.constant_(l3.weight[0,:], 1.0)
nn.init.constant_(l3.weight[1,:], 2.0)
nn.init.constant_(l3.weight[2,:], 3.0)
nn.init.constant_(l3.bias, 2.0)

# 結果確認
print(l3.weight)
print(l3.bias)
            </code></pre>
            <pre><code>
# 関数値計算
y3 = l3(x2)

# shape確認
print(y3.shape)

# 値確認
print(y3.data)
            </code></pre>
            <h3>カスタムクラスを利用したモデル定義</h3>
            <pre><code>
# モデルのクラス定義

class Net(nn.Module):
    def __init__(self, n_input, n_output):
        #  親クラスnn.Modulesの初期化呼び出し
        super().__init__()

        # 出力層の定義
        self.l1 = nn.Linear(n_input, n_output)

    # 予測関数の定義 
    def forward(self, x):
        x1 = self.l1(x) # 線形回帰
        return x1
            </code></pre>
            <p>このコードは、機械学習モデル定義のコードの一番本質的な部分である。</p>
            <p>Netというクラスの親クラスは、nn.Moduleである。</p>
            <p>クラスの内部にはforward関数が定義されていて、この関数で予測処理を実装する。</p>
            <p>予測は次のように行う。</p>
            <pre><code>
# ダミー入力
inputs = torch.ones(100,1)

# インスタンスの生成 (１入力1出力の線形モデル)
n_input = 1
n_output = 1
net = Net(n_input, n_output)

# 予測
outputs = net(inputs)
            </code></pre>
            <p>カスタムクラスのインスタンス変数netは自分自身が関数として動く。</p>
            <h3>MSELossクラスを利用した損失関数</h3>
            <p>「損失loss」とは、「予測関数」と「損失関数」を組み合わせてできあがった合成関数で、「損失計算」とは、この合成関数を計算することを意味する。</p>
            <p>この合成関数（損失）はパラメータ（weightとbias）を引数とする。</p>
            <p>合成関数（損失）をパラメータで偏微分することが「勾配計算」であり、勾配計算の結果が勾配降下法の「パラメータ修正」で用いられる。</p>
            <pre><code>
# 損失関数：平均２乗誤差
criterion = nn.MSELoss()
            </code></pre>
            <p>このコードでは、初期化処理の中で損失関数criterionを、クラスnn.MSELossのインスタンスとして定義している。</p>
            <pre><code>
# 誤差計算
loss = criterion(outputs, labels1) / 2.0
            </code></pre>
            <p>このコードでは、繰り返し処理の中で、損失関数を出力テンソルoutputsと正解テンソルlabelsの２つをパラメータとして取る関数として呼び出し、さらに2.0で割った結果を損失lossに代入している。これが「損失計算」である。</p>
            <p>また、損失lossに対してbackward関数を呼び出し、勾配計算をしている。</p>
            <h3>データ準備</h3>
            <pre><code>
# 学習用データ準備
                
data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+",
    skiprows=22, header=None)
x_org = np.hstack([raw_df.values[::2, :],
    raw_df.values[1::2, :2]])
yt = raw_df.values[1::2, 2]
feature_names = np.array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX',
    'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT'])

# 結果確認
print('元データ', x_org.shape, yt.shape)
print('項目名: ', feature_names)
            </code></pre>
            <pre><code>
# データ絞り込み (項目 RMのみ)
x = x_org[:,feature_names == 'RM']
print('絞り込み後', x.shape)
print(x[:5,:])

# 正解データ yの表示
print('正解データ')
print(yt[:5])
            </code></pre>
            <pre><code>
# 散布図の表示

plt.scatter(x, yt, s=10, c='b')
plt.xlabel('部屋数')
plt.ylabel('価格')
plt.title('部屋数と価格の散布図')
plt.show()
            </code></pre>
            <h3>モデル定義</h3>
            <b>変数定義</b>
            <pre><code>
# 変数定義

# 入力次元数
n_input= x.shape[1]

# 出力次元数
n_output = 1

print(f'入力次元数: {n_input}  出力次元数: {n_output}')
            </code></pre>
            <p>機械学習・ディープラーニングモデルとは、「入力ベクトルに対して出力ベクトルを返す関数」である。</p>
            <p>今回のモデルは、１入力１出力のとてもシンプルなモデルである。</p>
            <b>機械学習モデル（予測モデル）のクラス定義</b>
            <pre><code>
# 機械学習モデル（予測モデル）クラス定義

class Net(nn.Module):
    def __init__(self, n_input, n_output):
        #  親クラスnn.Modulesの初期化呼び出し
        super().__init__()

        # 出力層の定義
        self.l1 = nn.Linear(n_input, n_output)

        # 初期値を全部1にする
        nn.init.constant_(self.l1.weight, 1.0)
        nn.init.constant_(self.l1.bias, 1.0)

    # 予測関数の定義
    def forward(self, x):
        x1 = self.l1(x) # 線形回帰
        return x1
            </code></pre>
            <p>PyTorchでは、モデル用のクラスの内部で必ずforward関数を定義し、入力テンソルinputsを入力として出力テンソルoutputsを出力するための処理を記述するルールになっている。</p>
            <b>インスタンス生成</b>
            <pre><code>
# インスタンスの生成
# １入力1出力の線形モデル

net = Net(n_input, n_output)
            </code></pre>
            <p>Netクラスのインスタンスであるnet変数を定義すると、次の呼び出し方で入力テンソルから、予測値である出力テンソルを取得できる。</p>
            <pre><code>
outputs = net(inputs)
            </code></pre>
            <b>モデル内の変数値表示</b>
            <p>このように生成したモデルを表す変数netでは、親クラスのnn.Module内で定義されている便利な機能（関数）を利用可能である。</p>
            <p>その一つとして、モデル内の変数名とその値を取得する関数named_parametersがある。</p>
            <pre><code>
# モデル内のパラメータの確認
# モデル内の変数取得にはnamed_parameters関数を利用する
# 結果の第1要素が名前、第2要素が値
#
# predict.weightとpredict.biasがあることがわかる
# 初期値はどちらも1.0になっている

for parameter in net.named_parameters():
    print(f'変数名: {parameter[0]}')
    print(f'変数値: {parameter[1].data}')
            </code></pre>
            <b>parameters関数の呼び出し</b>
            <p>もう一つの便利機能として、parameters関数もある。</p>
            <p>parameters関数は、「パラメータ変数」だけが名前なしにリスト形式で返ってくる。</p>
            <p>最適化関数のインスタンス生成において、最適化対象のパラメータをリストで渡すときによく使われる。</p>
            <pre><code>
# パラメータのリスト取得にはparameters関数を利用する

for parameter in net.parameters():
    print(parameter)
            </code></pre>
            <b>モデルの概要表示</b>
            <pre><code>
# モデルの概要表示

print(net)
            </code></pre>
            <pre><code>
# モデルのサマリー表示

from torchinfo import summary
summary(net, (1,))
            </code></pre>
            <p>summaryという関数を呼び出すが、引数としてnet変数そのものと、入力変数のサイズを指定する。</p>
            <b>損失関数と最適化関数の定義</b>
            <pre><code>
# 損失関数： 平均2乗誤差
criterion = nn.MSELoss()

# 学習率
lr = 0.01

# 最適化関数: 勾配降下法
optimizer = optim.SGD(net.parameters(), lr=lr)
            </code></pre>
        </section>
        <section id="binary">
            <h2>Binary Classification</h2>
            
        </section>
        <section id="multi">
            <h2>Multiclass Classification</h2>
            
        </section>
        <section id="mnist">
            <h2>MNIST</h2>
            
        </section>
        <section id="cnn">
            <h2>CNN</h2>
            
        </section>
        <section id="tuning">
            <h2>Tuning</h2>
            
        </section>
        <section id="pretrain">
            <h2>Pretrained Model</h2>
            
        </section>
        <section id="custom">
            <h2>Custom Data</h2>
            
        </section>
    </main>
</body>
</html>
