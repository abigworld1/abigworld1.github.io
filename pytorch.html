<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【深層学習 | PyTorchの基本】走る作曲家のAIカフェ</title>
    <link rel="stylesheet" href="style.css"> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1050827580219099"
     crossorigin="anonymous"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
            <h2>目次</h2>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#source">Source</a></li>
                <li><a href="#python">Python For Deep Learning</a></li>
                <li><a href="#pytorch">Basics of PyTorch</a></li>
                <li><a href="#ml">Introduction to Machine Learning</a></li>
                <li><a href="#pf">Definition of the Prediction Function</a></li>
                <li><a href="#lr">Linear Regression</a></li>
                <li><a href="#binary">Binary Classification</a></li>
                <li><a href="#multi">Multiclass Classification</a></li>
                <li><a href="#mnist">MNIST</a></li>
                <li><a href="#cnn">CNN</a></li>
                <li><a href="#tuning">Tuning</a></li>
                <li><a href="#pretrain">Pretrained Model</a></li>
                <li><a href="#custom">Custom Data</a></li>
            </ul>
        </section>
        <section id="overview">
            <h2>Overview</h2>
            深層学習とは、人工ニューラルネットワークを多層に積み重ねて学習させる手法で、複雑なデータのパターンや特徴を自動的に抽出することができる機械学習の一分野です。<br>
            このページでは、深層学習のライブラリであるPyTorchを用いて、深層学習の基礎を学んでいきます。
        </section>
        <section id="source">
            <h2>Source</h2>
            <p>以下の講義・書籍を参考にしました。</p>
            <ul>
                <li><a href="https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI">MIT 6.S191: Introduction to Deep Learning</a></li>
                <li><a href="https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html">Quickstart — PyTorch Tutorials 2.4.0+cu121 documentation</a></li>
                <li><a href="https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/af0caf6d7af0dda755f4c9d7af9ccc2c/quickstart_tutorial.ipynb">quickstart_tutorial.ipynb</a></li>
                <li><a href="https://yutaroogawa.github.io/pytorch_tutorials_jp/">PyTorchチュートリアル（日本語翻訳版）</a></li>
                <li><a href="https://github.com/makaishi2/pytorch_book_info">makaishi2_pytorch_book_info_ 書籍「最短コースでわかるPyTorch深層学習プログラミング」用サポートサイト</a></li>
                <li><a href="https://www.amazon.co.jp/%E6%9C%80%E7%9F%AD%E3%82%B3%E3%83%BC%E3%82%B9%E3%81%A7%E3%82%8F%E3%81%8B%E3%82%8B-PyTorch-%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-%E8%B5%A4%E7%9F%B3-%E9%9B%85%E5%85%B8/dp/4296110322#customerReviews">最短コースでわかる PyTorch &深層学習プログラミング</a></li>
            </ul>     
        </section>
        <section id="python">
            <h2>Python For Deep Learning</h2>
            <h3>コンテナデータ型</h3>
            <p>コンテナデータ型：「リスト」やNumPy配列のように、名前から実際のデータにアクセスするのにインデックスを経由する必要があるデータの型。</p>
            <p>
                <pre><code>
x = np.array([5, 7, 9])
y = x
x[1] = -1
print(y)
                </code></pre>
            この出力は、<code2>[5, -1, 9]</code2>となる。xの要素の変更がyに影響しないようにするには、<code2>y = x.copy()</code2>を使う。</p>
            <p>PyTorchで扱うデータは「テンソル」(Tensor)というクラスのインスタンスに保存される。</p>
            <p>テンソルの場合も同様にcopyしておけば他からの影響を気にしないで済む。</p>
            <h3>カスタムクラス定義</h3>
            <b>オブジェクト指向の基礎概念</b>
            <p>クラス：「型」。</p>
            <p>インスタンス：「型」から生成された個別の実体。</p>
            <p>クラスは、「属性」と呼ばれるクラス内の変数を持っている。</p>
            <p>「関数」あるいは「メソッド」と呼ばれる処理機能も持っている。</p>
            <p>属性と呼ばれるクラス内の変数の値は、インスタンスごとに異なる。</p>
            <b>最初のクラス定義</b>
            <p>例）Pointというクラスを定義してみる。</p>
            <p>Pointクラスの属性のxとyは、点のx座標とy座標である。</p>
            <p>また、関数drawは、自分を点としてグラフに表示する関数である。</p>
            <p>以下のように実装される。</p>
            <pre><code>
import matplotlib.pyplot as plt

# 円描画に必要なライブラリ
import matplotlib.patches as patches

# クラスPointの定義
Class Point:
    # インスタンス生成時にxとyの２つの引数を持つ
    def __init__(self, x, y):
        # インスタンスの属性xに第１引数をセットする
        self.x = x
        # インスタンスの属性yに第２引数をセットする
        self.y = y
    # 描画関数drawの定義（引数なし）
        def draw(self):
            # (x, y)に点を描画する
            plt.plot(self.x, self.y, marker='o', markersize=10, c='k')
            </code></pre>
            <p><code2>__init__</code2>は初期化処理。</p>
            <p><code2>self</code2>はクラスからインスタンスが生成された際、インスタンス自身を指す。</p>
            <b>最初のインスタンス生成</b>
            <pre><code>
# クラスPointからインスタンス変数p1とp2を生成する
p1 = Point(2, 3)
p2 = Point(-1, -2)
            </code></pre>
            <p>引数リストの引数は、__init__関数の定義の引数からselfを取り除いたもの。</p>
            <b>インスタンス属性へのアクセス</b>
            <pre><code>
# p1とp2の属性x、yの参照
print(p1.x, p1.y)
print(p2.x, p2.y)
            </code></pre>
            <b>draw関数の呼び出し</b>
            <pre><code>
# p1とp2のdraw関数を呼び出し、2つの点を描画する
p1.draw()
p2.draw()
plt.xlim(-4, 4)
plt.ylim(-4, 4)
plt.show()
            </code></pre>
            <b>Circleクラスの定義</b>
            <p>中心点のx座標とy座標の他に半径を意味する属性rを持つ。</p>
            <p>xとyについてはPointクラスの定義を再利用できる（「クラスの継承」）。</p>
            <pre><code>
# Pointの子クラスCircleの定義
class Circle(Point)
    # Circleはインスタンス生成時に引数x、y、rを持つ
    def __init__(self, x, y, r):
        # xとyは、親クラスの属性として設定
        super().__init__(x, y)
        # rは、Circleの属性として設定
        self.r = r

    # Circleのdraw関数は、親の関数呼び出しのあとで、円の描画も独自に行う
    def draw(self):
        # 親クラスのdraw関数呼び出し
        super().draw()
                
        # 円の描画
        c = patches.Circle(xy=(self.x, self.y), radius=self.r, fc='b', ec='k')
        ax.add_patch(c)
            </code></pre>
            <p>CircleクラスはPointクラスの子クラスとして定義している。</p>
            <b>Circleインスタンスの生成とdraw関数の呼び出し</b>
            <pre><code>
# クラスCircleからインスタンス変数c1を生成する
c1 = Circle(1, 0, 2)

# p1, p2, c1のそれぞれのdraw関数を呼び出す
ax = plt.subplot()
p1.draw()
p2.draw()
c1.draw()
plt.xlim(-4, 4)
plt.ylim(-4, 4)
plt.show()
            </code></pre>
            <p>親クラスと同じ名前の関数を子クラスでも定義して振る舞いを変更させることを「オーバーライド」と呼ぶ。</p>
            <h3>インスタンスを関数として扱う</h3>
            <p>クラスから生成したインスタンスを呼び出し可能な関数にする。</p>
            <pre><code>
# 関数クラスHの定義
class H:
    def __call__(self, x):
        return 2*x**2 + 2
            </code></pre>
            <pre><code>
# hが関数として動作することを確認する

# NumPy配列としてxを定義
x = np.arange(-2, 2.1, 0.25)
print(x)

# Hクラスのインスタンスとしてhを生成
h = H()

# 関数hの呼び出し
y = h(x)
print(y)
            </code></pre>
        </section>
        <section id="pytorch">
            <h2>Basics of PyTorch</h2>
            <h3>重要な概念</h3>
            <p>PyTorchでは、テンソルという独自のクラスでデータを表現する。</p>
            <p>PyTorchの最大の特徴は、自動微分機能。</p>
            <h3>テンソル</h3>
            <b>ライブラリインポート</b>
            <pre><code>
import torch
            </code></pre>
            <b>色々な階層のテンソル</b>
            <pre><code>
# 0階テンソル（スカラー）
r0 = torch.tensor(1.0).float()
            </code></pre>
            <p>テンソル変数を作るのに一番簡単なのは、torch.tensor関数を使う方法。</p>
            <p>テンソル変数の生成時には、必ず後ろにfloat関数の呼び出しをつけて、dtype（テンソル変数の要素のデータ型）を強制的にfloat32に変換する。</p>
            <pre><code>
# 1階テンソル（ベクトル）

# 1階のNumPy変数作成
r1_np = np.array([1, 2, 3, 4, 5])

# NumPyからテンソルに変換
r1 = torch.tensor(r1_np).float()

# dtypeを調べる
print(r1.dtype)

# shapeを調べる
print(r1.shape)

# データを調べる
print(r1.data)
            </code></pre>
            <pre><code>
# 2階テンソル（行列）

# 2階のNumPy変数作成
r2_np = np.array([1, 5, 6], [4, 3, 2])

# NumPyからテンソルに変換
r2 = torch.tensor(r2_np).float()
            </code></pre>
            <pre><code>
# 3階テンソル

# 乱数seedの初期化
torch.manual_seed(123)

# shape=[3,2,2]の正規分布変数テンソルを作る
r3 = torch.randn((3, 2, 2))
            </code></pre>
            <b>整数値のテンソル</b>
            <p>PyTorchによる計算では、そのほとんどの場合、数値型としてdtype=float32を利用する。</p>
            <p>しかし、「多値分類」用の損失関数である、nn.CrossEntropyLossとnn.NLLLossは、損失関数呼び出し時に、第2引数に整数型を指定する必要がある。</p>
            <p>その際は、以下のように整数型へ変換する。</p>
            <pre><code>
r5 = r1.long()
            </code></pre>
            <p>このようにlong関数をかけると、dtype=torch.int64になる。</p>
            <b>view関数</b>
            <p>view関数は、NumPyのreshape関数に相当し、変数の階数を変換することができる。</p>
            <pre><code>
# 2階化（r3は[3,2,2]の3階テンソル）
# 要素数に-1を指定すると、この数を自動調整する
r6 = r3.view(3, -1)
            </code></pre>
            <p>結果として、[3, 4]の2階テンソルが得られる。</p>
            <pre><code>
# 1階化
r7 - r3.view(-1)
            </code></pre>
            <b>それ以外の属性</b>
            <p>テンソルはデータとして扱うことも、クラスとして扱うこともできる。</p>
            <pre><code>
# requires_grad属性
print('requires_grad: ', r1.requires_grad)

# device属性
print('device: ', r1.device)
            </code></pre>
            <b>item関数</b>
            <p>スカラー（0階テンソル）に対しては、テンソルからPython本来のクラスの数値（floatまたはint）を取り出すのにitem関数が使える。</p>
            <p>計算結果テンソルとしてのlossから、データ記録用に値だけを抽出する場合によく用いる。</p>
            <pre><code>
item = r0.item()
            </code></pre>
            <p>この関数は、1階以上のテンソルを対象にできない。</p>
            <b>max関数</b>
            <pre><code>
# max関数を引数無しで呼び出すと、全体の最大値が取得できる
print(r2.max())
            </code></pre>
            <pre><code>
# torch.max関数
# 2つ目の引数はどの軸で集約するかを意味する（軸=1: 行方向、軸=0: 列方向）
print(torch.max(r2, 1))
            </code></pre>
            <p>この呼び出し方をした場合、最大値の値そのものだけでなく、どのindexで最大値を取ったかも返ってくる。</p>
            <p>後ろに[1]をつけると、後者のみを抽出できる。</p>
            <pre><code>
# 何番目の要素が最大値を取るかは、indicesを調べると良い
# 以下の計算は、多値分類で予測ラベルを求めるときによく利用される
print(torch.max(r2, 1)[1])
            </code></pre>
            <p>「複数の予測器の出力のうち、最も大きな値を出した予測器のindexが予測結果ラベルになる」ことを実装している。</p>
            <b>NumPy変数への変換</b>
            <pre><code>
# NumPy化
r2_np = r2.data.numpy()
            </code></pre>
            <p>ただし、この方法だと、テンソル変数とNumPy配列は同じデータを指すため、テンソル側で値を変えるとNumPyも連動して値が変わる。</p>
            <p>連動しないようにするには、r2.data.numpy.copy()として、データのコピーを作る。</p>
            <h3>自動微分機能</h3>
            <p>PyTorchで自動微分を行う場合の処理の流れ：</p>
            <ol>
                <li>勾配計算用変数の定義：requires_grad=Trueとする</li>
                <li>テンソル変数間で計算：裏で計算グラフが自動生成される</li>
                <li>計算グラフの可視化：make_dot関数</li>
                <li>勾配計算：backward関数</li>
                <li>勾配値の取得：grad属性</li>
                <li>勾配値の初期化：zero_関数</li>
            </ol>
            <b>1. 勾配計算用変数の定義</b>
            <p>requires_grad属性をTrueに設定する。</p>
            <b>2. テンソル変数間で計算</b>
            <p>他のテンソル変数との間で演算する。</p>
            <p>このとき、計算式による値の計算が行われるのと同時に、裏で「計算グラフ」が生成される。</p>
            <p>この機能は「Define by Run」と呼ばれる。</p>
            <p>「計算グラフ」とは、データとそれに対する演算の順番を定義するもの。</p>
            <b>3. 計算グラフの可視化</b>
            <p>PyTorch内部の動きを確認するために行う。</p>
            <p>make_dotという関数を使うと、2.で自動生成された計算グラフを可視化できる。</p>
            <b>4. 勾配計算</b>
            <p>勾配計算は、計算結果を保存したテンソル変数（スカラー）に対して、backward関数を呼び出すことにより行われる。</p>
            <b>5. 勾配値の取得</b>
            <p>勾配計算の結果は、PyTorchでは勾配値と呼ばれる。</p>
            <p>勾配値はテンソル変数のgrad属性により取得できる。</p>
            <b>6. 勾配値の初期化</b>
            <p>grad属性に保存されている勾配値は、利用が終わったら値を初期化する必要がある。</p>
            <p>そのための関数がzero_関数である。</p>
            <h3>2次関数の勾配計算</h3>
            <p>例）\(y = 2x^2+2\)に対して自動微分計算を行う。</p>
            <b>1. 勾配計算用変数の定義</b>
            <pre><code>
# xをNumPy配列で定義
x_np = np.arange(-2, 2.1, 0.25)
            </code></pre>
            <pre><code>
# 1. 勾配計算用変数の定義

# requires=Trueとする
x = torch.tensor(x_np, requires_grad=True, dtype=torch.float32)
            </code></pre>
            <b>2. テンソル変数間で計算</b>
            <pre><code>
# 2次関数の計算
# 裏で計算グラフが自動生成される

y = 2 * x**2 + 2
            </code></pre>
            <p>このとき、変数yは自動的にテンソル変数になる。</p>
            <p>requires_grad属性がTrueのテンソル変数は、そのままではMatplotlibで使えないが、data属性を渡すとグラフ表示が可能。</p>
            <pre><code>
# グラフ描画

plt.plot(x.data, y.data)
plt.show()
            </code></pre>
            <p>勾配計算の対象はスカラーである必要があるため、yの値をsum関数ですべて足して、足した結果を新しいテンソル変数zに代入する。</p>
            <p>（「なぜ和をとることで微分計算ができるのか」についての説明は<a href = "https://qiita.com/makaishi2/items/a6cf19add4b6d16b8483">こちら</a>。）</p>
            <pre><code>
# 勾配計算のため、sum 関数で 1階テンソルの関数値をスカラー化する
# (sum 関数を各要素で偏微分した結果は1なので、元の関数の微分結果を取得可能 )
z = y.sum()
            </code></pre>
            <b>3. 計算グラフの可視化</b>
            <p>この変数zを使って、計算グラフの可視化を行う。</p>
            <pre><code>
# 3. 計算グラフの可視化

# 必要なライブラリのインポート
from torchviz import make_dot

# 可視化関数の呼び出し
g = make_dot(z, params={'x': x})
display(g)
            </code></pre>
            <p>make_dot関数の呼び出し時の第1引数として、可視化したい計算グラフの対象となる変数（今回の場合z）、第2引数のparamsとして微分計算対象の変数（今回の場合x）を辞書形式のパラメータリストにして渡す。</p>
            <p>出発点のxをテンソル変数とし、かつrequires_gradフラグをセットしておくと、計算の過程が自動的に記録される。</p>
            <p>値を計算しながら、計算過程を自動的に記録する機能は「Define by Run」と呼ばれる。</p>
            <p>計算グラフにおいて、青色のノードはmake_dot関数呼び出し時にparamsで指定した変数に該当し、リーフノードと呼ばれる。</p>
            <p>勾配値の計算が可能な変数を意味している。</p>
            <p>緑色のノードは出力ノード、灰色のノードは中間処理という意味になっている。</p>
            <p>一番上と一番下のノードに記載される()は、それぞれの変数のshapeを示している。</p>
            <p>(17)であれば、1階17次元のテンソル、()であれば、0階のスカラーということになる。</p>
            <p>"AccumulateGrad"は、末端のリーフノードの直下に配置され、勾配値を蓄積する場所を示している。</p>
            <p>計算グラフがあれば、PyTorch側で\(f(x) = 2x^2 + 2\)という2次関数を、指数関数、乗算関数、加算関数という基本的な関数の合成関数として認識できる。</p>
            <b>4. 勾配計算</b>
            <p>backward関数を呼び出すだけ。</p>
            <pre><code>
# 4. 勾配計算
z.backward()
            </code></pre>
            <b>5. 勾配値の取得</b>
            <pre><code>
print(x.grad)
            </code></pre>
            <b>6. 勾配値の初期化</b>
            <p>x.gradには最新の勾配計算の結果が入るのではなく、それまでの勾配計算結果を加算した値が入る。</p>
            <p>条件を変えて新しく勾配値を取得したい場合、勾配値の初期化が必要になる。</p>
            <pre><code>
# 6. 勾配の初期化は関数zero_()を使う
x.grad.zero_()
            </code></pre>
            <h3>シグモイド関数の勾配計算</h3>
            <b>シグモイド関数の定義</b>
            <pre><code>
# シグモイド関数の定義
sigmoid = torch.nn.Sigmoid()
            </code></pre>
            <b>2. テンソル変数でyの値を計算</b>
            <pre><code>
# 2. yの値の計算
y = sigmoid(x)
            </code></pre>
            <b>グラフ描画</b>
            <pre><code>
plt.plot(x.data, y.data)
plt.show()
            </code></pre>
            <b>最終結果をスカラー化</b>
            <pre><code>
z = y.sum()
            </code></pre>
            <b>3. 計算グラフの可視化</b>
            <pre><code>
3. 計算グラフの可視化
g = make_dot(z, params={'x': x})
            </code></pre>
            <b>4. 勾配計算、5. 勾配値の取得</b>
            <pre><code>
# 4. 勾配計算
z.backward()

# 5. 勾配値の確認
print(x.grad)
            </code></pre>
            <b>グラフに表示</b>
            <pre><code>
# 元の関数と勾配のグラフ化

plt.plot(x.data, y.data, c='b', label='y')
plt.plot(x.data, x.grad.data, c='k', label='x.grad')
plt.legend()
plt.show()
            </code></pre>
        </section>
        <section id="ml">
            <h2>Introduction to Machine Learning</h2>
            <h3>問題の定義</h3>
            <p>与えられた身長から体重を予測する機械学習モデルを作る（線形回帰）。</p>
            <h3>勾配降下法</h3>
            <p>勾配降下法は、「予測計算」「損失関数」「勾配計算」「パラメータ修正」の４つのステップを繰り返すことで、予測関数の中のパラメータを最適な値に近づけることである。</p>
            <b>①予測計算</b>
            <p>入力テンソルXを入力とし、予測結果は出力テンソルYpに出力されるものとする。</p>
            <p>今回は、予測関数の実体は次のような１次関数である。</p>
            <div class="scroll">
                \begin{align}
                Yp = W * X + B
                \end{align}
            </div>
            <p>予測関数により予測値Ypを求めることを「①予測計算」と呼ぶ。</p>
            <b>②損失計算</b>
            <p>「教師あり学習」における学習データは通常、入力と正解値の両方を含んでいるので、正解値の列を分離して正解テンソルYとする。</p>
            <p>予測計算の結果であるYpと正解であるYの、2つのテンソルを入力とする「損失」lossを定義する。</p>
            <p>損失が最小になるようなWとBを見つけることが目標である。</p>
            <p>この損失を計算する過程が「②損失計算」である。</p>
            <p>損失関数は、予測関数の性質に応じて適したものを選ぶ。</p>
            <p>今回は値を予測する回帰モデルであるため、YとYpの差の2乗を利用することにする。</p>
            <p>正確には、すべてのデータ系列の差を2乗して、その平均をとった関数である「平均２乗誤差」を選択する。</p>
            <b>③勾配計算</b>
            <p>予測関数を構成するパラメータWとBの値を少しだけ変え、その時の損失の変化の度合（勾配）を調べる。これが「③勾配計算」である。</p>
            <p>損失関数の最低値を目指すために、WとBをずらすベストな方向が勾配にあたる。</p>
            <b>④パラメータ修正</b>
            <p>勾配に小さな定数（学習率）lrをかけ、その値だけWとBを同時に減らす。この操作が「④パラメータ修正」である。</p>
            <h3>データ前処理</h3>
            <pre><code>
# サンプルデータの宣言
sampleData1 = np.array([
    [166, 58.7],
    [176.0, 75.7],
    [171.0, 62.1],
    [173.0, 70.4],
    [169.0,60.1]
])
print(sampleData1)
            </code></pre>
            <p>学習データを入力データxと正解データyに分割する。</p>
            <pre><code>
# 機械学習モデルで扱うため、身長だけを抜き出した変数xと
# 体重だけを抜き出した変数yをセットする

x = sampleData1[:,0]
y = sampleData1[:,1]
            </code></pre>
            <p>データの散布図を表示する。</p>
            <pre><code>
# 散布図表示で状況の確認

plt.scatter(x,  y,  c='k',  s=50)
plt.xlabel('$x$: 身長(cm) ')
plt.ylabel('$y$: 体重(kg)')
plt.title('身長と体重の関係')
plt.show()
            </code></pre>
            <b>データの変換</b>
            <p>勾配降下法では、対象となる数値は絶対値が1以内に収まるような比較的小さな値の方が望ましい。</p>
            <p>今回の学習データは身長も体重も大きな数値なので、それぞれ平均値を引くことで勾配降下法がやりやすい条件に変換する。</p>
            <p>変換後のxとyをXとYで表す。</p>
            <pre><code>
X = x - x.mean()
Y = y - y.mean()
            </code></pre>
            <h3>予測計算</h3>
            <p>まず、変換後のXとYをテンソル変数に変換する。</p>
            <pre><code>
# XとYをテンソル変数化する

X = torch.tensor(X).float()
Y = torch.tensor(Y).float()
            </code></pre>
            <p>次に、１次関数の係数にあたる変数Wと定数項にあたる変数Bもテンソル変数として定義する。</p>
            <pre><code>
# 重み変数の定義
# WとBは勾配計算をするので、requires_grad=Trueとする

W = torch.tensor(1.0, requires_grad=True).float()
B = torch.tensor(1.0, requires_grad=True).float()
            </code></pre>
            <p>２つの変数には初期値として1.０の値を設定する。</p>
            <p>また、この２つの変数は勾配降下法の対象となるので、requires_grad属性をTrueに設定して自動微分ができるようにする。</p>
            <p>予測関数を定義し、予測値を計算する。</p>
            <pre><code>
# 予測関数は一次関数

def pred(X):
    return W * X + B

# 予測値の計算

Yp =  pred(X)
            </code></pre>
            <p>計算グラフを表示する。</p>
            <pre><code>
# 予測値の計算グラフ可視化

params = {'W': W, 'B': B}
g = make_dot(Yp, params=params)
display(g)
            </code></pre>
            <h3>損失計算</h3>
            <p>損失関数はMSE（平均２乗誤差）と呼ばれる方法で計算する。</p>
            <pre><code>
# 損失関数は誤差二乗平均

def mse(Yp, Y):
    loss = ((Yp - Y) ** 2).mean()
    return loss

# 損失計算

loss = mse(Yp, Y)
            </code></pre>
            <p>ここで得られた損失(loss)は、１次関数の係数Wと定数項Bの関数になっている。計算グラフで確認する。</p>
            <pre><code>
# 損失の計算グラフ可視化

params = {'W': W, 'B': B}
g = make_dot(loss, params=params)
display(g)
            </code></pre>
            <h3>勾配計算</h3>
            <p>backward関数を呼び出すだけ。</p>
            <pre><code>
# 勾配計算

loss.backward()

# 勾配値確認

print(W.grad)
print(B.grad)
            </code></pre>
            <h3>パラメータ修正</h3>
            <p>勾配計算ができたら、その値に一定の学習率lr（0.01や0.001）を掛けた結果を、もとのパラメータ値から引くのが勾配降下法の基本的な考え方である。</p>
            <pre><code>
# 学習率の定義

lr = 0.001

#  勾配を元にパラメータ修正

W -= lr * W.grad
B -= lr * B.grad
            </code></pre>
            <p>ただし、このコードではエラーが発生する。</p>
            <p>勾配計算をしている最中の変数は他に影響が及んでしまうため、勝手に値を修正できない。</p>
            <p>この場合、with torch.no_grad()というコンテキストを設定すると、そのコンテキストの内部では一時的に計算グラフ生成機能が止まり、変数の修正が可能になる。</p>
            <pre><code>
# 勾配を元にパラメータ修正
# with torch.no_grad() を付ける必要がある

with torch.no_grad():
    W -= lr * W.grad
    B -= lr * B.grad

    # 計算済みの勾配値をリセットする
    W.grad.zero_()
    B.grad.zero_()
            </code></pre>
            <p>勾配値を使ってパラメータ値を更新したあと、次の勾配計算の準備のため、zero_関数で勾配値の初期化もしている。</p>
            <h3>繰り返し計算</h3>
            <p>実際に繰り返し計算を行う。初期化のセルは以下の通り。</p>
            <pre><code>
# 初期化

# WとBを変数として扱う
W = torch.tensor(1.0, requires_grad=True).float()
B = torch.tensor(1.0, requires_grad=True).float()

# 繰り返し回数
num_epochs = 500

# 学習率
lr = 0.001

# 記録用配列初期化
history = np.zeros((0, 2))
            </code></pre>
            <p>ループ処理のセルは以下の通り。</p>
            <pre><code>
# ループ処理

for epoch in range(num_epochs):

    # 予測計算
    Yp = pred(X)

    # 損失計算
    loss = mse(Yp, Y)

    # 勾配計算
    loss.backward()

    with torch.no_grad():
        # パラメータ修正
        W -= lr * W.grad
        B -= lr * B.grad

        # 勾配値の初期化
        W.grad.zero_()
        B.grad.zero_()

    # 損失の記録
    if (epoch %10 == 0):
        item = np.array([epoch, loss.item()])
        history = np.vstack((history, item))
        print(f'epoch = {epoch}  loss = {loss:.4f}')
            </code></pre>
            <h3>結果評価</h3>
            <p>WとBの最終的な値と、損失の開始時、終了時の値を表示する。</p>
            <pre><code>
# パラメータの最終値
print('W = ', W.data.numpy())
print('B = ', B.data.numpy())

#損失の確認
print(f'初期状態: 損失:{history[0,1]:.4f}')
print(f'最終状態: 損失:{history[-1,1]:.4f}')
            </code></pre>
            <p>損失の減り方をグラフで可視化する（学習曲線）。</p>
            <pre><code>
# 学習曲線の表示 (損失)

plt.plot(history[:,0], history[:,1], 'b')
plt.xlabel('繰り返し回数')
plt.ylabel('損失')
plt.title('学習曲線(損失)')
plt.show()
            </code></pre>
            <p>求めたWとBの値から直線の式を算出し、散布図に重ね描きする。</p>
            <pre><code>
# xの範囲を求める(Xrange)
X_max = X.max()
X_min = X.min()
X_range = np.array((X_min, X_max))
X_range = torch.from_numpy(X_range).float()
print(X_range)

# 対応するyの予測値を求める
Y_range = pred(X_range)
print(Y_range.data)

# グラフ描画

plt.scatter(X,  Y,  c='k',  s=50)
plt.xlabel('$X$')
plt.ylabel('$Y$')
plt.plot(X_range.data, Y_range.data, lw=2, c='b')
plt.title('身長と体重の相関直線(加工後)')
plt.show()
            </code></pre>
            <p>最後に、平均値を引き算した(X, Y)から元の(x, y)に戻して同じ散布図表示をする。</p>
            <pre><code>
# y座標値とx座標値の計算

x_range = X_range + x.mean()
yp_range = Y_range + y.mean()

# グラフ描画

plt.scatter(x,  y,  c='k',  s=50)
plt.xlabel('$x$')
plt.ylabel('$y$')
plt.plot(x_range, yp_range.data, lw=2, c='b')
plt.title('身長と体重の相関直線(加工前)')
plt.show()
            </code></pre>
            <h3>最適化関数の利用</h3>
            <p>WとBのパラメータの変更は、「最適化関数」と呼ばれる関数を経由して変更するのが主流。</p>
            <pre><code>
# 初期化

# WとBを変数として扱う
W = torch.tensor(1.0, requires_grad=True).float()
B = torch.tensor(1.0, requires_grad=True).float()

# 繰り返し回数
num_epochs = 500

# 学習率
lr = 0.001

# optimizerとしてSGD(確率的勾配降下法)を指定する
import torch.optim as optim
optimizer = optim.SGD([W, B], lr=lr)

# 記録用配列初期化
history = np.zeros((0, 2))
            </code></pre>
            <p>SGDというクラスのインスタンスを生成し、optimizerという変数に保存している。</p>
            <pre><code>
# ループ処理

for epoch in range(num_epochs):

    # 予測計算
    Yp = pred(X)

    # 損失計算
    loss = mse(Yp, Y)

    # 勾配計算
    loss.backward()

    # パラメータ修正
    optimizer.step()

    #勾配値初期化
    optimizer.zero_grad()

    # 損失値の記録
    if (epoch %10 == 0):
        item = np.array([epoch, loss.item()])
        history = np.vstack((history, item))
        print(f'epoch = {epoch}  loss = {loss:.4f}')
            </code></pre>
            <p>最適化関数を利用してパラメータ値を間接的に変更している。</p>
            <b>最適化関数のチューニング</b>
            <p>最適化関数を導入することで、学習に関して色々なチューニングを簡単にできるようになる。</p>
            <pre><code>
# 初期化

# WとBを変数として扱う
W = torch.tensor(1.0, requires_grad=True).float()
B = torch.tensor(1.0, requires_grad=True).float()

# 繰り返し回数
num_epochs = 500

# 学習率
lr = 0.001

# optimizerとしてSGD(確率的勾配降下法)を指定する
import torch.optim as optim
optimizer = optim.SGD([W, B], lr=lr, momentum=0.9)

# 記録用配列初期化
history2 = np.zeros((0, 2))
            </code></pre>
            <p>momentum=0.9というオプションを、最適化関数のインスタンスoptimizerの生成時に設定している。</p>
            <p>これにより、学習の速度を速くすることができる。</p>
        </section>
        <section id="pf">
            <h2>Definition of the Prediction Function</h2>
            
        </section>
        <section id="lr">
            <h2>Linear Regression</h2>
            
        </section>
        <section id="binary">
            <h2>Binary Classification</h2>
            
        </section>
        <section id="multi">
            <h2>Multiclass Classification</h2>
            
        </section>
        <section id="mnist">
            <h2>MNIST</h2>
            
        </section>
        <section id="cnn">
            <h2>CNN</h2>
            
        </section>
        <section id="tuning">
            <h2>Tuning</h2>
            
        </section>
        <section id="pretrain">
            <h2>Pretrained Model</h2>
            
        </section>
        <section id="custom">
            <h2>Custom Data</h2>
            
        </section>
    </main>
</body>
</html>
