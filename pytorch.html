<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【深層学習 | PyTorchの基本】走る作曲家のAIカフェ</title>
    <link rel="stylesheet" href="style.css"> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1050827580219099"
     crossorigin="anonymous"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
            <h2>目次</h2>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#source">Source</a></li>
                <li><a href="#python">Python For Deep Learning</a></li>
                <li><a href="#pytorch">Basics of PyTorch</a></li>
                <li><a href="#ml">Introduction to Machine Learning</a></li>
                <li><a href="#pf">Definition of the Prediction Function</a></li>
                <li><a href="#lr">Linear Regression</a></li>
                <li><a href="#binary">Binary Classification</a></li>
                <li><a href="#multi">Multiclass Classification</a></li>
                <li><a href="#mnist">MNIST</a></li>
                <li><a href="#cnn">CNN</a></li>
                <li><a href="#tuning">Tuning</a></li>
                <li><a href="#pretrain">Pretrained Model</a></li>
                <li><a href="#custom">Custom Data</a></li>
            </ul>
        </section>
        <section id="overview">
            <h2>Overview</h2>
            深層学習とは、人工ニューラルネットワークを多層に積み重ねて学習させる手法で、複雑なデータのパターンや特徴を自動的に抽出することができる機械学習の一分野です。<br>
            このページでは、深層学習のライブラリであるPyTorchを用いて、深層学習の基礎を学んでいきます。
        </section>
        <section id="source">
            <h2>Source</h2>
            <p>以下の講義・書籍を参考にしました。</p>
            <ul>
                <li><a href="https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI">MIT 6.S191: Introduction to Deep Learning</a></li>
                <li><a href="https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html">Quickstart — PyTorch Tutorials 2.4.0+cu121 documentation</a></li>
                <li><a href="https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/af0caf6d7af0dda755f4c9d7af9ccc2c/quickstart_tutorial.ipynb">quickstart_tutorial.ipynb</a></li>
                <li><a href="https://yutaroogawa.github.io/pytorch_tutorials_jp/">PyTorchチュートリアル（日本語翻訳版）</a></li>
                <li><a href="https://github.com/makaishi2/pytorch_book_info">makaishi2_pytorch_book_info_ 書籍「最短コースでわかるPyTorch深層学習プログラミング」用サポートサイト</a></li>
                <li><a href="https://www.amazon.co.jp/%E6%9C%80%E7%9F%AD%E3%82%B3%E3%83%BC%E3%82%B9%E3%81%A7%E3%82%8F%E3%81%8B%E3%82%8B-PyTorch-%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-%E8%B5%A4%E7%9F%B3-%E9%9B%85%E5%85%B8/dp/4296110322#customerReviews">最短コースでわかる PyTorch &深層学習プログラミング</a></li>
            </ul>     
        </section>
        <section id="python">
            <h2>Python For Deep Learning</h2>
            <h3>コンテナデータ型</h3>
            <p>コンテナデータ型：「リスト」やNumPy配列のように、名前から実際のデータにアクセスするのにインデックスを経由する必要があるデータの型。</p>
            <p>
                <pre><code>
x = np.array([5, 7, 9])
y = x
x[1] = -1
print(y)
                </code></pre>
            この出力は、<code2>[5, -1, 9]</code2>となる。xの要素の変更がyに影響しないようにするには、<code2>y = x.copy()</code2>を使う。</p>
            <p>PyTorchで扱うデータは「テンソル」(Tensor)というクラスのインスタンスに保存される。</p>
            <p>テンソルの場合も同様にcopyしておけば他からの影響を気にしないで済む。</p>
            <h3>カスタムクラス定義</h3>
            <b>オブジェクト指向の基礎概念</b>
            <p>クラス：「型」。</p>
            <p>インスタンス：「型」から生成された個別の実体。</p>
            <p>クラスは、「属性」と呼ばれるクラス内の変数を持っている。</p>
            <p>「関数」あるいは「メソッド」と呼ばれる処理機能も持っている。</p>
            <p>属性と呼ばれるクラス内の変数の値は、インスタンスごとに異なる。</p>
            <b>最初のクラス定義</b>
            <p>例）Pointというクラスを定義してみる。</p>
            <p>Pointクラスの属性のxとyは、点のx座標とy座標である。</p>
            <p>また、関数drawは、自分を点としてグラフに表示する関数である。</p>
            <p>以下のように実装される。</p>
            <pre><code>
import matplotlib.pyplot as plt

# 円描画に必要なライブラリ
import matplotlib.patches as patches

# クラスPointの定義
Class Point:
    # インスタンス生成時にxとyの２つの引数を持つ
    def __init__(self, x, y):
        # インスタンスの属性xに第１引数をセットする
        self.x = x
        # インスタンスの属性yに第２引数をセットする
        self.y = y
    # 描画関数drawの定義（引数なし）
        def draw(self):
            # (x, y)に点を描画する
            plt.plot(self.x, self.y, marker='o', markersize=10, c='k')
            </code></pre>
            <p><code2>__init__</code2>は初期化処理。</p>
            <p><code2>self</code2>はクラスからインスタンスが生成された際、インスタンス自身を指す。</p>
            <b>最初のインスタンス生成</b>
            <pre><code>
# クラスPointからインスタンス変数p1とp2を生成する
p1 = Point(2, 3)
p2 = Point(-1, -2)
            </code></pre>
            <p>引数リストの引数は、__init__関数の定義の引数からselfを取り除いたもの。</p>
            <b>インスタンス属性へのアクセス</b>
            <pre><code>
# p1とp2の属性x、yの参照
print(p1.x, p1.y)
print(p2.x, p2.y)
            </code></pre>
            <b>draw関数の呼び出し</b>
            <pre><code>
# p1とp2のdraw関数を呼び出し、2つの点を描画する
p1.draw()
p2.draw()
plt.xlim(-4, 4)
plt.ylim(-4, 4)
plt.show()
            </code></pre>
            <b>Circleクラスの定義</b>
            <p>中心点のx座標とy座標の他に半径を意味する属性rを持つ。</p>
            <p>xとyについてはPointクラスの定義を再利用できる（「クラスの継承」）。</p>
            <pre><code>
# Pointの子クラスCircleの定義
class Circle(Point)
    # Circleはインスタンス生成時に引数x、y、rを持つ
    def __init__(self, x, y, r):
        # xとyは、親クラスの属性として設定
        super().__init__(x, y)
        # rは、Circleの属性として設定
        self.r = r

    # Circleのdraw関数は、親の関数呼び出しのあとで、円の描画も独自に行う
    def draw(self):
        # 親クラスのdraw関数呼び出し
        super().draw()
                
        # 円の描画
        c = patches.Circle(xy=(self.x, self.y), radius=self.r, fc='b', ec='k')
        ax.add_patch(c)
            </code></pre>
            <p>CircleクラスはPointクラスの子クラスとして定義している。</p>
            <b>Circleインスタンスの生成とdraw関数の呼び出し</b>
            <pre><code>
# クラスCircleからインスタンス変数c1を生成する
c1 = Circle(1, 0, 2)

# p1, p2, c1のそれぞれのdraw関数を呼び出す
ax = plt.subplot()
p1.draw()
p2.draw()
c1.draw()
plt.xlim(-4, 4)
plt.ylim(-4, 4)
plt.show()
            </code></pre>
            <p>親クラスと同じ名前の関数を子クラスでも定義して振る舞いを変更させることを「オーバーライド」と呼ぶ。</p>
            <h3>インスタンスを関数として扱う</h3>
            <p>クラスから生成したインスタンスを呼び出し可能な関数にする。</p>
            <pre><code>
# 関数クラスHの定義
class H:
    def __call__(self, x):
        return 2*x**2 + 2
            </code></pre>
            <pre><code>
# hが関数として動作することを確認する

# NumPy配列としてxを定義
x = np.arange(-2, 2.1, 0.25)
print(x)

# Hクラスのインスタンスとしてhを生成
h = H()

# 関数hの呼び出し
y = h(x)
print(y)
            </code></pre>
        </section>
        <section id="pytorch">
            <h2>Basics of PyTorch</h2>
            <h3>重要な概念</h3>
            <p>PyTorchでは、テンソルという独自のクラスでデータを表現する。</p>
            <p>PyTorchの最大の特徴は、自動微分機能。</p>
            <h3>テンソル</h3>
            <b>ライブラリインポート</b>
            <pre><code>
import torch
            </code></pre>
            <b>色々な階層のテンソル</b>
            <pre><code>
# 0階テンソル（スカラー）
r0 = torch.tensor(1.0).float()
            </code></pre>
            <p>テンソル変数を作るのに一番簡単なのは、torch.tensor関数を使う方法。</p>
            <p>テンソル変数の生成時には、必ず後ろにfloat関数の呼び出しをつけて、dtype（テンソル変数の要素のデータ型）を強制的にfloat32に変換する。</p>
            <pre><code>
# 1階テンソル（ベクトル）

# 1階のNumPy変数作成
r1_np = np.array([1, 2, 3, 4, 5])

# NumPyからテンソルに変換
r1 = torch.tensor(r1_np).float()

# dtypeを調べる
print(r1.dtype)

# shapeを調べる
print(r1.shape)

# データを調べる
print(r1.data)
            </code></pre>
            <pre><code>
# 2階テンソル（行列）

# 2階のNumPy変数作成
r2_np = np.array([1, 5, 6], [4, 3, 2])

# NumPyからテンソルに変換
r2 = torch.tensor(r2_np).float()
            </code></pre>
            <pre><code>
# 3階テンソル

# 乱数seedの初期化
torch.manual_seed(123)

# shape=[3,2,2]の正規分布変数テンソルを作る
r3 = torch.randn((3, 2, 2))
            </code></pre>
            <b>整数値のテンソル</b>
            <p>PyTorchによる計算では、そのほとんどの場合、数値型としてdtype=float32を利用する。</p>
            <p>しかし、「多値分類」用の損失関数である、nn.CrossEntropyLossとnn.NLLLossは、損失関数呼び出し時に、第2引数に整数型を指定する必要がある。</p>
            <p>その際は、以下のように整数型へ変換する。</p>
            <pre><code>
r5 = r1.long()
            </code></pre>
            <p>このようにlong関数をかけると、dtype=torch.int64になる。</p>
            <b>view関数</b>
            <p>view関数は、NumPyのreshape関数に相当し、変数の階数を変換することができる。</p>
            <pre><code>
# 2階化（r3は[3,2,2]の3階テンソル）
# 要素数に-1を指定すると、この数を自動調整する
r6 = r3.view(3, -1)
            </code></pre>
            <p>結果として、[3, 4]の2階テンソルが得られる。</p>
            <pre><code>
# 1階化
r7 - r3.view(-1)
            </code></pre>
            <b>それ以外の属性</b>
            <p>テンソルはデータとして扱うことも、クラスとして扱うこともできる。</p>
            <pre><code>
# requires_grad属性
print('requires_grad: ', r1.requires_grad)

# device属性
print('device: ', r1.device)
            </code></pre>
            <b>item関数</b>
            <p>スカラー（0階テンソル）に対しては、テンソルからPython本来のクラスの数値（floatまたはint）を取り出すのにitem関数が使える。</p>
            <p>計算結果テンソルとしてのlossから、データ記録用に値だけを抽出する場合によく用いる。</p>
            <pre><code>
item = r0.item()
            </code></pre>
            <p>この関数は、1階以上のテンソルを対象にできない。</p>
            <b>max関数</b>
            <pre><code>
# max関数を引数無しで呼び出すと、全体の最大値が取得できる
print(r2.max())
            </code></pre>
            <pre><code>
# torch.max関数
# 2つ目の引数はどの軸で集約するかを意味する（軸=1: 行方向、軸=0: 列方向）
print(torch.max(r2, 1))
            </code></pre>
            <p>この呼び出し方をした場合、最大値の値そのものだけでなく、どのindexで最大値を取ったかも返ってくる。</p>
            <p>後ろに[1]をつけると、後者のみを抽出できる。</p>
            <pre><code>
# 何番目の要素が最大値を取るかは、indicesを調べると良い
# 以下の計算は、多値分類で予測ラベルを求めるときによく利用される
print(torch.max(r2, 1)[1])
            </code></pre>
            <p>「複数の予測器の出力のうち、最も大きな値を出した予測器のindexが予測結果ラベルになる」ことを実装している。</p>
            <b>NumPy変数への変換</b>
            <pre><code>
# NumPy化
r2_np = r2.data.numpy()
            </code></pre>
            <p>ただし、この方法だと、テンソル変数とNumPy配列は同じデータを指すため、テンソル側で値を変えるとNumPyも連動して値が変わる。</p>
            <p>連動しないようにするには、r2.data.numpy.copy()として、データのコピーを作る。</p>
            <h3>自動微分機能</h3>
            <p>PyTorchで自動微分を行う場合の処理の流れ：</p>
            <ol>
                <li>勾配計算用変数の定義：requires_grad=Trueとする</li>
                <li>テンソル変数間で計算：裏で計算グラフが自動生成される</li>
                <li>計算グラフの可視化：make_dot関数</li>
                <li>勾配計算：backward関数</li>
                <li>勾配値の取得：grad属性</li>
                <li>勾配値の初期化：zero_関数</li>
            </ol>
            <b>1. 勾配計算用変数の定義</b>
            <p>requires_grad属性をTrueに設定する。</p>
            <b>2. テンソル変数間で計算</b>
            <p>他のテンソル変数との間で演算する。</p>
            <p>このとき、計算式による値の計算が行われるのと同時に、裏で「計算グラフ」が生成される。</p>
            <p>この機能は「Define by Run」と呼ばれる。</p>
            <p>「計算グラフ」とは、データとそれに対する演算の順番を定義するもの。</p>
            <b>3. 計算グラフの可視化</b>
            <p>PyTorch内部の動きを確認するために行う。</p>
            <p>make_dotという関数を使うと、2.で自動生成された計算グラフを可視化できる。</p>
            <b>4. 勾配計算</b>
            <p>勾配計算は、計算結果を保存したテンソル変数（スカラー）に対して、backward関数を呼び出すことにより行われる。</p>
            <b>5. 勾配値の取得</b>
            <p>勾配計算の結果は、PyTorchでは勾配値と呼ばれる。</p>
            <p>勾配値はテンソル変数のgrad属性により取得できる。</p>
            <b>6. 勾配値の初期化</b>
            <p>grad属性に保存されている勾配値は、利用が終わったら値を初期化する必要がある。</p>
            <p>そのための関数がzero_関数である。</p>
            <h3>2次関数の勾配計算</h3>
            <p>例）\(y = 2x^2+2\)に対して自動微分計算を行う。</p>
            <b>1. 勾配計算用変数の定義</b>
            <pre><code>
# xをNumPy配列で定義
x_np = np.arange(-2, 2.1, 0.25)
            </code></pre>
            <pre><code>
# 1. 勾配計算用変数の定義

# requires=Trueとする
x = torch.tensor(x_np, requires_grad=True, dtype=torch.float32)
            </code></pre>
            <b>2. テンソル変数間で計算</b>
            <pre><code>
# 2次関数の計算
# 裏で計算グラフが自動生成される

y = 2 * x**2 + 2
            </code></pre>
            <p>このとき、変数yは自動的にテンソル変数になる。</p>
            <p>requires_grad属性がTrueのテンソル変数は、そのままではMatplotlibで使えないが、data属性を渡すとグラフ表示が可能。</p>
            <pre><code>
# グラフ描画

plt.plot(x.data, y.data)
plt.show()
            </code></pre>
            <p>勾配計算の対象はスカラーである必要があるため、yの値をsum関数ですべて足して、足した結果を新しいテンソル変数zに代入する。</p>
            <p>（「なぜ和をとることで微分計算ができるのか」についての説明は<a href = "https://qiita.com/makaishi2/items/a6cf19add4b6d16b8483">こちら</a>。）</p>
            <pre><code>
# 勾配計算のため、sum 関数で 1階テンソルの関数値をスカラー化する
# (sum 関数を各要素で偏微分した結果は1なので、元の関数の微分結果を取得可能 )
z = y.sum()
            </code></pre>
            <b>3. 計算グラフの可視化</b>
            <p>この変数zを使って、計算グラフの可視化を行う。</p>
            <pre><code>
# 3. 計算グラフの可視化

# 必要なライブラリのインポート
from torchviz import make_dot

# 可視化関数の呼び出し
g = make_dot(z, params={'x': x})
display(g)
            </code></pre>
            <p>make_dot関数の呼び出し時の第1引数として、可視化したい計算グラフの対象となる変数（今回の場合z）、第2引数のparamsとして微分計算対象の変数（今回の場合x）を辞書形式のパラメータリストにして渡す。</p>
            <p>出発点のxをテンソル変数とし、かつrequires_gradフラグをセットしておくと、計算の過程が自動的に記録される。</p>
            <p>値を計算しながら、計算過程を自動的に記録する機能は「Define by Run」と呼ばれる。</p>
            <p>計算グラフにおいて、青色のノードはmake_dot関数呼び出し時にparamsで指定した変数に該当し、リーフノードと呼ばれる。</p>
            <p>勾配値の計算が可能な変数を意味している。</p>
            <p>緑色のノードは出力ノード、灰色のノードは中間処理という意味になっている。</p>
            <p>一番上と一番下のノードに記載される()は、それぞれの変数のshapeを示している。</p>
            <p>(17)であれば、1階17次元のテンソル、()であれば、0階のスカラーということになる。</p>
            <p>"AccumulateGrad"は、末端のリーフノードの直下に配置され、勾配値を蓄積する場所を示している。</p>
            <p>計算グラフがあれば、PyTorch側で\(f(x) = 2x^2 + 2\)という2次関数を、指数関数、乗算関数、加算関数という基本的な関数の合成関数として認識できる。</p>
            <b>4. 勾配計算</b>
            <p>backward関数を呼び出すだけ。</p>
            <pre><code>
# 4. 勾配計算
z.backward()
            </code></pre>
            <b>5. 勾配値の取得</b>
            <pre><code>
print(x.grad)
            </code></pre>
            <b>6. 勾配値の初期化</b>
            <p>x.gradには最新の勾配計算の結果が入るのではなく、それまでの勾配計算結果を加算した値が入る。</p>
            <p>条件を変えて新しく勾配値を取得したい場合、勾配値の初期化が必要になる。</p>
            <pre><code>
# 6. 勾配の初期化は関数zero_()を使う
x.grad.zero_()
            </code></pre>
            <h3>シグモイド関数の勾配計算</h3>
            <b>シグモイド関数の定義</b>
            <pre><code>
# シグモイド関数の定義
sigmoid = torch.nn.Sigmoid()
            </code></pre>
            <b>2. テンソル変数でyの値を計算</b>
            <pre><code>
# 2. yの値の計算
y = sigmoid(x)
            </code></pre>
            <b>グラフ描画</b>
            <pre><code>
plt.plot(x.data, y.data)
plt.show()
            </code></pre>
            <b>最終結果をスカラー化</b>
            <pre><code>
z = y.sum()
            </code></pre>
            <b>3. 計算グラフの可視化</b>
            <pre><code>
3. 計算グラフの可視化
g = make_dot(z, params={'x': x})
            </code></pre>
            <b>4. 勾配計算、5. 勾配値の取得</b>
            <pre><code>
# 4. 勾配計算
z.backward()

# 5. 勾配値の確認
print(x.grad)
            </code></pre>
            <b>グラフに表示</b>
            <pre><code>
# 元の関数と勾配のグラフ化

plt.plot(x.data, y.data, c='b', label='y')
plt.plot(x.data, x.grad.data, c='k', label='x.grad')
plt.legend()
plt.show()
            </code></pre>
        </section>
        <section id="ml">
            <h2>Introduction to Machine Learning</h2>
            <h3>問題の定義</h3>
            <p>与えられた身長から体重を予測する機械学習モデルを作る（線形回帰）。</p>
            <h3>勾配降下法</h3>
            <p>勾配降下法は、「予測計算」「損失関数」「勾配計算」「パラメータ修正」の４つのステップを繰り返すことで、予測関数の中のパラメータを最適な値に近づけることである。</p>
            <b>①予測計算</b>
            <p>入力テンソルXを入力とし、予測結果は出力テンソルYpに出力されるものとする。</p>
            <p>今回は、予測関数の実体は次のような１次関数である。</p>
            <div class="scroll">
                \begin{align}
                Yp = W * X + B
                \end{align}
            </div>
            <p>予測関数により予測値Ypを求めることを「①予測計算」と呼ぶ。</p>
            <b>②損失計算</b>
            <p>「教師あり学習」における学習データは通常、入力と正解値の両方を含んでいるので、正解値の列を分離して正解テンソルYとする。</p>
            <p>予測計算の結果であるYpと正解であるYの、2つのテンソルを入力とする「損失」lossを定義する。</p>
            <p>損失が最小になるようなWとBを見つけることが目標である。</p>
            <p>この損失を計算する過程が「②損失計算」である。</p>
            <p>損失関数は、予測関数の性質に応じて適したものを選ぶ。</p>
            <p>今回は値を予測する回帰モデルであるため、YとYpの差の2乗を利用することにする。</p>
            <p>正確には、すべてのデータ系列の差を2乗して、その平均をとった関数である「平均２乗誤差」を選択する。</p>
            <b>③勾配計算</b>
            <p>予測関数を構成するパラメータWとBの値を少しだけ変え、その時の損失の変化の度合（勾配）を調べる。これが「③勾配計算」である。</p>
            <p>損失関数の最低値を目指すために、WとBをずらすベストな方向が勾配にあたる。</p>
            <b>④パラメータ修正</b>
            <p>勾配に小さな定数（学習率）lrをかけ、その値だけWとBを同時に減らす。この操作が「④パラメータ修正」である。</p>
            <h3>データ前処理</h3>
            <pre><code>
# サンプルデータの宣言
sampleData1 = np.array([
    [166, 58.7],
    [176.0, 75.7],
    [171.0, 62.1],
    [173.0, 70.4],
    [169.0,60.1]
])
print(sampleData1)
            </code></pre>
            <p>学習データを入力データxと正解データyに分割する。</p>
            <pre><code>
# 機械学習モデルで扱うため、身長だけを抜き出した変数xと
# 体重だけを抜き出した変数yをセットする

x = sampleData1[:,0]
y = sampleData1[:,1]
            </code></pre>
            <p>データの散布図を表示する。</p>
            <pre><code>
# 散布図表示で状況の確認

plt.scatter(x,  y,  c='k',  s=50)
plt.xlabel('$x$: 身長(cm) ')
plt.ylabel('$y$: 体重(kg)')
plt.title('身長と体重の関係')
plt.show()
            </code></pre>
            <b>データの変換</b>
            <p>勾配降下法では、対象となる数値は絶対値が1以内に収まるような比較的小さな値の方が望ましい。</p>
            <p>今回の学習データは身長も体重も大きな数値なので、それぞれ平均値を引くことで勾配降下法がやりやすい条件に変換する。</p>
            <p>変換後のxとyをXとYで表す。</p>
            <pre><code>
X = x - x.mean()
Y = y - y.mean()
            </code></pre>
            <h3>予測計算</h3>
            <p>まず、変換後のXとYをテンソル変数に変換する。</p>
            <pre><code>
# XとYをテンソル変数化する

X = torch.tensor(X).float()
Y = torch.tensor(Y).float()
            </code></pre>
            <p>次に、１次関数の係数にあたる変数Wと定数項にあたる変数Bもテンソル変数として定義する。</p>
            <pre><code>
# 重み変数の定義
# WとBは勾配計算をするので、requires_grad=Trueとする

W = torch.tensor(1.0, requires_grad=True).float()
B = torch.tensor(1.0, requires_grad=True).float()
            </code></pre>
            <p>２つの変数には初期値として1.０の値を設定する。</p>
            <p>また、この２つの変数は勾配降下法の対象となるので、requires_grad属性をTrueに設定して自動微分ができるようにする。</p>
            <p>予測関数を定義し、予測値を計算する。</p>
            <pre><code>
# 予測関数は一次関数

def pred(X):
    return W * X + B

# 予測値の計算

Yp =  pred(X)
            </code></pre>
            <p>計算グラフを表示する。</p>
            <pre><code>
# 予測値の計算グラフ可視化

params = {'W': W, 'B': B}
g = make_dot(Yp, params=params)
display(g)
            </code></pre>
            <h3>損失計算</h3>
            <p>損失関数はMSE（平均２乗誤差）と呼ばれる方法で計算する。</p>
            <pre><code>
# 損失関数は誤差二乗平均

def mse(Yp, Y):
    loss = ((Yp - Y) ** 2).mean()
    return loss

# 損失計算

loss = mse(Yp, Y)
            </code></pre>
            <p>ここで得られた損失(loss)は、１次関数の係数Wと定数項Bの関数になっている。計算グラフで確認する。</p>
            <pre><code>
# 損失の計算グラフ可視化

params = {'W': W, 'B': B}
g = make_dot(loss, params=params)
display(g)
            </code></pre>
            <h3>勾配計算</h3>
            <p>backward関数を呼び出すだけ。</p>
            <pre><code>
# 勾配計算

loss.backward()

# 勾配値確認

print(W.grad)
print(B.grad)
            </code></pre>
            <h3>パラメータ修正</h3>
            <p>勾配計算ができたら、その値に一定の学習率lr（0.01や0.001）を掛けた結果を、もとのパラメータ値から引くのが勾配降下法の基本的な考え方である。</p>
            <pre><code>
# 学習率の定義

lr = 0.001

#  勾配を元にパラメータ修正

W -= lr * W.grad
B -= lr * B.grad
            </code></pre>
            <p>ただし、このコードではエラーが発生する。</p>
            <p>勾配計算をしている最中の変数は他に影響が及んでしまうため、勝手に値を修正できない。</p>
            <p>この場合、with torch.no_grad()というコンテキストを設定すると、そのコンテキストの内部では一時的に計算グラフ生成機能が止まり、変数の修正が可能になる。</p>
            <pre><code>
# 勾配を元にパラメータ修正
# with torch.no_grad() を付ける必要がある

with torch.no_grad():
    W -= lr * W.grad
    B -= lr * B.grad

    # 計算済みの勾配値をリセットする
    W.grad.zero_()
    B.grad.zero_()
            </code></pre>
            <p>勾配値を使ってパラメータ値を更新したあと、次の勾配計算の準備のため、zero_関数で勾配値の初期化もしている。</p>
            <h3>繰り返し計算</h3>
            <p>実際に繰り返し計算を行う。初期化のセルは以下の通り。</p>
            <pre><code>
# 初期化

# WとBを変数として扱う
W = torch.tensor(1.0, requires_grad=True).float()
B = torch.tensor(1.0, requires_grad=True).float()

# 繰り返し回数
num_epochs = 500

# 学習率
lr = 0.001

# 記録用配列初期化
history = np.zeros((0, 2))
            </code></pre>
            <p>ループ処理のセルは以下の通り。</p>
            <pre><code>
# ループ処理

for epoch in range(num_epochs):

    # 予測計算
    Yp = pred(X)

    # 損失計算
    loss = mse(Yp, Y)

    # 勾配計算
    loss.backward()

    with torch.no_grad():
        # パラメータ修正
        W -= lr * W.grad
        B -= lr * B.grad

        # 勾配値の初期化
        W.grad.zero_()
        B.grad.zero_()

    # 損失の記録
    if (epoch %10 == 0):
        item = np.array([epoch, loss.item()])
        history = np.vstack((history, item))
        print(f'epoch = {epoch}  loss = {loss:.4f}')
            </code></pre>
            <h3>結果評価</h3>
            <p>WとBの最終的な値と、損失の開始時、終了時の値を表示する。</p>
            <pre><code>
# パラメータの最終値
print('W = ', W.data.numpy())
print('B = ', B.data.numpy())

#損失の確認
print(f'初期状態: 損失:{history[0,1]:.4f}')
print(f'最終状態: 損失:{history[-1,1]:.4f}')
            </code></pre>
            <p>損失の減り方をグラフで可視化する（学習曲線）。</p>
            <pre><code>
# 学習曲線の表示 (損失)

plt.plot(history[:,0], history[:,1], 'b')
plt.xlabel('繰り返し回数')
plt.ylabel('損失')
plt.title('学習曲線(損失)')
plt.show()
            </code></pre>
            <p>求めたWとBの値から直線の式を算出し、散布図に重ね描きする。</p>
            <pre><code>
# xの範囲を求める(Xrange)
X_max = X.max()
X_min = X.min()
X_range = np.array((X_min, X_max))
X_range = torch.from_numpy(X_range).float()
print(X_range)

# 対応するyの予測値を求める
Y_range = pred(X_range)
print(Y_range.data)

# グラフ描画

plt.scatter(X,  Y,  c='k',  s=50)
plt.xlabel('$X$')
plt.ylabel('$Y$')
plt.plot(X_range.data, Y_range.data, lw=2, c='b')
plt.title('身長と体重の相関直線(加工後)')
plt.show()
            </code></pre>
            <p>最後に、平均値を引き算した(X, Y)から元の(x, y)に戻して同じ散布図表示をする。</p>
            <pre><code>
# y座標値とx座標値の計算

x_range = X_range + x.mean()
yp_range = Y_range + y.mean()

# グラフ描画

plt.scatter(x,  y,  c='k',  s=50)
plt.xlabel('$x$')
plt.ylabel('$y$')
plt.plot(x_range, yp_range.data, lw=2, c='b')
plt.title('身長と体重の相関直線(加工前)')
plt.show()
            </code></pre>
            <h3>最適化関数の利用</h3>
            <p>WとBのパラメータの変更は、「最適化関数」と呼ばれる関数を経由して変更するのが主流。</p>
            <pre><code>
# 初期化

# WとBを変数として扱う
W = torch.tensor(1.0, requires_grad=True).float()
B = torch.tensor(1.0, requires_grad=True).float()

# 繰り返し回数
num_epochs = 500

# 学習率
lr = 0.001

# optimizerとしてSGD(確率的勾配降下法)を指定する
import torch.optim as optim
optimizer = optim.SGD([W, B], lr=lr)

# 記録用配列初期化
history = np.zeros((0, 2))
            </code></pre>
            <p>SGDというクラスのインスタンスを生成し、optimizerという変数に保存している。</p>
            <pre><code>
# ループ処理

for epoch in range(num_epochs):

    # 予測計算
    Yp = pred(X)

    # 損失計算
    loss = mse(Yp, Y)

    # 勾配計算
    loss.backward()

    # パラメータ修正
    optimizer.step()

    #勾配値初期化
    optimizer.zero_grad()

    # 損失値の記録
    if (epoch %10 == 0):
        item = np.array([epoch, loss.item()])
        history = np.vstack((history, item))
        print(f'epoch = {epoch}  loss = {loss:.4f}')
            </code></pre>
            <p>最適化関数を利用してパラメータ値を間接的に変更している。</p>
            <b>最適化関数のチューニング</b>
            <p>最適化関数を導入することで、学習に関して色々なチューニングを簡単にできるようになる。</p>
            <pre><code>
# 初期化

# WとBを変数として扱う
W = torch.tensor(1.0, requires_grad=True).float()
B = torch.tensor(1.0, requires_grad=True).float()

# 繰り返し回数
num_epochs = 500

# 学習率
lr = 0.001

# optimizerとしてSGD(確率的勾配降下法)を指定する
import torch.optim as optim
optimizer = optim.SGD([W, B], lr=lr, momentum=0.9)

# 記録用配列初期化
history2 = np.zeros((0, 2))
            </code></pre>
            <p>momentum=0.9というオプションを、最適化関数のインスタンスoptimizerの生成時に設定している。</p>
            <p>これにより、学習の速度を速くすることができる。</p>
        </section>
        <section id="pf">
            <h2>Definition of the Prediction Function</h2>
            <p>PyTorchでは、予測関数を細かい機能に分け、機能の一つ一つに対応する部品を用意し、その部品を組み合わせることで、複雑な関数を作る（ビルディングブロック）。</p>
            <p>この部品のことを、「レイヤー関数」と呼ぶ。</p>
            <b>レイヤー関数：</b>
            <p>テンソルを入力とし、テンソルを出力とする関数群。機械学習モデルは、レイヤー関数を部品として、これを組み合わせて作る。ReLU関数のような活性化関数もレイヤー関数の一種とする。</p>
            <b>パラメータ：</b>
            <p>レイヤー関数の内部でも持っている、入力テンソル以外のデータ。学習とはレイヤー関数のパラメータ値を調整することを意味する。すべてのレイヤー関数がパラメータを持つわけではない。</p>
            <b>入力テンソル：</b>
            <p>機械学習モデル全体を１つの関数（合成関数）とみなした場合、関数の入力となるテンソル。ニューラルネットワークの概念では、「入力層」に該当する。</p>
            <b>出力テンソル：</b>
            <p>機械学習もでる全体を１つの関数（合成関数）とみなした場合、関数の出力となるテンソル。ニューラルネットワークの概念では、「出力層」に該当する。</p>
            <b>機械学習モデル：</b>
            <p>複数のレイヤー関数を組み合わせて（１つの場合もある）、入力テンソルに対して望ましい（正解データにできるだけ近い）出力テンソルを出力する合成関数。</p>
            <b>学習：</b>
            <p>レイヤー関数内部のパラメータ値を、望ましい出力テンソルが得られるように調整すること。具体的な手段として、勾配降下法などの最適化関数が用いられる。</p>
            <h3>予測関数の内部構造</h3>
            <p>まず、部品となるレイヤー関数をインスタンスとして定義する。</p>
            <pre><code>
# レイヤー関数定義

# 最初の線形関数
# 784 入力数
# 128 出力数
l1 = nn.Linear(784, 128)

# 2番目の線形関数
# 128 入力数
# 10 出力数
l2 = nn.Linear(128, 10)

# 活性化関数
relu = nn.ReLU(inplace=True)
            </code></pre>
            <p>２つの線形関数と、１つの活性化関数（ReLU関数）を定義している。</p>
            <p>最初の線形関数の２つ目のパラメータ値（128）が、２番目の線形関数の最初のパラメータ値と等しい。</p>
            <p>この128という値が、「隠れ層」のノード数に該当することになる。</p>
            <p>この３つの関数を組み合わせて入力から出力を得る。</p>
            <pre><code>
# 入力テンソルから出力テンソルを計算

# ダミー入力データを作成
inputs = torch.randn(100, 784)

# 中間テンソル1の計算
m1 = l1(inputs)

# 中間テンソル2の計算
m2 = relu(m1)

# 出力テンソルの計算
outputs = l2(m2)

# 入力テンソルと出力テンソルのshape確認
print('入力テンソル', inputs.shape)
print('出力テンソル', outputs.shape)
            </code></pre>
            <p>最初に100行、784列の2階テンソル（行列）を作っている。</p>
            <p>機械学習では複数件のデータを同時に扱うことが原則である。</p>
            <p>入力テンソルの最初のインデックスは常に「複数あるデータのうち何番目のデータか」を意味している。</p>
            <p>[100, 784]というshapeは「784個の要素を持つ１階テンソル（ベクトル）のデータが100件ある」と読む。</p>
            <p>直線状につながる合成関数は、nn.Sequentialという部品を利用してより簡単に実装できる。</p>
            <pre><code>
# nn.Sequentialを使って、全体を合成関数として定義

net2 = nn.Sequential(
    l1,
    relu,
    l2
)

outputs2 = net2(inputs)

# 入力テンソルと出力テンソルのshape確認
print('入力テンソル', inputs.shape)
print('出力テンソル', outputs2.shape)
            </code></pre>
            <p>PyTorchによる機械学習プログラムは「予測関数」「損失関数」「最適化関数」の３つのパートに分けることが可能である。</p>
            <p>繰り返し処理の順番という観点でいうと、「予測計算」「損失計算」「勾配計算」「パラメータ修正」を繰り返すという形になる。</p>
            <h3>活性化関数の目的</h3>
            <p>単に線形関数を合成しただけの関数は、結局１階層の線形関数と同じである。</p>
            <p>「非線形関数」と呼ばれる活性化関数を線形関数の間に入れることにより初めて深い階層のディープラーニングモデルが意味を持つ。</p>
            <p>活性化関数にはもう一つ、線形関数の出力を整形するという役割がある。</p>
            <p>具体的には、2値分類モデルではシグモイド関数を、多値分類モデルではsoftmax関数をこの目的で利用し、モデルの出力値を0から1の値を持つ「確率値」にする。</p>
        </section>
        <section id="lr">
            <h2>Linear Regression</h2>
            <h3>問題の定義</h3>
            <p><a href="https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html#:~:text=This%20dataset%20contains%20information">「ボストン・データセット」</a>を利用する。</p>
            <p>ボストン近郊を506の地域に分割し、それぞれの地域で様々な観点の統計情報を取得している。</p>
            <p>その中の１項目に「住宅平均価格」があり、「他の項目から住宅平均価格を予測する」という回帰モデルの題材としてよく利用される。</p>
            <p>まずは、入力項目のうち「平均部屋数」を意味するRMという項目を使って、目的関数にあたる不動産価格を予測する、単回帰と呼ばれるモデルを作る。</p>
            <p>その後、もう一つ「低所得者率」を意味するLSTATという項目も追加し、2入力1出力のモデルを作る。</p>
            <p>このモデルは、「単回帰」に対して「重回帰」と呼ばれる。両者を合わせて「線形回帰」と呼ぶ。</p>
            <p>１次関数はレイヤー関数でいうと、線形関数（nn.Linear）に該当する。</p>
            <p>線形回帰モデルは、他のレイヤー関数との組み合わせなしに、nn.Linearというう単独のレイヤー関数でモデルを実装できる。</p>
            <h3>線形関数</h3>
            <p>インスタンスを生成するコードは以下の通り。</p>
            <pre><code>
# 入力：２、出力：３の線形関数の定義
l3 = nn.Linear(2, 3)
            </code></pre>
            <p>このレイヤー関数はインスタンス生成時、２つの引数をとる。</p>
            <p>最初の引数は入力テンソルの次元数を、次の引数は出力テンソルの次元数を意味する。</p>
            <p>つまり、この例だと、２次元テンソルを入力として３次元テンソルを出力とする関数になる。</p>
            <b>１入力１出力</b>
            <pre><code>
# 乱数の種固定
torch.manual_seed(123)

# 入力:1 出力:1 の線形関数の定義
l1 = nn.Linear(1, 1)

# 線形関数の表示
print(l1)
            </code></pre>
            <p>レイヤー関数には、nn.Linear以外の関数うを含め、統一的にnamed_parametersという関数が組み込まれている。</p>
            <p>この関数を呼び出すと、（パラメータ名、パラメータ参照）のリストを返す。</p>
            <p>変数l1内にどのようなパラメータがあり、どのような値とshapeを持っているか調べる。</p>
            <pre><code>
# パラメータ名、パラメータ値、shapeの表示

for param in l1.named_parameters():
    print('name: ', param[0])
    print('tensor: ', param[1])
    print('shape: ', param[1].shape)
            </code></pre>
            <p>結果から、[1, 1]というshapeを持つ変数weightと、[1]というshapeを持つ変数biasがあることが分かる。</p>
            <p>weightは「重み」（１次関数の係数）を、biasはバイアス（１次関数の定数項）を意味する。</p>
            <p>今回は、入力も出力も１次元なので、weightもbiasも本来ならスカラー（０階テンソル）で問題ないはずである。</p>
            <p>わざわざ[1, 1]という行列、[1]というベクトルになっているのは、入力・出力テンソルの次元数が２以上に増えた時も簡単に拡張できるようにするためである。</p>
            <p>２つのパラメータでは、requires=Trueになっている。通常、レイヤー関数内のパラメータは学習対象である。</p>
            <p>weightとbiasはともにランダムな値に設定されている。</p>
            <p>通常乱数値がセットされるパラメータに明示的な値を設定したい場合、次のコードのように、nn.init.constant_を呼び出す（ここでは、\(y = 2x + 1\)としてみる）。</p>
            <pre><code>
# 初期値設定
nn.init.constant_(l1.weight, 2.0)
nn.init.constant_(l1.bias, 1.0)

# 結果確認
print(l1.weight)
print(l1.bias)
            </code></pre>
            <p>ダミーデータをこの関数にかけて、１次関数として動作していることを確認する。</p>
            <pre><code>
# テスト用データ生成

# x_npをnumpy配列で定義(x_np = [-2. -1. 0. 1. 2.])
x_np = np.arange(-2, 2.1, 1)

# Tensor化
x = torch.tensor(x_np).float()

# サイズを(N,1)に変更
x = x.view(-1,1)

# 結果確認
print(x.shape)
print(x)
            </code></pre>
            <p>weightが[1, 1]の行列であるため、入力変数xは[5, 1]の２次元テンソルに変換している。</p>
            <b>２入力１出力</b>
            <p>線形関数\(y = x_1 + x_2 + 2\)を作る。</p>
            <pre><code>
# 2次元numpy配列
x2_np = np.array([[0, 0], [0, 1], [1, 0], [1,1]])

# Tensor化
x2 =  torch.tensor(x2_np).float()

# 結果確認
print(x2.shape)
print(x2)
            </code></pre>
            <p>以下のコードで４行２列のテストデータを用意する。</p>
            <pre><code>
# 2次元numpy配列
x2_np = np.array([[0, 0], [0, 1], [1, 0], [1,1]])

# Tensor化
x2 =  torch.tensor(x2_np).float()

# 結果確認
print(x2.shape)
print(x2)
            </code></pre>
            <p>以下のように関数呼び出しを行う。</p>
            <pre><code>
# 関数値計算
y2 = l2(x2)

# shape確認
print(y2.shape)

# 値確認
print(y2.data)
            </code></pre>
            <b>２入力３出力</b>
            <pre><code>
# 入力:2 出力:3 の線形関数の定義

l3 = nn.Linear(2, 3)

# 初期値設定
nn.init.constant_(l3.weight[0,:], 1.0)
nn.init.constant_(l3.weight[1,:], 2.0)
nn.init.constant_(l3.weight[2,:], 3.0)
nn.init.constant_(l3.bias, 2.0)

# 結果確認
print(l3.weight)
print(l3.bias)
            </code></pre>
            <pre><code>
# 関数値計算
y3 = l3(x2)

# shape確認
print(y3.shape)

# 値確認
print(y3.data)
            </code></pre>
            <h3>カスタムクラスを利用したモデル定義</h3>
            <pre><code>
# モデルのクラス定義

class Net(nn.Module):
    def __init__(self, n_input, n_output):
        #  親クラスnn.Modulesの初期化呼び出し
        super().__init__()

        # 出力層の定義
        self.l1 = nn.Linear(n_input, n_output)

    # 予測関数の定義 
    def forward(self, x):
        x1 = self.l1(x) # 線形回帰
        return x1
            </code></pre>
            <p>このコードは、機械学習モデル定義のコードの一番本質的な部分である。</p>
            <p>Netというクラスの親クラスは、nn.Moduleである。</p>
            <p>クラスの内部にはforward関数が定義されていて、この関数で予測処理を実装する。</p>
            <p>予測は次のように行う。</p>
            <pre><code>
# ダミー入力
inputs = torch.ones(100,1)

# インスタンスの生成 (１入力1出力の線形モデル)
n_input = 1
n_output = 1
net = Net(n_input, n_output)

# 予測
outputs = net(inputs)
            </code></pre>
            <p>カスタムクラスのインスタンス変数netは自分自身が関数として動く。</p>
            <h3>MSELossクラスを利用した損失関数</h3>
            <p>「損失loss」とは、「予測関数」と「損失関数」を組み合わせてできあがった合成関数で、「損失計算」とは、この合成関数を計算することを意味する。</p>
            <p>この合成関数（損失）はパラメータ（weightとbias）を引数とする。</p>
            <p>合成関数（損失）をパラメータで偏微分することが「勾配計算」であり、勾配計算の結果が勾配降下法の「パラメータ修正」で用いられる。</p>
            <pre><code>
# 損失関数：平均２乗誤差
criterion = nn.MSELoss()
            </code></pre>
            <p>このコードでは、初期化処理の中で損失関数criterionを、クラスnn.MSELossのインスタンスとして定義している。</p>
            <pre><code>
# 誤差計算
loss = criterion(outputs, labels1) / 2.0
            </code></pre>
            <p>このコードでは、繰り返し処理の中で、損失関数を出力テンソルoutputsと正解テンソルlabelsの２つをパラメータとして取る関数として呼び出し、さらに2.0で割った結果を損失lossに代入している。これが「損失計算」である。</p>
            <p>また、損失lossに対してbackward関数を呼び出し、勾配計算をしている。</p>
            <h3>データ準備</h3>
            <pre><code>
# 学習用データ準備
                
data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+",
    skiprows=22, header=None)
x_org = np.hstack([raw_df.values[::2, :],
    raw_df.values[1::2, :2]])
yt = raw_df.values[1::2, 2]
feature_names = np.array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX',
    'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO','B', 'LSTAT'])

# 結果確認
print('元データ', x_org.shape, yt.shape)
print('項目名: ', feature_names)
            </code></pre>
            <pre><code>
# データ絞り込み (項目 RMのみ)
x = x_org[:,feature_names == 'RM']
print('絞り込み後', x.shape)
print(x[:5,:])

# 正解データ yの表示
print('正解データ')
print(yt[:5])
            </code></pre>
            <pre><code>
# 散布図の表示

plt.scatter(x, yt, s=10, c='b')
plt.xlabel('部屋数')
plt.ylabel('価格')
plt.title('部屋数と価格の散布図')
plt.show()
            </code></pre>
            <h3>モデル定義</h3>
            <b>変数定義</b>
            <pre><code>
# 変数定義

# 入力次元数
n_input= x.shape[1]

# 出力次元数
n_output = 1

print(f'入力次元数: {n_input}  出力次元数: {n_output}')
            </code></pre>
            <p>機械学習・ディープラーニングモデルとは、「入力ベクトルに対して出力ベクトルを返す関数」である。</p>
            <p>今回のモデルは、１入力１出力のとてもシンプルなモデルである。</p>
            <b>機械学習モデル（予測モデル）のクラス定義</b>
            <pre><code>
# 機械学習モデル（予測モデル）クラス定義

class Net(nn.Module):
    def __init__(self, n_input, n_output):
        #  親クラスnn.Modulesの初期化呼び出し
        super().__init__()

        # 出力層の定義
        self.l1 = nn.Linear(n_input, n_output)

        # 初期値を全部1にする
        nn.init.constant_(self.l1.weight, 1.0)
        nn.init.constant_(self.l1.bias, 1.0)

    # 予測関数の定義
    def forward(self, x):
        x1 = self.l1(x) # 線形回帰
        return x1
            </code></pre>
            <p>PyTorchでは、モデル用のクラスの内部で必ずforward関数を定義し、入力テンソルinputsを入力として出力テンソルoutputsを出力するための処理を記述するルールになっている。</p>
            <b>インスタンス生成</b>
            <pre><code>
# インスタンスの生成
# １入力1出力の線形モデル

net = Net(n_input, n_output)
            </code></pre>
            <p>Netクラスのインスタンスであるnet変数を定義すると、次の呼び出し方で入力テンソルから、予測値である出力テンソルを取得できる。</p>
            <pre><code>
outputs = net(inputs)
            </code></pre>
            <b>モデル内の変数値表示</b>
            <p>このように生成したモデルを表す変数netでは、親クラスのnn.Module内で定義されている便利な機能（関数）を利用可能である。</p>
            <p>その一つとして、モデル内の変数名とその値を取得する関数named_parametersがある。</p>
            <pre><code>
# モデル内のパラメータの確認
# モデル内の変数取得にはnamed_parameters関数を利用する
# 結果の第1要素が名前、第2要素が値
#
# predict.weightとpredict.biasがあることがわかる
# 初期値はどちらも1.0になっている

for parameter in net.named_parameters():
    print(f'変数名: {parameter[0]}')
    print(f'変数値: {parameter[1].data}')
            </code></pre>
            <b>parameters関数の呼び出し</b>
            <p>もう一つの便利機能として、parameters関数もある。</p>
            <p>parameters関数は、「パラメータ変数」だけが名前なしにリスト形式で返ってくる。</p>
            <p>最適化関数のインスタンス生成において、最適化対象のパラメータをリストで渡すときによく使われる。</p>
            <pre><code>
# パラメータのリスト取得にはparameters関数を利用する

for parameter in net.parameters():
    print(parameter)
            </code></pre>
            <b>モデルの概要表示</b>
            <pre><code>
# モデルの概要表示

print(net)
            </code></pre>
            <pre><code>
# モデルのサマリー表示

from torchinfo import summary
summary(net, (1,))
            </code></pre>
            <p>summaryという関数を呼び出すが、引数としてnet変数そのものと、入力変数のサイズを指定する。</p>
            <b>損失関数と最適化関数の定義</b>
            <pre><code>
# 損失関数： 平均2乗誤差
criterion = nn.MSELoss()

# 学習率
lr = 0.01

# 最適化関数: 勾配降下法
optimizer = optim.SGD(net.parameters(), lr=lr)
            </code></pre>
            <h3>勾配降下法</h3>
            <b>入力値xと正解値ytのテンソル化</b>
            <pre><code>
# 入力変数x と正解値 ytのテンソル変数化

inputs = torch.tensor(x).float()
labels = torch.tensor(yt).float()

# 次元数確認

print(inputs.shape)
print(labels.shape)
            </code></pre>
            <p>正解値をテンソル変数化した変数labelsに関しては、これからMSELossのインスタンスであるcriterion関数に、予測値とともに渡して、損失を計算する。</p>
            <p>そのときに引数はN次元のベクトル形式でなく、(N, 1)次元の行列形式であることが必要である。</p>
            <p>そのため、view関数を利用してデータサイズを変更する。</p>
            <pre><code>
# 損失値計算用にlabels変数を(N,1)次元の行列に変換する

labels1 = labels.view((-1, 1))

# 次元数確認
print(labels1.shape)
            </code></pre>
            <p>あとは①予測計算、②損失計算、③勾配計算、④パラメータ修正、を繰り返せば勾配降下法が実現できる。</p>
            <b>①予測計算</b>
            <pre><code>
# 予測計算

outputs = net(inputs)
            </code></pre>
            <b>②損失計算</b>
            <pre><code>
#  損失計算
loss = criterion(outputs, labels1)

# 損失値の取得
print(f'{loss.item():.5f}')
            </code></pre>
            <b>計算グラフの可視化</b>
            <pre><code>
# 損失の計算グラフ可視化

g = make_dot(loss, params=dict(net.named_parameters()))
display(g)
            </code></pre>
            <b>③勾配計算</b>
            <pre><code>
# 予測計算
outputs = net(inputs)

# 損失計算
loss = criterion(outputs, labels1)

# 勾配計算
loss.backward()

# 勾配の結果が取得可能に
print(net.l1.weight.grad)
print(net.l1.bias.grad)
            </code></pre>
            <b>④パラメータ修正</b>
            <pre><code>
# パラメータ修正
optimizer.step()

# パラメータ値が変わる
print(net.l1.weight)
print(net.l1.bias)
            </code></pre>
            <p>勾配値の初期化も忘れずに。</p>
            <pre><code>
# 勾配値の初期化
optimizer.zero_grad()

# 勾配値がすべてゼロになっている
print(net.l1.weight.grad)
print(net.l1.bias.grad)
            </code></pre>
            <b>繰り返し計算</b>
            <p>まず、初期化処理の部分を示す。</p>
            <pre><code>
# 学習率
lr = 0.01

# インスタンス生成　(パラメータ値初期化)
net = Net(n_input, n_output)

# 損失関数： 平均2乗誤差
criterion = nn.MSELoss()

# 最適化関数: 勾配降下法
optimizer = optim.SGD(net.parameters(), lr=lr)

# 繰り返し回数
num_epochs = 50000

# 評価結果記録用 (損失関数値のみ記録)
history = np.zeros((0,2))
            </code></pre>
            <p>続いて、ループ処理の部分を示す。</p>
            <pre><code>
# 繰り返し計算メインループ

for epoch in range(num_epochs):

    # 勾配値初期化
    optimizer.zero_grad()

    # 予測計算
    outputs = net(inputs)

    # 損失計算
    loss = criterion(outputs, labels1) / 2.0

    # 勾配計算
    loss.backward()

    # パラメータ修正
    optimizer.step()

    # 100回ごとに途中経過を記録する
    if ( epoch % 100 == 0):
        history = np.vstack((history, np.array([epoch, loss.item()])))
        print(f'Epoch {epoch} loss: {loss.item():.5f}')
            </code></pre>
            <h3>結果確認</h3>
            <pre><code>
# 損失初期値と最終値

print(f'損失初期値: {history[0,1]:.5f}')
print(f'損失最終値: {history[-1,1]:.5f}')
            </code></pre>
            <pre><code>
# 学習曲線の表示 (損失)
# 最初の1つを除く

plt.plot(history[1:,0], history[1:,1], 'b')
plt.xlabel('繰り返し回数')
plt.ylabel('損失')
plt.title('学習曲線(損失)')
plt.show()
            </code></pre>
            <pre><code>
# 回帰直線の算出

# xの最小値、最大値
xse = np.array((x.min(), x.max())).reshape(-1,1)
Xse = torch.tensor(xse).float()

with torch.no_grad():
  Yse = net(Xse)

print(Yse.numpy())
            </code></pre>
            <pre><code>
# 散布図と回帰直線の描画

plt.scatter(x, yt, s=10, c='b')
plt.xlabel('部屋数')
plt.ylabel('価格')
plt.plot(Xse.data, Yse.data, c='k')
plt.title('散布図と回帰直線')
plt.show()
            </code></pre>
            <h3>重回帰モデルへの拡張</h3>
            <p>これまでは、入力変数が平均部屋数（RM）の１項目だけだったが、新しくLSTATを追加して２入力とする。</p>
            <p>新しい入力変数をx2とする。</p>
            <pre><code>
# 列(LSTAT: 低所得者率)の追加

x_add = x_org[:,feature_names == 'LSTAT']
x2 = np.hstack((x, x_add))

# shapeの表示
print(x2.shape)

# 入力データxの表示
print(x2[:5,:])
            </code></pre>
            <p>入力データの次元数n_inputが２になるため、モデルインスタンスを生成し直す。</p>
            <pre><code>
# 今度は入力次元数=2

n_input = x2.shape[1]
print(n_input)

# モデルインスタンスの生成
net = Net(n_input, n_output)
            </code></pre>
            <p>インスタンス変数net内のパラメータがどう変わったか、いくつかの方法で確認する。</p>
            <pre><code>
# モデル内のパラメータの確認
# predict.weight が2次元に変わった

for parameter in net.named_parameters():
    print(f'変数名: {parameter[0]}')
    print(f'変数値: {parameter[1].data}')
            </code></pre>
            <pre><code>
# モデルの概要表示

print(net)
            </code></pre>
            <pre><code>
# モデルのサマリー表示

from torchinfo import summary
summary(net, (2,))
            </code></pre>
            <p>param # が３となっているのは、weightが２個とbiasが１個という意味。</p>
            <p>新しい入力変数x2を改めてテンソルinputsに定義し直す。</p>
            <pre><code>
# 入力変数x2 のテンソル変数化
# labels, labels1は前のものをそのまま利用

inputs = torch.tensor(x2).float()
            </code></pre>
            <p>繰り返し計算は以下の通り。</p>
            <pre><code>
# 初期化処理

# 学習率
lr = 0.01

# インスタンス生成　(パラメータ値初期化)
net = Net(n_input, n_output)

# 損失関数： 平均2乗誤差
criterion = nn.MSELoss()

# 最適化関数: 勾配降下法
optimizer = optim.SGD(net.parameters(), lr=lr)

# 繰り返し回数
num_epochs = 50000

# 評価結果記録用 (損失関数値のみ記録)
history = np.zeros((0,2))
            </code></pre>
            <pre><code>
# 繰り返し計算メインループ

for epoch in range(num_epochs):

    # 勾配値初期化
    optimizer.zero_grad()

    # 予測計算
    outputs = net(inputs)

    # 誤差計算
    loss = criterion(outputs, labels1) / 2.0

    # 勾配計算
    loss.backward()

    # パラメータ修正
    optimizer.step()

    # 100回ごとに途中経過を記録する
    if ( epoch % 100 == 0):
        history = np.vstack((history, np.array([epoch, loss.item()])))
        print(f'Epoch {epoch} loss: {loss.item():.5f}')
            </code></pre>
            <p>損失計算の値がinf→nanになってしまうので、学習率の値を再設定してやり直す。</p>
            <h3>学習率の変更</h3>
            <pre><code>
# 繰り返し回数
#num_epochs = 50000
num_epochs = 2000

# 学習率
#l r = 0.01
lr = 0.001

# モデルインスタンスの生成
net = Net(n_input, n_output)

# 損失関数： 平均2乗誤差
criterion = nn.MSELoss()

# 最適化関数: 勾配降下法
optimizer = optim.SGD(net.parameters(), lr=lr)
            </code></pre>
            <pre><code>
# 繰り返し計算メインループ

# 評価結果記録用 (損失関数値のみ記録)
history = np.zeros((0,2))

for epoch in range(num_epochs):

    # 勾配値初期化
    optimizer.zero_grad()

    # 予測計算
    outputs = net(inputs)

    # 誤差計算
    loss = criterion(outputs, labels1) / 2.0

    #勾配計算
    loss.backward()

    # パラメータ修正
    optimizer.step()

    # 100回ごとに途中経過を記録する
    if ( epoch % 100 == 0):
        history = np.vstack((history, np.array([epoch, loss.item()])))
        print(f'Epoch {epoch} loss: {loss.item():.5f}')
            </code></pre>
            <pre><code>
# 損失初期値、最終値

print(f'損失初期値: {history[0,1]:.5f}')
print(f'損失最終値: {history[-1,1]:.5f}')
            </code></pre>
            <pre><code>
# 学習曲線の表示 (損失)

plt.plot(history[:,0], history[:,1], 'b')
plt.xlabel('繰り返し回数')
plt.ylabel('損失')
plt.title('学習曲線(損失)')
plt.show()
            </code></pre>
        </section>
        <section id="binary">
            <h2>Binary Classification</h2>
            <h3>問題の定義</h3>
            <p>「アイリス・データセット」を使用する。</p>
            <p>このデータセットは、Setosa、Versicolour、Virgincaという３種類のアヤメの花に対して、「花弁」と「がく片」のそれぞれの「長さ」と「幅」を測定した結果である。</p>
            <p>４種類のサイズ（長さまたは幅）を入力に、花の種類を予測するモデルを作るための学習データとしてよく利用される。</p>
            <p>まずは２種類の花だけを対象に、入力項目も２つだけに絞りこむ。</p>
            <p>２値ロジスティック回帰モデルを作る。</p>
            <p>「分類」モデルは、「精度」という指標値を導入することにより、「回帰」モデルよりもモデルの性能を判断しやすいという特徴がある。</p>
            <h3>精度（Accuracy）</h3>
            <p>（正解件数）/（全体件数）によって、モデルがどの程度の比率で正しく予測できているかを数値化する。</p>
            <h3>訓練データと検証データの分割</h3>
            <p>過学習をチェックするためには、検証データを事前に準備しておくことが重要。</p>
            <h3>シグモイド関数</h3>
            <pre><code>
# NumPy配列でxデータを定義
x_np = np.arange(-4, 4.1, 0.25)

# データをTensor形式に変換
x = torch.tensor(x_np).float()

# yの値を計算
y = torch.sigmoid(x)

# グラフ描画
plt.title('シグモイド関数のグラフ')
plt.plot(x.data, y.data)
plt.show()
            </code></pre>
            <p>シグモイド関数は、数式では次のように表される。</p>
            <div class="scroll">
                \begin{align}
                f(x) = \frac{1}{1 + \exp(-x)}
                \end{align}
            </div>
            <p>次の特徴を持つ。</p>
            <ul>
                <li>単調増加関数</li>
                <li>0と1の間の値を取る</li>
                <li>x = 0のときの値は0.5</li>
                <li>グラフは点(0, 0.5)に関して点対称</li>
            </ul>
            <p>この性質は、関数値を「確率」として解釈して利用するのに適している。</p>
            <p>この性質を利用することで２値分類モデルの予測の仕組みが実装される。</p>
            <p>入力ベクトルxをnn.Linearで実装される線形関数にかけて結果を得るところまでは、線形回帰モデルとまったく同じである。</p>
            <p>唯一異なるのは、線形関数の出力uに対してシグモイド関数をかけて、確率値を取得するところである。</p>
            <h3>交差エントロピー関数</h3>
            <p>線形回帰では２次関数だった損失関数は、分類モデルでは交差エントロピー関数になる。</p>
            <p>シグモイド関数の出力として得られる確率値は、厳密には「入力データに対して分類結果が１になる確率」である。</p>
            <p>２値分類の場合、正解値は１か０のいずれかなので、「分類結果が１になる確率」がf(u)なら、「分類結果が０になる確率」は1-f(u)で表されることになる。</p>
            <p>ここで、損失関数に最尤推定という考えを導入する。</p>
            <p>「すべてのデータに対する確信度（分類結果が正解の確率）を掛け合わせた結果を最大にするパラメータが一番もっともらしいので採用する」という考えである。</p>
            <p>対数尤度関数は次の通りである。</p>
            <div class="scroll">
                \begin{align}
                \sum_{i} {yt_i \cdot \log(f(u_i)) + (1 - u_i) \log(1 - f(u_i))}
                \end{align}
            </div>
            <p>損失関数は、できるだけ小さくすることが目標なので、この式にマイナスをかけ、さらにデータ件数で割って平均を取ったものが、交差エントロピー関数と呼ばれる、２値分類モデルで損失関数として利用される関数になる。</p>
            <p>PyTorchで２値分類用の交差エントロピー関数を利用する場合、nn.BCELossというクラスを利用する。</p>
            <h3>データ準備</h3>
            <b>データ読み込み</b>
            <pre><code>
# 学習用データ準備

# ライブラリのインポート
from sklearn.datasets import load_iris

# データ読み込み
iris = load_iris()

# 入力データと正解データ取得
x_org, y_org = iris.data, iris.target

# 結果確認
print('元データ', x_org.shape, y_org.shape)
            </code></pre>
            <b>データ絞り込み</b>
            <p>次にデータを絞り込む。絞り込みは行方向と列方向（xのみ対象）に対して行う。</p>
            <p>アイリス・データセットは元々150行のデータがあるが、これを先頭から100行に限定すると、正解値がSetosa、Versicolourのみのデータになる。</p>
            <p>xの列方向も最初の２列のみ（がく片sepalの長さと幅）に絞り込む。</p>
            <pre><code>
# データ絞り込み
#   クラス0, 1のみ
#   項目sepal_lengthとsepal_widthのみ

x_data = iris.data[:100,:2]
y_data = iris.target[:100]

# 結果確認
print('対象データ', x_data.shape, y_data.shape)
            </code></pre>
            <b>訓練データと検証データへの分割</b>
            <pre><code>
# 　元データのサイズ
print(x_data.shape, y_data.shape)

# 訓練データ、検証データに分割 (シャフルも同時に実施)
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(
    x_data, y_data, train_size=70, test_size=30,
    random_state=123)
print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)
            </code></pre>
            <p>順番を変えずに分割すると、データに偏りが出てしまいがちだが、同時にシャッフルもしているので問題ない。</p>
            <p>また、random_stateパラメータを指定すると、シャッフルする際の乱数の種も固定になるので、分割結果も常に同じになる。</p>
            <b>散布図表示</b>
            <pre><code>
# 散布図の表示

x_t0 = x_train[y_train == 0]
x_t1 = x_train[y_train == 1]
plt.scatter(x_t0[:,0], x_t0[:,1], marker='x', c='b', label='0 (setosa)')
plt.scatter(x_t1[:,0], x_t1[:,1], marker='o', c='k', label='1 (versicolor)')
plt.xlabel('sepal_length')
plt.ylabel('sepal_width')
plt.legend()
plt.show()
            </code></pre>
            <h3>モデル定義</h3>
            <p>まずは、入力次元数と出力次元数の定義を行う。</p>
            <pre><code>
# 入力次元数　(今の場合2)
n_input= x_train.shape[1]

# 出力次元数
n_output = 1

# 結果確認
print(f'n_input: {n_input}  n_output:{n_output}')
            </code></pre>
            <p>以下でモデル定義を行う。</p>
            <pre><code>
# モデルの定義
# 2入力1出力のロジスティック回帰モデル

class Net(nn.Module):
    def __init__(self, n_input, n_output):
        super().__init__()
        self.l1 = nn.Linear(n_input, n_output)
        self.sigmoid = nn.Sigmoid()

        # 初期値を全部1にする
        self.l1.weight.data.fill_(1.0)
        self.l1.bias.data.fill_(1.0)

    # 予測関数の定義
    def forward(self, x):
        # 最初に入力値を線形関数にかけたを計算する
        x1 = self.l1(x)
        # 計算結果にシグモイド関数をかける
        x2 = self.sigmoid(x1)
        return x2
            </code></pre>
            <p>次に、インスタンス生成、モデル内のパラメータ確認を行う。</p>
            <pre><code>
# インスタンスの生成

net = Net(n_input, n_output)
            </code></pre>
            <pre><code>
# モデル内のパラメータの確認
# l1.weightとl1.biasがあることがわかる

for parameter in net.named_parameters():
    print(parameter)
            </code></pre>
            <pre><code>
# モデルの概要表示

print(net)
            </code></pre>
            <pre><code>
# モデルのサマリー表示

summary(net, (2,))
            </code></pre>
            <b>最適化アルゴリズムと損失関数の定義</b>
            <pre><code>
# 損失関数： 交差エントロピー関数
criterion = nn.BCELoss()

# 学習率
lr = 0.01

# 最適化関数: 勾配降下法
optimizer = optim.SGD(net.parameters(), lr=lr)
            </code></pre>
            <h3>勾配降下法</h3>
            <b>入力データと正解データのテンソル化</b>
            <p>訓練に使うinputsとlabelsだけでなく、検証に使うinputs_testとlabels_testも精度評価用に準備しておく。</p>
            <p>損失関数BCELossを使用する場合、正解データとしての第２引数は、第１引数とshapeがそろっている必要がある。</p>
            <p>そのため、BCELoss用の変数としてlabels1といlabels_testも用意しておく。</p>
            <pre><code>
# 入力データ x_train と正解データ y_train のテンソル化

inputs = torch.tensor(x_train).float()
labels = torch.tensor(y_train).float()

# 正解データはN行1列の行列に変換する
labels1 = labels.view((-1,1))

# 検証データのテンソル化
inputs_test = torch.tensor(x_test).float()
labels_test = torch.tensor(y_test).float()

# 検証用の正解データもN行1列の行列に変換する
labels1_test = labels_test.view((-1,1))
            </code></pre>
            <pre><code>
# 予測計算
outputs = net(inputs)

# 損失計算
loss = criterion(outputs, labels1)

# 損失の計算グラフ可視化
g = make_dot(loss, params=dict(net.named_parameters()))
display(g)
            </code></pre>
            <b>初期化処理</b>
            <pre><code>
# 学習率
lr = 0.01

# 初期化
net = Net(n_input, n_output)

# 損失関数： 交差エントロピー関数
criterion = nn.BCELoss()

# 最適化関数: 勾配降下法
optimizer = optim.SGD(net.parameters(), lr=lr)

# 繰り返し回数
num_epochs = 10000

# 記録用リストの初期化
history = np.zeros((0,5))
            </code></pre>
            <p>history変数が５列なのは、以下の情報を含めるからである。</p>
            <ol>
                <li>繰り返し数</li>
                <li>訓練データの損失</li>
                <li>訓練データの精度</li>
                <li>検証データの損失</li>
                <li>検証データの精度</li>
            </ol>
            <b>メインループ</b>
            <pre><code>
# 繰り返し計算メインループ

for epoch in range(num_epochs):
    # 訓練フェーズ

    #勾配値初期化
    optimizer.zero_grad()

    # 予測計算
    outputs = net(inputs)

    # 損失計算
    loss = criterion(outputs, labels1)

    # 勾配計算
    loss.backward()

    # パラメータ修正
    optimizer.step()

    # 損失の保存(スカラー値の取得)
    train_loss = loss.item()

    # 予測ラベル(1 or 0)計算
    predicted = torch.where(outputs < 0.5, 0, 1)

    # 精度計算
    train_acc = (predicted == labels1).sum() / len(y_train)

    # 予測フェーズ

    # 予測計算
    outputs_test = net(inputs_test)

    # 損失計算
    loss_test = criterion(outputs_test, labels1_test)

    # 損失の保存（スカラー値の取得）
    val_loss =  loss_test.item()

    # 予測ラベル(1 or 0)計算
    predicted_test = torch.where(outputs_test < 0.5, 0, 1)

    # 精度計算
    val_acc = (predicted_test == labels1_test).sum() / len(y_test)

    if ( epoch % 10 == 0):
        print (f'Epoch [{epoch}/{num_epochs}], loss: {train_loss:.5f} acc: {train_acc:.5f} val_loss: {val_loss:.5f}, val_acc: {val_acc:.5f}')
        item = np.array([epoch, train_loss, train_acc, val_loss, val_acc])
        history = np.vstack((history, item))
            </code></pre>
            <p><code2>train_loss = loss.item()</code2>では、損失をスカラー化してtrain_lossに保存している（item関数を使うと、テンソルから数値を取り出せる）。</p>
            <h3>結果確認</h3>
            <p>まず、損失と精度を確認する。</p>
            <pre><code>
#損失と精度の確認

print(f'初期状態: 損失: {history[0,3]:.5f} 精度: {history[0,4]:.5f}' )
print(f'最終状態: 損失: {history[-1,3]:.5f} 精度: {history[-1,4]:.5f}' )
            </code></pre>
            <p>次に、学習曲線を可視化する。</p>
            <pre><code>
# 学習曲線の表示 (損失)

plt.plot(history[:,0], history[:,1], 'b', label='訓練')
plt.plot(history[:,0], history[:,3], 'k', label='検証')
plt.xlabel('繰り返し回数')
plt.ylabel('損失')
plt.title('学習曲線(損失)')
plt.legend()
plt.show()
            </code></pre>
            <pre><code>
# 学習曲線の表示 (精度)

plt.plot(history[:,0], history[:,2], 'b', label='訓練')
plt.plot(history[:,0], history[:,4], 'k', label='検証')
plt.xlabel('繰り返し回数')
plt.ylabel('精度')
plt.title('学習曲線(精度)')
plt.legend()
plt.show()
            </code></pre>
            <p>今回のロジスティック回帰モデルでは、２つの分類結果の境界になる直線が存在し、その直線のことを決定境界と呼ぶ。</p>
            <pre><code>
# 検証データを散布図用に準備

x_t0 = x_test[y_test==0]
x_t1 = x_test[y_test==1]
            </code></pre>
            <pre><code>
# パラメータの取得

bias = net.l1.bias.data.numpy()
weight = net.l1.weight.data.numpy()
print(f'BIAS = {bias}, WEIGHT = {weight}')

# 決定境界描画用 x1の値から x2の値を計算する
def decision(x):
    return(-(bias + weight[0,0] * x)/ weight[0,1])

# 散布図のx1の最小値と最大値
xl = np.array([x_test[:,0].min(), x_test[:,0].max()])
yl = decision(xl)

# 結果確認
print(f'xl = {xl}  yl = {yl}')
            </code></pre>
            <pre><code>
# 散布図表示
plt.scatter(x_t0[:,0], x_t0[:,1], marker='x',
        c='b', s=50, label='class 0')
plt.scatter(x_t1[:,0], x_t1[:,1], marker='o',
        c='k', s=50, label='class 1')

# 決定境界直線
plt.plot(xl, yl, c='b')
plt.xlabel('sepal_length')
plt.ylabel('sepal_width')
plt.legend()
plt.show()
            </code></pre>
            <h3>BCEWithLogitsLoss関数</h3>
            <p>BCELossとよく似た関数として、BCEWithLogitsLossがある。</p>
            <pre><code>
# モデルの定義
# 2入力1出力のロジスティック回帰モデル

class Net(nn.Module):
    def __init__(self, n_input, n_output):
        super().__init__()
        self.l1 = nn.Linear(n_input, n_output)

        # 初期値を全部1にする
        # 「ディープラーニングの数学」と条件を合わせる目的
        self.l1.weight.data.fill_(1.0)
        self.l1.bias.data.fill_(1.0)

    # 予測関数の定義
    def forward(self, x):
        # 入力値と行列の積を計算する
        x1 = self.l1(x)
        return x1
            </code></pre>
            <pre><code>
# 学習率
lr = 0.01

# 初期化
net = Net(n_input, n_output)

# 損失関数： logits付き交差エントロピー関数
criterion = nn.BCEWithLogitsLoss()

# 最適化関数: 勾配降下法
optimizer = optim.SGD(net.parameters(), lr=lr)

# 繰り返し回数
num_epochs = 10000

# 記録用リストの初期化
history = np.zeros((0,5))
            </code></pre>
            <pre><code>
# 繰り返し計算メインループ

for epoch in range(num_epochs):
    # 訓練フェーズ

    #勾配値初期化
    optimizer.zero_grad()

    # 予測計算
    outputs = net(inputs)

    # 損失計算
    loss = criterion(outputs, labels1)

    # 勾配計算
    loss.backward()

    # パラメータ修正
    optimizer.step()

    # 損失のスカラー化
    train_loss = loss.item()

    # 予測ラベル(1 or 0)計算
    predicted = torch.where(outputs < 0.0, 0, 1)

    # 精度計算
    train_acc = (predicted == labels1).sum() / len(y_train)

    # 予測フェーズ

    # 予測計算
    outputs_test = net(inputs_test)

    # 損失計算
    loss_test = criterion(outputs_test, labels1_test)

    # 損失のスカラー化
    val_loss =  loss_test.item()

    #予測ラベル(1 or 0)計算
    predicted_test = torch.where(outputs_test < 0.0, 0, 1)

    # 精度計算
    val_acc = (predicted_test == labels1_test).sum() / len(y_test)

    if ( epoch % 10 == 0):
        print (f'Epoch [{epoch}/{num_epochs}], loss: {train_loss:.5f} acc: {train_acc:.5f} val_loss: {val_loss:.5f}, val_acc: {val_acc:.5f}')
        item = np.array([epoch, train_loss, train_acc, val_loss, val_acc])
        history = np.vstack((history, item))
            </code></pre>
            <p>BCELWithLogitsLossは、シグモイド関数の後で、交差エントロピー関数を呼び出す。</p>
            <p>BCELossを使う場合と異なり、モデルの出力がどちらのクラスに属しているかの基準は0.5より大きいかどうかでなく、0より大きいかどうかになる。</p>
            <p>「指数関数（シグモイド関数などに含まれる）と対数関数（交差エントロピー関数などに含まれる）を独立して計算すると結果が不安定になりやすいから、極力セットで計算すべき」というポリシーがPyTorchにはある。</p>
        </section>
        <section id="multi">
            <h2>Multiclass Classification</h2>
            <h3>問題の定義</h3>
            <p>ここでも、「アイリス・データセット」を利用する。</p>
            <p>３種類すべてのアヤメの学習データを利用する。</p>
            <p>入力項目は、最初は２個に絞り込む。</p>
            <p>多値分類モデルでは、分類先グループ数をNとしたときに、N次元出力になる。</p>
            <h3>複数の分類器</h3>
            <p>次の考え方を用いる。</p>
            <ul>
                <li>モデルの出力を１次元でなく、Ｎ次元とする。</li>
                <li>Ｎ個の出力（分類器）は、それぞれ分類先のグループと１対１に割り当てる。</li>
                <li>個々の分類器の出力が、入力データがそのグループである確率を表すものとする。</li>
                <li>確率値の一番大きい分類器に該当するグループがモデルとしての予測値になる。</li>
            </ul>
            <h3>softmax関数</h3>
            <p>softmax関数は、数式では以下のように表される。</p>
            <div class="scroll">
                \begin{align}
                y_i = \frac{\exp(x_i)}{\sum_{k=1}^{n} \exp(x_k)} \quad (i = 1, 2, \cdots, n)
                \end{align}
            </div>
            <p>シグモイド関数同様に、出力を確率値として扱える性質を持っている。</p>
            <p>softmax関数は、入力値として一番大きな項目の確率値が一番高くなる（さらに全確率値を足すと１になる）。</p>
            <p>しかし、他の項目の確率値も完全にゼロになるわけではなく、ある程度の値が残る。</p>
            <p>単純に最大値を調べる関数なら、一番大きな値の項目が確率値を全取り（確率値＝１）して、残りの項目の確率値はゼロになる。</p>
            <p>そこまでしないソフトな最大値関数というのが、この関数名の由来になっている。</p>
            <h3>交差エントロピー関数</h3>
            <p>２値分類のときと同様に、損失関数には、交差エントロピー関数を利用する。</p>
            <p>しかし、名前が同じでも、数式が多少異なる。</p>
            <div class="scroll">
                \begin{align}
                \sum_{i=0}^{N-1} (yt_i \log(yp_i))
                \end{align}
            </div>
            <p>ここで、\(yt_i\)は、正解のときに１、正解でないときに０の値をとる。</p>
            <p>交差エントロピー関数は、まずsoftmax関数の出力である中間テンソル\(x2\)のすべての要素に対して対数計算をする。</p>
            <p>その後で、正解値の要素だけを抽出する。</p>
            <p>「正解値の要素だけ抽出」という操作をするには、正解値が、（0, 1, 2）のどれかという整数値で与えられる必要がある。</p>
            <p>PyTorchの交差エントロピー関数は、このような操作をする目的で、損失関数に渡す正解値（第２引数）は整数値から構成される値である必要がある。</p>
            <h3>予測関数と損失関数の関係</h3>
            <p>予測関数側では活性化関数は不要で、線形関数の出力をそのまま出力とする。</p>
            <p>予測関数出力から確率値を得たい場合は、予測関数出力にsoftmax関数をかける。</p>
            <p>損失関数はCrossEntropyLoss関数を利用する。</p>
            <h3>データ準備</h3>
            <pre><code>
# 学習用データ準備

# ライブラリのインポート
from sklearn.datasets import load_iris

# データ読み込み
iris = load_iris()

# 入力データと正解データ取得
x_org, y_org = iris.data, iris.target

# 結果確認
print('元データ', x_org.shape, y_org.shape)
            </code></pre>
            <b>データ絞り込み</b>
            <pre><code>
# データ絞り込み

# 入力データに関しては、sepal length(0)とpetal length(2)のみ抽出
x_select = x_org[:,[0,2]]

# 結果確認
print('元データ', x_select.shape, y_org.shape)
            </code></pre>
            <b>訓練データと検証データの分割</b>
            <pre><code>
# 訓練データ、検証データに分割 (シャフルも同時に実施)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(
    x_select, y_org, train_size=75, test_size=75,
    random_state=123)
print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)
            </code></pre>
            <b>散布図の表示</b>
            <pre><code>
# データを正解値ごとに分割

x_t0 = x_train[y_train == 0]
x_t1 = x_train[y_train == 1]
x_t2 = x_train[y_train == 2]
            </code></pre>
            <pre><code>
# 散布図の表示

plt.scatter(x_t0[:,0], x_t0[:,1], marker='x', c='k', s=50, label='0 (setosa)')
plt.scatter(x_t1[:,0], x_t1[:,1], marker='o', c='b', s=50, label='1 (versicolour)')
plt.scatter(x_t2[:,0], x_t2[:,1], marker='+', c='k', s=50, label='2 (virginica)')
plt.xlabel('sepal_length')
plt.ylabel('petal_length')
plt.legend()
plt.show()
            </code></pre>
            <h3>モデルの定義</h3>
            <b>入力次元数と出力次元数の確認</b>
            <pre><code>
# 学習用パラメータ設定

# 入力次元数
n_input = x_train.shape[1]

# 出力次元数
# 分類先クラス数　今回は3になる
n_output = len(list(set(y_train)))

# 結果確認
print(f'n_input: {n_input}  n_output: {n_output}')
            </code></pre>
            <b>モデル定義</b>
            <pre><code>
# モデルの定義
# 2入力3出力のロジスティック回帰モデル

class Net(nn.Module):
    def __init__(self, n_input, n_output):
        super().__init__()
        self.l1 = nn.Linear(n_input, n_output)

        # 初期値を全部1にする
        self.l1.weight.data.fill_(1.0)
        self.l1.bias.data.fill_(1.0)

    def forward(self, x):
        x1 = self.l1(x)
        return x1

# インスタンスの生成
net = Net(n_input, n_output)
            </code></pre>
            <b>モデル内のパラメータ確認</b>
            <pre><code>
# モデル内のパラメータの確認
# l1.weightが行列にl1.biasがベクトルになっている

for parameter in net.named_parameters():
    print(parameter)
            </code></pre>
            <pre><code>
# モデルの概要表示

print(net)
            </code></pre>
            <pre><code>
# モデルのサマリー表示

summary(net, (2,))
            </code></pre>
            <b>最適化アルゴリズムと損失関数定義</b>
            <pre><code>
# 損失関数： 交差エントロピー関数
criterion = nn.CrossEntropyLoss()

# 学習率
lr = 0.01

# 最適化関数: 勾配降下法
optimizer = optim.SGD(net.parameters(), lr=lr)
            </code></pre>
            <p>損失関数定義にnn.CrossEntropyLossクラスを利用している。</p>
            <p>このクラスで生成した損失関数は、「softmax関数」「対数関数」「正解値の抽出」の３つを全部まとめてやってしまう関数となっている。</p>
            <h3>勾配降下法</h3>
            <b>データのテンソル変数化</b>
            <pre><code>
# 入力変数x_trainと正解値 y_trainのテンソル変数化

inputs = torch.tensor(x_train).float()
labels = torch.tensor(y_train).long()

# 検証用変数のテンソル変数化

inputs_test = torch.tensor(x_test).float()
labels_test = torch.tensor(y_test).long()
            </code></pre>
            <p>loss = criterion(outputs, labels)において、第２引数は整数でないといけないため、long関数を呼び出している。</p>
            <b>損失の計算グラフ可視化</b>
            <pre><code>
# 予測計算
outputs = net(inputs)

#  損失計算
loss = criterion(outputs, labels)

# 損失の計算グラフ可視化
g = make_dot(loss, params=dict(net.named_parameters()))
display(g)
            </code></pre>
            <b>予測ラベルの取得方法</b>
            <p>softmax関数は入力の段階で最大であった項目が出力後も最大になる性質を持っている。</p>
            <p>そのため、softmax関数の前の状態で最大の値を持つ項目を見つければ、それが予測値ラベルになる。</p>
            <pre><code>
# torch.max関数呼び出し
# 2つめの引数は軸を意味している。1だと行ごとの集計

print(torch.max(outputs, 1))
            </code></pre>
            <p>torch.max関数は、最大値そのものと最大値をとったインデックスの２つを同時に返す仕様になっている。</p>
            <p>ラベル値を取得したい場合は、２つ目のindicesをとってくればよい。</p>
            <pre><code>
# ラベル値の配列を取得
torch.max(outputs, 1)[1]
            </code></pre>
            <b>繰り返し処理</b>
            <pre><code>
# 学習率
lr = 0.01

# 初期化
net = Net(n_input, n_output)

# 損失関数： 交差エントロピー関数
criterion = nn.CrossEntropyLoss()

# 最適化関数: 勾配降下法
optimizer = optim.SGD(net.parameters(), lr=lr)

# 繰り返し回数
num_epochs = 10000

# 評価結果記録用
history = np.zeros((0,5))
            </code></pre>
            <pre><code>
# 繰り返し計算メインループ

for epoch in range(num_epochs):

    # 訓練フェーズ

    #勾配の初期化
    optimizer.zero_grad()

    # 予測計算
    outputs = net(inputs)

    # 損失計算
    loss = criterion(outputs, labels)

    # 勾配計算
    loss.backward()

    # パラメータ修正
    optimizer.step()

    # 予測ラベル算出
    predicted = torch.max(outputs, 1)[1]

    # 損失と精度の計算
    train_loss = loss.item()
    train_acc = (predicted == labels).sum()  / len(labels)

    #予測フェーズ

    # 予測計算
    outputs_test = net(inputs_test)

    # 損失計算
    loss_test = criterion(outputs_test, labels_test)

    # 予測ラベル算出
    predicted_test = torch.max(outputs_test, 1)[1]

    # 損失と精度の計算
    val_loss =  loss_test.item()
    val_acc =  (predicted_test == labels_test).sum() / len(labels_test)

    if ((epoch) % 10 == 0):
        print (f'Epoch [{epoch}/{num_epochs}], loss: {train_loss:.5f} acc: {train_acc:.5f} val_loss: {val_loss:.5f}, val_acc: {val_acc:.5f}')
        item = np.array([epoch, train_loss, train_acc, val_loss, val_acc])
        history = np.vstack((history, item))
            </code></pre>
            <h3>結果確認</h3>
            <pre><code>
#損失と精度の確認

print(f'初期状態: 損失: {history[0,3]:.5f} 精度: {history[0,4]:.5f}' )
print(f'最終状態: 損失: {history[-1,3]:.5f} 精度: {history[-1,4]:.5f}' )
            </code></pre>
            <pre><code>
# 学習曲線の表示 (損失)

plt.plot(history[:,0], history[:,1], 'b', label='訓練')
plt.plot(history[:,0], history[:,3], 'k', label='検証')
plt.xlabel('繰り返し回数')
plt.ylabel('損失')
plt.title('学習曲線(損失)')
plt.legend()
plt.show()
            </code></pre>
            <pre><code>
# 学習曲線の表示 (精度)

plt.plot(history[:,0], history[:,2], 'b', label='訓練')
plt.plot(history[:,0], history[:,4], 'k', label='検証')
plt.xlabel('繰り返し回数')
plt.ylabel('精度')
plt.title('学習曲線(精度)')
plt.legend()
plt.show()
            </code></pre>
        </section>
        <section id="mnist">
            <h2>MNIST</h2>
            <h3>問題の定義</h3>
            <p>MNISTを使用する。</p>
            <p>様々なパターンの手書き数字が、訓練用に６万枚、検証用に１万枚用意されていて、ディープラーニングの実習によく用いられる。</p>
            <p>画像データは縦、横それぞれ28画素ある。画素ごとの色の濃淡は、0から255までの整数値で表される。</p>
            <p>ただし、実習で利用するPyTorchのライブラリ経由で取得する場合は、[0, 1]の範囲の浮動小数点表現になっている。</p>
            <p>PyTorchで取得する場合、このデータは[1, 28, 28]の３階テンソルになっている。最初の１はカラー画像を意識した色次元を意味している。</p>
            <p>この形式の画像データは、CNNであれば、このままの状態で機械学習モデルに入力できる。</p>
            <p>しかし、今回扱う「全結合型ニューラルネットワーク」の場合、入力は１階テンソル（ベクトル）形式になっていることが前提になる。</p>
            <p>そこで、今回は入力データに加工を施すことで、784(28×28)要素の１次元配列に展開した形式のデータを入力とする。</p>
            <p>数字は全部で10種類あるので、分類先のクラス数は10個である。</p>
            <p>今回作るモデルは、入力784次元、出力10次元の隠れ層ありニューラルネットワークである。</p>
            <h3>GPUの利用</h3>
            <b>GPUの存在チェック</b>
            <pre><code>
# デバイスの割り当て
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
            </code></pre>
            <b>GPU利用のルール</b>
            <p>PyTorchでGPUを利用する場合のルールは以下の通り。</p>
            <ol>
                <li>テンソル変数はデータがCPU / GPU上のどちらにあるのかを属性として持っている。</li>
                <li>CPUとGPU間でデータはto関数で転送する。</li>
                <li>２つの変数が両方ともGPU上にある場合、演算はGPUで行われる。</li>
                <li>変数の片方がCPU、もう一方がGPUの場合、演算はエラーになる。</li>
            </ol>
            <h3>データ前処理</h3>
            <p>PyTorchでは、torchvision.transformersというライブラリに、前処理に便利な部品がそろっていて、この部品を組み合わせて使うだけで、簡単に望む形式のデータへ変換できる。</p>
            <h3>ミニバッチ学習法</h3>
            <p>事前に決めた数でグループを作り、このグループ単位で勾配計算をする方法。グループのメンバーを選ぶ際に乱数を使うため、繰り返し処理ごとに別のグループができる。</p>
            <p>こうすることで、勾配降下法の計算結果が局所最適化にとどまってしまうことを避けられる。</p>
            <p>元の学習データ全件でまとめて勾配計算する方法を「バッチ学習法」と呼ぶのに対して、この学習法は「ミニバッチ学習法」と呼ばれる。</p>
            <p>PyTorchでは、簡単にこのミニバッチ学習法が使えるデータローダーという仕組みが用意されている。</p>
            <h3>データセットによる読み込み</h3>
            <p>PyTorchでは、データ準備のための道具立てが行き届いている。</p>
            <ul>
                <li>データ入手：Dataset（データセット）</li>
                <li>データ加工：Transforms</li>
                <li>ミニバッチ用データセット生成：DataLoader（データローダー）</li>
            </ul>
            <b>データ取得</b>
            <pre><code>
# ライブラリインポート
import torchvision.datasets as datasets

# ダウンロード先ディレクトリ名
data_root = './data'

train_set0 = datasets.MNIST(
    # 元データダウンロード先の指定
    root = data_root,
    # 訓練データか検証データか
    train = True,
    # 元データがない場合にダウンロードするか
    download = True)
            </code></pre>
            <p>データセットクラスのインスタンスの１つであるdatasets.MNISTを利用して、train_set0という変数に読み込んでいる。</p>
            <p>データセットクラスを使って読み込んだデータは、Pythonで手軽に扱えるようになっている。</p>
            <p>以下のコードで、データがどのようなファイルとしてダウンロードされたかを確認する。</p>
            <pre><code>
# ダウンロードしたファイルの確認

!ls -lR ./data/MNIST
            </code></pre>
            <p>読み込んだデータセットであるtrain_set0は、Pythonのリストとして、（入力データ、正解データ）のセットを順に取得できる。</p>
            <p>以下のコードは、データセットの最初の要素を入力データimageと正解データlabelに代入し、それぞれの型をtype関数で調べたものである。</p>
            <pre><code>
# データ件数の確認
print('データ件数: ', len(train_set0))

# 最初の要素の取得
image, label = train_set0[0]

# データ型の確認
print('入力データの型: ', type(image))
print('正解データの型: ', type(label))
            </code></pre>
            <p>データの内容を確認する。</p>
            <pre><code>
# 入力データの画像表示

plt.figure(figsize=(2,3))
plt.title(f'{label}')
plt.imshow(image, cmap='gray_r')
plt.axis('off')
plt.show()
            </code></pre>
            <p>最初の20個のデータを正解データ付きで表示してみる。</p>
            <pre><code>
# 正解データ付きで、最初の20個をイメージ表示

plt.figure(figsize=(10, 3))
for i in range(20):
    ax = plt.subplot(2, 10, i + 1)

    # image と labelの取得
    image, label = train_set0[i]

    # イメージ表示
    plt.imshow(image, cmap='gray_r')
    ax.set_title(f'{label}')
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()
            </code></pre>
            <h3>Transformsによるデータ前処理</h3>
            <b>ToTensorの利用</b>
            <p>入力データの形式をPyTorchで扱えるテンソル形式に変換する機能。</p>
            <pre><code>
# ライブラリインポート
import torchvision.transforms as transforms

transform1 = transforms.Compose([
    # データのTensor化
    transforms.ToTensor(),
])

train_set1 = datasets.MNIST(
    root=data_root,  train=True,  download=True,
    transform = transform1)
            </code></pre>
            <p>transform1というインスタンスを定義し、その中でToTensorクラスを呼び出している。</p>
            <p>さらに、このtransform1をtrain_setのtransformオプションとして指定している。</p>
            <p>変換後のデータセットのうち入力データ側がどうなったかを調べる。</p>
            <pre><code>
# 変換結果の確認

image, label = train_set1[0]
print('入力データの型: ', type(image))
print('入力データのshape: ', image.shape)
print('最小値: ', image.data.min())
print('最大値: ', image.data.max())
            </code></pre>
            <b>Normalizeの利用</b>
            <p>データを正規化する。今回のデータは既に値の範囲が[0, 1]になるように正規化されているが、データ範囲を[-1, 1]に変更する。</p>
            <p>Normalize(\(\mu, \sigma\))により、元のデータ\(x\)は\(X = \frac{x - \mu}{\sigma}\)に変換される。</p>
            <p>\(\mu = \sigma = 0.5\)とすると、[0, 1]の範囲の値\(x\)を[-1, 1]の範囲の値\(X\)に変換できる。</p>
            <pre><code>
transform2 = transforms.Compose([
    # データのTensor化
    transforms.ToTensor(),

    # データの正規化
    transforms.Normalize(0.5,  0.5),
])

train_set2 = datasets.MNIST(
    root = data_root,  train = True,  download = True,
    transform = transform2)
            </code></pre>
            <p>Composeというクラスに複数の部品をリスト形式で渡すと、それぞれの処理を順次行うtransformを作ることができる。</p>
            <p>以下のコードで結果を確認する。</p>
            <pre><code>
# 変換結果の確認

image, label = train_set2[0]
print('shape: ', image.shape)
print('最小値: ', image.data.min())
print('最大値: ', image.data.max())
            </code></pre>
            <b>Lambdaクラスを利用して１次元化</b>
            <p>全結合ニューラルネットワークの入力にできるよう、入力変数のshapeを元の[1, 28, 28]から[784]に変更する。</p>
            <pre><code>
transform3 = transforms.Compose([
    # データのTensor化
    transforms.ToTensor(),

    # データの正規化
    transforms.Normalize(0.5, 0.5),

    # Tensorの1階テンソル化
    transforms.Lambda(lambda x: x.view(-1)),
])

train_set3 = datasets.MNIST(
    root = data_root,  train = True,
    download=True, transform = transform3)
            </code></pre>
            <p>結果を確認する。</p>
            <pre><code>
# 変換結果の確認

image, label = train_set3[0]
print('shape: ', image.shape)
print('最小値: ', image.data.min())
print('最大値: ', image.data.max())
            </code></pre>
            <b>最終的な実装</b>
            <p>最終的なTransformsとデータセットの定義は以下の通り。</p>
            <pre><code>
# データ変換用関数 Transforms
# (1) Imageをテンソル化
# (2) [0, 1]の範囲の値を[-1, 1]の範囲にする
# (3) データのshapeを[1, 28, 28]から[784]に変換

transform = transforms.Compose([
    # (1) データのテンソル化
    transforms.ToTensor(),

    # (2) データの正規化
    transforms.Normalize(0.5, 0.5),

    # (3) 1階テンソルに変換
    transforms.Lambda(lambda x: x.view(-1)),
])
            </code></pre>
            <pre><code>
# データ取得用関数 Dataset

# 訓練用データセットの定義
train_set = datasets.MNIST(
    root = data_root, train = True,
    download = True, transform = transform)

# 検証データセットの定義
test_set = datasets.MNIST(
    root = data_root, train = False,
    download = True, transform = transform)
            </code></pre>
            <p>train_setとtest_setという２種類のデータセットを定義している。</p>
            <p>インスタンス生成時のオプションであるtrain=Falseをつけると、検証データのデータセットを読みだせる。</p>
            <h3>データローダーによるミニバッチ用データ生成</h3>
            <pre><code>
# ライブラリインポート
from torch.utils.data import DataLoader

# ミニバッチのサイズ指定
batch_size = 500

# 訓練用データローダー
# 訓練用なので、シャッフルをかける
train_loader = DataLoader(
    train_set, batch_size = batch_size,
    shuffle = True)

# 検証用データローダー
# 検証時にシャッフルは不要
test_loader = DataLoader(
    test_set,  batch_size = batch_size,
    shuffle = False)
            </code></pre>
            <p>定義したデータローダーは<code2>for inputs, labels in train_loader:</code2>のような形でループを定義すると、今回の例でいうと、６万件の学習データをbatch_size（ここでは500）分の小さなグループに分けてくれる。</p>
            <pre><code>
# 何組のデータが取得できるか
print(len(train_loader))

# DataLoaderから最初の1セットを取得する
for images, labels in train_loader:
    break

print(images.shape)
print(labels.shape)
            </code></pre>
            <p>ここで取得したimagesとlabelsに対して、その先頭20個の画像と正解データを表示する。</p>
            <pre><code>
# イメージ表示
plt.figure(figsize=(10, 3))
for i in range(20):
    ax = plt.subplot(2, 10, i + 1)

    # numpyに変換
    image = images[i].numpy()
    label = labels[i]

    # imgの範囲を[0, 1]に戻す
    image2 = (image + 1)/ 2
    # イメージ表示
    plt.imshow(image2.reshape(28, 28),cmap='gray_r')
    ax.set_title(f'{label}')
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()
            </code></pre>
            <h3>モデル定義</h3>
            <pre><code>
# 入力次元数
n_input = image.shape[0]

# 出力次元数
# 分類先クラス数　今回は10になる
n_output = 10

#   隠れ層のノード数
n_hidden = 128

# 結果確認
print(f'n_input: {n_input}  n_hidden: {n_hidden} n_output: {n_output}')
            </code></pre>
            <pre><code>
# モデルの定義
# 784入力10出力1隠れ層のニューラルネットワークモデル

class Net(nn.Module):
    def __init__(self, n_input, n_output, n_hidden):
        super().__init__()

        # 隠れ層の定義 (隠れ層のノード数: n_hidden)
        self.l1 = nn.Linear(n_input, n_hidden)

        # 出力層の定義
        self.l2 = nn.Linear(n_hidden, n_output)

        # ReLU関数の定義
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x1 = self.l1(x)
        x2 = self.relu(x1)
        x3 = self.l2(x2)
        return x3
            </code></pre>
            <p>２層目（出力層）の線形関数の出力に対して活性化関数が無いのは、この後の損失関数側にsoftmax関数も含める予定だからである。</p>
            <p>また、これまでと異なり、パラメータの初期値を全て1.0にする設定をなくしているのは、全て1.0にすると、モデルのパラメータ数が膨大になった影響でうまく学習できないからである。</p>
            <pre><code>
# 乱数の固定化
torch.manual_seed(123)
torch.cuda.manual_seed(123)

# モデルインスタンス生成
net = Net(n_input, n_output, n_hidden)

# モデルをGPU側に送る
net = net.to(device)
            </code></pre>
            <p>最適化アルゴリズムと損失関数を定義。</p>
            <pre><code>
# 学習率
lr = 0.01

# アルゴリズム: 勾配降下法
optimizer = torch.optim.SGD(net.parameters(), lr=lr)

# 損失関数： 交差エントロピー関数
criterion = nn.CrossEntropyLoss()
            </code></pre>
            <p>モデルの確認。</p>
            <pre><code>
# モデル内のパラメータの確認
# l1.weight, l1.bias, l2.weight, l2.biasがあることがわかる

for parameter in net.named_parameters():
    print(parameter)
            </code></pre>
            <pre><code>
# モデルの概要表示

print(net)
            </code></pre>
            <pre><code>
# モデルのサマリー表示

summary(net, (784,))
            </code></pre>
            <h3>勾配降下法</h3>
            <b>予測計算</b>
            <p>まず、訓練用のデータローダーであるtrain_setから１セット分のデータを取得する（本来のループ処理を１回分だけ行うためのダミーコード）。</p>
            <pre><code>
# 訓練データセット　最初の1項目を取得
# データローダーから最初の1セットを取得する
for images, labels in train_loader:
    break
            </code></pre>
            <p>データローダーから取得した学習データは当初、CPU側にあるので、GPU側に送付する。</p>
            <pre><code>
# データローダーから取得したデータをGPUに送る
inputs = images.to(device)
labels = labels.to(device)
            </code></pre>
            <p>net関数に入力変数inputsを渡して予測値を計算する。</p>
            <pre><code>
# 予測計算
outputs = net(inputs)

# 結果確認
print(outputs)
            </code></pre>
            <b>損失計算</b>
            <p>損失を計算すると同時に、損失を対象とした計算グラフを可視化する。</p>
            <pre><code>
#  損失計算
loss = criterion(outputs, labels)

# 損失値の取得
print(loss.item())

# 損失の計算グラフ可視化
g = make_dot(loss, params=dict(net.named_parameters()))
display(g)
            </code></pre>
            <p>線形関数をモデルに２つ取り入れた（隠れ層のあるモデルを作った）ため、biasとweightが２つずつ計算グラフに現れる。</p>
            <p>勾配計算とパラメータ修正は以下の通り。</p>
            <pre><code>
# 勾配計算の実行
loss.backward()
            </code></pre>
            <pre><code>
# 勾配計算の結果
w = net.to('cpu')
print(w.l1.weight.grad.numpy())
print(w.l1.bias.grad.numpy())
print(w.l2.weight.grad.numpy())
print(w.l2.bias.grad.numpy())
            </code></pre>
            <pre><code>
# 勾配降下法の適用
optimizer.step()
            </code></pre>
            <pre><code>
# パラメータ値の表示
print(net.l1.weight)
print(net.l1.bias)
            </code></pre>
            <b>繰り返し処理</b>
            <p>まず、初期化処理は以下の通り。</p>
            <pre><code>
# 乱数の固定化
torch.manual_seed(123)
torch.cuda.manual_seed(123)
torch.backends.cudnn.deterministic = True
torch.use_deterministic_algorithms = True

# 学習率
lr = 0.01

# モデルインスタンス生成
net = Net(n_input, n_output, n_hidden).to(device)

# 損失関数： 交差エントロピー関数
criterion = nn.CrossEntropyLoss()

# 最適化関数: 勾配降下法
optimizer = optim.SGD(net.parameters(), lr=lr)

# 繰り返し回数
num_epochs = 100

# 評価結果記録用
history = np.zeros((0,5))
            </code></pre>
            <p>続いて、ループ部分は以下の通り。</p>
            <pre><code>
# tqdmライブラリのインポート
from tqdm.notebook import tqdm

# 繰り返し計算メインループ

for epoch in range(num_epochs):
    # 1エポックあたりの正解数(精度計算用)
    n_train_acc, n_val_acc = 0, 0
    # 1エポックあたりの累積損失(平均化前)
    train_loss, val_loss = 0, 0
    # 1エポックあたりのデータ累積件数
    n_train, n_test = 0, 0

    # 訓練フェーズ
    for inputs, labels in tqdm(train_loader):
        # 1バッチあたりのデータ件数
        train_batch_size = len(labels)
        # 1エポックあたりのデータ累積件数
        n_train += train_batch_size

        # GPUヘ転送
        inputs = inputs.to(device)
        labels = labels.to(device)

        #勾配の初期化
        optimizer.zero_grad()

        # 予測計算
        outputs = net(inputs)

        # 損失計算
        loss = criterion(outputs, labels)

        # 勾配計算
        loss.backward()

        # パラメータ修正
        optimizer.step()

        # 予測ラベル導出
        predicted = torch.max(outputs, 1)[1]

        # 平均前の損失と正解数の計算
        # lossは平均計算が行われているので平均前の損失に戻して加算
        train_loss += loss.item() * train_batch_size
        n_train_acc += (predicted == labels).sum().item()

    #予測フェーズ
    for inputs_test, labels_test in test_loader:
        # 1バッチあたりのデータ件数
        test_batch_size = len(labels_test)
        # 1エポックあたりのデータ累積件数
        n_test += test_batch_size

        inputs_test = inputs_test.to(device)
        labels_test = labels_test.to(device)

        # 予測計算
        outputs_test = net(inputs_test)

        # 損失計算
        loss_test = criterion(outputs_test, labels_test)

        #予測ラベル導出
        predicted_test = torch.max(outputs_test, 1)[1]

        #  平均前の損失と正解数の計算
        # lossは平均計算が行われているので平均前の損失に戻して加算
        val_loss +=  loss_test.item() * test_batch_size
        n_val_acc +=  (predicted_test == labels_test).sum().item()

    # 精度計算
    train_acc = n_train_acc / n_train
    val_acc = n_val_acc / n_test
    # 損失計算
    ave_train_loss = train_loss / n_train
    ave_val_loss = val_loss / n_test
    # 結果表示
    print (f'Epoch [{epoch+1}/{num_epochs}], loss: {ave_train_loss:.5f} acc: {train_acc:.5f} val_loss: {ave_val_loss:.5f}, val_acc: {val_acc:.5f}')
    # 記録
    item = np.array([epoch+1 , ave_train_loss, train_acc, ave_val_loss, val_acc])
    history = np.vstack((history, item))
            </code></pre>
            <p>ミニバッチ処理でも繰り返し計算が入り、２重の繰り返しループになった。</p>
            <p>その影響で、データローダーから学習データを取り出すたびにデータをGPUに送る処理が必要になっている。</p>
            <p>また、訓練フェーズでミニバッチ処理用にループを回す際に、tqdm関数を経由するようにし、プログレスバーを表示させている。</p>
            <h3>結果確認</h3>
            <pre><code>
#損失と精度の確認

print(f'初期状態: 損失: {history[0,3]:.5f} 精度: {history[0,4]:.5f}' )
print(f'最終状態: 損失: {history[-1,3]:.5f} 精度: {history[-1,4]:.5f}' )
            </code></pre>
            <pre><code>
# 学習曲線の表示 (損失)

plt.rcParams['figure.figsize'] = (9,8)
plt.plot(history[:,0], history[:,1], 'b', label='訓練')
plt.plot(history[:,0], history[:,3], 'k', label='検証')
plt.xlabel('繰り返し回数')
plt.ylabel('損失')
plt.title('学習曲線(損失)')
plt.legend()
plt.show()
            </code></pre>
            <pre><code>
# 学習曲線の表示 (精度)

plt.rcParams['figure.figsize'] = (9,8)
plt.plot(history[:,0], history[:,2], 'b', label='訓練')
plt.plot(history[:,0], history[:,4], 'k', label='検証')
plt.xlabel('繰り返し回数')
plt.ylabel('精度')
plt.title('学習曲線(精度)')
plt.legend()
plt.show()
            </code></pre>
            <p>イメージ表示での確認も行う。</p>
            <pre><code>
# DataLoaderから最初の1セットを取得する
for images, labels in test_loader:
    break

# 予測結果の取得
inputs = images.to(device)
labels = labels.to(device)
outputs = net(inputs)
predicted = torch.max(outputs, 1)[1]
            </code></pre>
            <pre><code>
# 最初の50件でイメージを「正解値:予測値」と表示

plt.figure(figsize=(10, 8))
for i in range(50):
  ax = plt.subplot(5, 10, i + 1)

  # numpyに変換
  image = images[i]
  label = labels[i]
  pred = predicted[i]
  if (pred == label):
    c = 'k'
  else:
    c = 'b'

  # imgの範囲を[0, 1]に戻す
  image2 = (image + 1)/ 2

  # イメージ表示
  plt.imshow(image2.reshape(28, 28),cmap='gray_r')
  ax.set_title(f'{label}:{pred}', c=c)
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)
plt.show()
            </code></pre>
            <h3>隠れ層の２層化</h3>
            <b>モデルクラスの定義</b>
            <pre><code>
# モデルの定義
# 784入力10出力2隠れ層のニューラルネットワークモデル

class Net2(nn.Module):
    def __init__(self, n_input, n_output, n_hidden):
        super().__init__()

        # 隠れ層1の定義 (隠れ層のノード数: n_hidden)
        self.l1 = nn.Linear(n_input, n_hidden)

        # 隠れ層2の定義 (隠れ層のノード数: n_hidden)
        self.l2 = nn.Linear(n_hidden, n_hidden)

        # 出力層の定義
        self.l3 = nn.Linear(n_hidden, n_output)

        # ReLU関数の定義
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x1 = self.l1(x)
        x2 = self.relu(x1)
        x3 = self.l2(x2)
        x4 = self.relu(x3)
        x5 = self.l3(x4)
        return x5
            </code></pre>
            <pre><code>
# 乱数の固定化
torch.manual_seed(123)
torch.cuda.manual_seed(123)

# モデルインスタンス生成
net = Net2(n_input, n_output, n_hidden).to(device)

# 損失関数： 交差エントロピー関数
criterion = nn.CrossEntropyLoss()

# 最適化関数: 勾配降下法
optimizer = torch.optim.SGD(net.parameters(), lr=lr)
            </code></pre>
            <pre><code>
# モデルの概要表示

print(net)
            </code></pre>
            <pre><code>
# モデルのサマリー表示

summary(net, (784,))
            </code></pre>
            <pre><code>
# DataLoaderから最初の1セットを取得する
for images, labels in test_loader:
    break

# 予測結果の取得
inputs = images.to(device)
labels = labels.to(device)
            </code></pre>
            <pre><code>
# 予測計算
outputs = net(inputs)

#  損失計算
loss = criterion(outputs, labels)

# 損失の計算グラフ可視化
make_dot(loss, params=dict(net.named_parameters()))
            </code></pre>
            <pre><code>
# 勾配計算
loss.backward()

# 勾配計算結果の一部
w = net.to('cpu').l1.weight.grad.numpy()
print(w)

# 各要素の絶対値の平均
print(np.abs(w).mean())
            </code></pre>
            <pre><code>
# 乱数の固定化
torch.manual_seed(123)
torch.cuda.manual_seed(123)
torch.backends.cudnn.deterministic = True
torch.use_deterministic_algorithms = True

# モデルインスタンス生成
net = Net2(n_input, n_output, n_hidden).to(device)

# 損失関数： 交差エントロピー関数
criterion = nn.CrossEntropyLoss()

# 最適化関数: 勾配降下法
optimizer = optim.SGD(net.parameters(), lr=lr)

# 繰り返し回数
num_epochs = 200

# 評価結果記録用
history2 = np.zeros((0,5))
            </code></pre>
            <pre><code>
# tqdmライブラリのインポート
from tqdm.notebook import tqdm

# 繰り返し計算メインループ

for epoch in range(num_epochs):
    # 1エポックあたりの正解数(精度計算用)
    n_train_acc, n_val_acc = 0, 0
    # 1エポックあたりの累積損失(平均化前)
    train_loss, val_loss = 0, 0
    # 1エポックあたりのデータ累積件数
    n_train, n_test = 0, 0

    # 訓練フェーズ
    for inputs, labels in tqdm(train_loader):
        # 1バッチあたりのデータ件数
        train_batch_size = len(labels)
        # 1エポックあたりのデータ累積件数
        n_train += train_batch_size

        # GPUヘ転送
        inputs = inputs.to(device)
        labels = labels.to(device)

        #勾配の初期化
        optimizer.zero_grad()

        # 予測計算
        outputs = net(inputs)

        # 損失計算
        loss = criterion(outputs, labels)

        # 勾配計算
        loss.backward()

        # パラメータ修正
        optimizer.step()

        # 予測ラベル導出
        predicted = torch.max(outputs, 1)[1]

        # 平均前の損失と正解数の計算
        # lossは平均計算が行われているので平均前の損失に戻して加算
        train_loss += loss.item() * train_batch_size
        n_train_acc += (predicted == labels).sum().item()

    #予測フェーズ
    for inputs_test, labels_test in test_loader:
        # 1バッチあたりのデータ件数
        test_batch_size = len(labels_test)
        # 1エポックあたりのデータ累積件数
        n_test += test_batch_size

        inputs_test = inputs_test.to(device)
        labels_test = labels_test.to(device)

        # 予測計算
        outputs_test = net(inputs_test)

        # 損失計算
        loss_test = criterion(outputs_test, labels_test)

        #予測ラベル導出
        predicted_test = torch.max(outputs_test, 1)[1]

        #  平均前の損失と正解数の計算
        # lossは平均計算が行われているので平均前の損失に戻して加算
        val_loss +=  loss_test.item() * test_batch_size
        n_val_acc +=  (predicted_test == labels_test).sum().item()

    # 精度計算
    train_acc = n_train_acc / n_train
    val_acc = n_val_acc / n_test
    # 損失計算
    ave_train_loss = train_loss / n_train
    ave_val_loss = val_loss / n_test
    # 結果表示
    print (f'Epoch [{epoch+1}/{num_epochs}], loss: {ave_train_loss:.5f} acc: {train_acc:.5f} val_loss: {ave_val_loss:.5f}, val_acc: {val_acc:.5f}')
    # 記録
    item = np.array([epoch+1 , ave_train_loss, train_acc, ave_val_loss, val_acc])
    history2 = np.vstack((history2, item))
            </code></pre>
            <pre><code>
#損失と精度の確認

print(f'初期状態: 損失: {history2[0,3]:.5f} 精度: {history2[0,4]:.5f}' )
print(f'最終状態: 損失: {history2[-1,3]:.5f} 精度: {history2[-1,4]:.5f}' )
            </code></pre>
            <pre><code>
# 学習曲線の表示 (損失)

plt.plot(history2[:,0], history2[:,1], 'b', label='訓練')
plt.plot(history2[:,0], history2[:,3], 'k', label='検証')
plt.xlabel('繰り返し回数')
plt.ylabel('損失')
plt.title('学習曲線(損失)')
plt.legend()
plt.show()
            </code></pre>
            <pre><code>
# 学習曲線の表示 (精度)

plt.plot(history2[:,0], history2[:,2], 'b', label='訓練')
plt.plot(history2[:,0], history2[:,4], 'k', label='検証')
plt.xlabel('繰り返し回数')
plt.ylabel('精度')
plt.title('学習曲線(精度)')
plt.legend()
plt.show()
            </code></pre>
            <p>精度が改善したことから、隠れ層を２層にした効果はあったと考えられる。</p>
        </section>
        <section id="cnn">
            <h2>CNN</h2>
            <h3>問題の定義</h3>
            <p>"CIFAR-10データセット"と呼ぶデータセットを利用する。</p>
            <p>画素数32×32のカラーイメージデータが、airplane、auotomobile、birdなど10種類のカテゴリに分けられていて、イメージを予測する分類問題の学習データとしてよく用いられる。</p>
            <p>訓練用５万枚、検証用１万枚のデータが公開されている。</p>
            <p>MNISTとの最大の違いは、カラー画像である点である。そのため、元データも画像１枚当たり、(3, 32, 32)という３階テンソルで表現されている。</p>
            <p>CNNでは、「色、タテ、ヨコ」の３階テンソルの構造を保ったまま画像を処理する。</p>
            <p>２階層目以降では「色」に該当するインデックスは色ではなくなってしまうため、この奥行にあたるインデックスのことを一般的に「チャネル」と呼ぶ。</p>
            <p>CIFAR-10の問題としての難しさを確認するため、最初にあえて学習データを１階テンソル化し、「全結合型ニューラルネットワーク」でモデル構築を試みる。その後で、CNNにモデルを変えてみて、精度がどの程度向上するのか確かめることにする。</p>
            <p>CIFAR-10もMNISTと同様に、いくつかのライブラリで加工済みのデータを入手可能である。</p>
            <p>PyTorchのライブラリで学習データを入手する場合、イメージ一枚のデータ形式は、[3, 32, 32]になる。</p>
            <h3>CNNの処理概要</h3>
            <b>CNNの全体像</b>
            <p>CNNを特徴付けているのは畳み込み処理（Convolution）とプーリング処理（Pooling）である。</p>
            <b>畳み込み処理</b>
            <p>まず3×3や5×5など小さな正方形領域の配列を用意する。この配列のことを畳み込み処理では「カーネル」と呼ぶ。</p>
            <p>次に元の画像をカーネルと同じ大きさの正方形領域を切り取って、カーネルと対応する要素間で積を取り、その結果を加算した結果（積和）を出力領域の出力とする。</p>
            <p>切り取る領域を１つずつずらして、同じように積和を取り、隣の領域の出力とする。</p>
            <p>この処理の順番に繰り返すことにより、新しい正方形の出力パターンが出来上がる。</p>
            <p>実際には、入力チャネルは複数あり、積和演算は全チャネルにまたがって行われるため、「カーネル」もそれに対応して入力チャネル分ある。</p>
            <p>さらに、出力チャネルも複数あるため、カーネルは全体で４階テンソルの構造を持つことになる。</p>
            <p>この４階テンソルのカーネル配列がニューラルネットワークの「パラメータ」に該当し、このテンソルの値が学習対象になる。</p>
            <p>畳み込み処理は、カーネルの内容により、特定の傾きの直線が強調されるなど図形の特徴量の抽出に向いている。</p>
            <p>また、カーネルが場所を移動しながら学習するため、位置の移動に無関係な特徴量を検出できることになる。</p>
            <b>プーリング処理</b>
            <p>プーリング処理として最もよく利用されるのは、Max Poolingである。</p>
            <p>2×2などの小さな矩形領域で対象画像を区切り、その範囲での最大値を出力する。</p>
            <p>矩形領域をずらして再度最大値を取得し、隣の領域の値とする。</p>
            <p>これを順次繰り返して、新しい小さな矩形領域の値を求める。</p>
            <p>畳み込み処理は、１画素ずつずらして処理することが多いのに対して、プーリング処理は重なる領域を取らないようにずらすのが普通である。</p>
            <p>矩形領域のサイズは2×2が多いので、タテヨコともに元の画像の半分の画素数の新しい画像ができることになる。</p>
            <p>プーリング処理では、画像を縮小するのと同じ効果が期待できる。そのため、物体の大きさによらない普遍的な特徴量を抽出するのに向いている。</p>
            <h3>PyTorchでのCNN実装</h3>
            <b>CNNの全体構成</b>
            <p>「畳み込み処理」は、PyTorchのレイヤー関数でいうと、nn.Conv2dというレイヤー関数が担う。</p>
            <p>「プーリング処理」を担うのは、nn.MaxPool2dというレイヤー関数である。</p>
            <p>「畳み込み関数」は関数の内部にパラメータを持つ。これに対して「プーリング関数」は単なる演算なので、パラメータは持たない。</p>
            <p>レイヤー関数で初めて出てくるものとして、「１階化関数（flatten）」がある。これは、チャネル、タテ、ヨコと３階の広がりを持つ、「畳み込み関数」「プーリング関数」の出力を、ヨコ１列の１階テンソルにする操作である。</p>
            <p>分類モデルでは、最終的な出力が１階テンソルなので、このような操作が必要になる。</p>
            <p>通常のディープラーニングでは、精度を上げるため「畳み込み関数」「プーリング関数」のセットを何度も繰り返すことが多い。</p>
            <b>nn.Conv2dとnn.MaxPool2d</b>
            <p>CNNモデルの定義のうち、「畳み込み関数」と「プーリング関数」に関わる変数定義を抜き出したものを以下に示す。</p>
            <pre><code>
# CNN前半部分 レイヤー関数の定義

conv1 = nn.Conv2d(3, 32, 3)
relu = nn.ReLU(inplace=True)
conv2 = nn.Conv2d(32, 32, 3)
maxpool = nn.MaxPool2d((2,2))
            </code></pre>
            <p>畳み込み関数は、nn.Conv2dというレイヤー関数で実現されている。</p>
            <p>第１引数は入力チャネル数、第２引数は出力チャネル数、そして第３チャネルはフィルターとなる小さな正方形（カーネル）の１辺の画素数を意味するカーネルサイズを指定している。</p>
            <p>プーリング関数はnn.MaxPool2dというレイヤー関数で実現する。引数の(2, 2)は、それぞれ小さい矩形のタテとヨコの画素数を示している、</p>
            <p>畳み込み関数であるconv1とconv2は、内部にパラメータを持つ。weightとbiasのshapeを確認する。</p>
            <pre><code>
# conv1の確認
print(conv1)

# conv1の内部変数のshape確認
print(conv1.weight.shape)
print(conv1.bias.shape)

# conv2の内部変数のshape確認
print(conv2.weight.shape)
print(conv2.bias.shape)
            </code></pre>
            <p>conv1とconv2のweightは４つのインデックスを持つ４階テンソルになっている。</p>
            <p>weightのshapeの最初の要素である「32」は出力チャネル数に対応している。</p>
            <p>２つ目の要素の「3」は入力チャネル数である。</p>
            <p>例えば、出力チャネル１用のカーネルは「入力チャネル０用」から「入力チャネル２用」まで３枚ある。</p>
            <p>実際の畳み込み計算をする際には、積和演算を全部で入力チャネル数回行う。</p>
            <p>その結果をすべて加算したものが、出力チャネル１のパターンになっている。</p>
            <p>今は特定のチャネルである「出力チャネル１」に注目したが、全体では「出力チャネル０用」から「出力チャネル３１用」まで全部で３２セット、このようなパラメータが存在する。</p>
            <b>畳み込み処理とプーリング層のシミュレーション</b>
            <p>入力として使うダミーデータを生成する。</p>
            <pre><code>
# ダミーで入力と同じサイズのTensorを生成
inputs = torch.randn(100, 3, 32, 32)
print(inputs.shape)
            </code></pre>
            <pre><code>
# CNNの前半部分をシミュレーションする

x1 = conv1(inputs)
x2 = relu(x1)
x3 = conv2(x2)
x4 = relu(x3)
x5 = maxpool(x4)
            </code></pre>
            <pre><code>
# それぞれのshapeの確認

print(inputs.shape)
print(x1.shape)
print(x2.shape)
print(x3.shape)
print(x4.shape)
print(x5.shape)
            </code></pre>
            <p>結果の読み方は、最初の数字が学習データの件数、２番目の数字がチャネル数、３番目と４番目が、タテとヨコの画素数である。</p>
            <b>nn.Sequential</b>
            <p>PyTorchの中で「コンテナ（入れ物）」と呼ばれているクラスの１つ。</p>
            <pre><code>
# 関数定義
features = nn.Sequential(
    conv1,
    relu,
    conv2,
    relu,
    maxpool
)
            </code></pre>
            <b>nn.Flatten</b>
            <p>この部品の目的は、「畳み込み処理」「プーリング処理」の最中は３階テンソルの形で扱われていたデータを、線形関数（nn.Linear）で扱えるよう、１階テンソルの形に変換することである。</p>
            <pre><code>
# 関数定義
flatten = nn.Flatten()

# 動作テスト
outputs2 = flatten(outputs)

# 結果確認
print(outputs.shape)
print(outputs2.shape)
            </code></pre>
            <h3>共通関数の利用</h3>
            <b>eval_loss（損失計算）</b>
            <pre><code>
# 損失計算用
def eval_loss(loader, device, net, criterion):

    # データローダーから最初の1セットを取得する
    for images, labels in loader:
        break

    # デバイスの割り当て
    inputs = images.to(device)
    labels = labels.to(device)

    # 予測計算
    outputs = net(inputs)

    #  損失計算
    loss = criterion(outputs, labels)

    return loss
            </code></pre>
            <p>最初にデータローダーを使って、入力データと正解データを取得し、入力データとモデルインスタンスから予測値を計算、予測値と正解データを使って損失を計算するのが、処理の流れである。</p>
            <b>fit（学習）</b>
            <p>呼び出し時の引数は次の８つである。</p>
            <ul>
                <li>net : 学習対象のモデルインスタンス</li>
                <li>optimizer : 最適化関数のインスタンス</li>
                <li>criterion : 損失関数のインスタンス</li>
                <li>num_epochs : 繰り返し数</li>
                <li>train_loader : 訓練用のデータローダー</li>
                <li>test_loader : 検証用のデータローダー</li>
                <li>device : GPU or CPU</li>
                <li>history : これまでのhistory</li>
            </ul>
            <p>戻り値はhistoryで、（繰り返し回数、訓練損失、訓練精度、検証損失、損失精度）の２次元配列をNumPy形式で返す。</p>
            <p>このfit関数では、引数historyに今までのhistoryを渡すことで、「追加学習」の扱いで過去分も追加したhistoryを生成してくれる。</p>
            <pre><code>
# 学習用関数
def fit(net, optimizer, criterion, num_epochs, train_loader, test_loader, device, history):

    # tqdmライブラリのインポート
    from tqdm.notebook import tqdm

    base_epochs = len(history)

    for epoch in range(base_epochs, num_epochs+base_epochs):
        # 1エポックあたりの正解数(精度計算用)
        n_train_acc, n_val_acc = 0, 0
        # 1エポックあたりの累積損失(平均化前)
        train_loss, val_loss = 0, 0
        # 1エポックあたりのデータ累積件数
        n_train, n_test = 0, 0

        #訓練フェーズ
        net.train()

        for inputs, labels in tqdm(train_loader):
            # 1バッチあたりのデータ件数
            train_batch_size = len(labels)
            # 1エポックあたりのデータ累積件数
            n_train += train_batch_size

            # GPUヘ転送
            inputs = inputs.to(device)
            labels = labels.to(device)

            # 勾配の初期化
            optimizer.zero_grad()

            # 予測計算
            outputs = net(inputs)

            # 損失計算
            loss = criterion(outputs, labels)

            # 勾配計算
            loss.backward()

            # パラメータ修正
            optimizer.step()

            # 予測ラベル導出
            predicted = torch.max(outputs, 1)[1]

            # 平均前の損失と正解数の計算
            # lossは平均計算が行われているので平均前の損失に戻して加算
            train_loss += loss.item() * train_batch_size
            n_train_acc += (predicted == labels).sum().item()

        #予測フェーズ
        net.eval()

        for inputs_test, labels_test in test_loader:
            # 1バッチあたりのデータ件数
            test_batch_size = len(labels_test)
            # 1エポックあたりのデータ累積件数
            n_test += test_batch_size

            # GPUヘ転送
            inputs_test = inputs_test.to(device)
            labels_test = labels_test.to(device)

            # 予測計算
            outputs_test = net(inputs_test)

            # 損失計算
            loss_test = criterion(outputs_test, labels_test)

            # 予測ラベル導出
            predicted_test = torch.max(outputs_test, 1)[1]

            #  平均前の損失と正解数の計算
            # lossは平均計算が行われているので平均前の損失に戻して加算
            val_loss +=  loss_test.item() * test_batch_size
            n_val_acc +=  (predicted_test == labels_test).sum().item()

        # 精度計算
        train_acc = n_train_acc / n_train
        val_acc = n_val_acc / n_test
        # 損失計算
        avg_train_loss = train_loss / n_train
        avg_val_loss = val_loss / n_test
        # 結果表示
        print (f'Epoch [{(epoch+1)}/{num_epochs+base_epochs}], loss: {avg_train_loss:.5f} acc: {train_acc:.5f} val_loss: {avg_val_loss:.5f}, val_acc: {val_acc:.5f}')
        # 記録
        item = np.array([epoch+1, avg_train_loss, train_acc, avg_val_loss, val_acc])
        history = np.vstack((history, item))
    return history
            </code></pre>
            <p>netというモデルインスタンスに対して、訓練フェーズ、予測フェーズそれぞれの先頭でnet.train(), net.eval()という関数を呼び出している。この２つの関数はモデルクラスを定義する際に利用している親クラスnn.Moduleで定義されている。</p>
            <p>ドロップアウト関数（nn.Dropout）やBN関数（nn.BatchNorm2d）というレイヤー関数では、それぞれの関数に対して「今は訓練フェーズ」「今は予測フェーズ」という違いを教えてあげる必要がある。</p>
            <b>evaluate_history（学習ログ）</b>
            <p>学習結果の評価に関しては、</p>
            <ul>
                <li>historyの先頭と最後をprint関数で表示して学習結果の概要を表示</li>
                <li>学習曲線を損失、精度の２つで表示</li>
            </ul>
            <p>というのが、今までのパターンだったが、この典型的な評価パターンをまとめて行う関数が、evaluate_historyの役割である。</p>
            <p>引数はhistoryだけでよい。</p>
            <pre><copde>
# 学習ログ解析

def evaluate_history(history):
    #損失と精度の確認
    print(f'初期状態: 損失: {history[0,3]:.5f} 精度: {history[0,4]:.5f}')
    print(f'最終状態: 損失: {history[-1,3]:.5f} 精度: {history[-1,4]:.5f}' )

    num_epochs = len(history)
    unit = num_epochs / 10

    # 学習曲線の表示 (損失)
    plt.figure(figsize=(9,8))
    plt.plot(history[:,0], history[:,1], 'b', label='訓練')
    plt.plot(history[:,0], history[:,3], 'k', label='検証')
    plt.xticks(np.arange(0,num_epochs+1, unit))
    plt.xlabel('繰り返し回数')
    plt.ylabel('損失')
    plt.title('学習曲線(損失)')
    plt.legend()
    plt.show()

    # 学習曲線の表示 (精度)
    plt.figure(figsize=(9,8))
    plt.plot(history[:,0], history[:,2], 'b', label='訓練')
    plt.plot(history[:,0], history[:,4], 'k', label='検証')
    plt.xticks(np.arange(0,num_epochs+1,unit))
    plt.xlabel('繰り返し回数')
    plt.ylabel('精度')
    plt.title('学習曲線(精度)')
    plt.legend()
    plt.show()
            </copde></pre>
            <b>show_images_labels（予測結果表示）</b>
            <p>事前学習済みモデルで正しい予測ができているかを、元データのイメージ表示とともに行う。また、モデルを作る前に、イメージとラベルだけを表示することもできる。</p>
            <p>対象はデータローダーで取得される先頭50件のデータになる。</p>
            <p>引数は次の４つである。</p>
            <ul>
                <li>loader : 検証用データローダー</li>
                <li>classes : 正解データに対応するラベル値のリスト</li>
                <li>net : 予測対象の事前学習済みモデルインスタンス。Noneを渡すと正解データのみ表示でき、学習前にデータの様子を確認したいときに使う。</li>
                <li>device : 予測計算に利用するデバイス。予測結果も表示するときにのみ利用する。</li>
            </ul>
            <pre><code>
# イメージとラベル表示
def show_images_labels(loader, classes, net, device):

    # データローダーから最初の1セットを取得する
    for images, labels in loader:
        break
    # 表示数は50個とバッチサイズのうち小さい方
    n_size = min(len(images), 50)

    if net is not None:
      # デバイスの割り当て
      inputs = images.to(device)
      labels = labels.to(device)

      # 予測計算
      outputs = net(inputs)
      predicted = torch.max(outputs,1)[1]
      #images = images.to('cpu')

    # 最初のn_size個の表示
    plt.figure(figsize=(20, 15))
    for i in range(n_size):
        ax = plt.subplot(5, 10, i + 1)
        label_name = classes[labels[i]]
        # netがNoneでない場合は、予測結果もタイトルに表示する
        if net is not None:
          predicted_name = classes[predicted[i]]
          # 正解かどうかで色分けをする
          if label_name == predicted_name:
            c = 'k'
          else:
            c = 'b'
          ax.set_title(label_name + ':' + predicted_name, c=c, fontsize=20)
        # netがNoneの場合は、正解ラベルのみ表示
        else:
          ax.set_title(label_name, fontsize=20)
        # TensorをNumPyに変換
        image_np = images[i].numpy().copy()
        # 軸の順番変更 (channel, row, column) -> (row, column, channel)
        img = np.transpose(image_np, (1, 2, 0))
        # 値の範囲を[-1, 1] -> [0, 1]に戻す
        img = (img + 1)/2
        # 結果表示
        plt.imshow(img)
        ax.set_axis_off()
    plt.show()
            </code></pre>
            <b>torch_seed（乱数初期化）</b>
            <pre><code>
# PyTorch乱数固定用

def torch_seed(seed=123):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.use_deterministic_algorithms = True
            </code></pre>
            <p>PyTorchでは、GPUを使って計算した場合、乱数の種を指定しただけでは結果が同じにならないことがある。その事象を避けるためにプロパティを設定する最後の２行を追加している。</p>
            <h3>データ準備</h3>
            <b>Transforms定義</b>
            <p>データ前処理に関しては、その後で２通りのモデルを作ることを想定して１階テンソル版、３階テンソル版の２バージョンを用意する。</p>
            <pre><code>
# Transformsの定義

# transformer1 1階テンソル化

transform1 = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(0.5, 0.5),
    transforms.Lambda(lambda x: x.view(-1)),
])

# transformer2 正規化のみ実施

# 検証データ用 : 正規化のみ実施
transform2 = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(0.5, 0.5),
])
            </code></pre>
            <p>transform1が全結合型ニューラルネットワーク用、transform2がCNN用になる。</p>
            <b>データセット定義</b>
            <pre><code>
# データ取得用関数 Datasets

data_root = './data'

# 訓練データセット 1階テンソル版
train_set1 = datasets.CIFAR10(
    root = data_root, train = True,
    download = True, transform = transform1)

# 検証データセット 1階テンソル版
test_set1 = datasets.CIFAR10(
    root = data_root, train = False,
    download = True, transform = transform1)

# 訓練データセット 3階テンソル版
train_set2 = datasets.CIFAR10(
    root =  data_root, train = True,
    download = True, transform = transform2)

# 検証データセット 3階テンソル版
test_set2 = datasets.CIFAR10(
    root = data_root, train = False,
    download = True, transform = transform2)
            </code></pre>
            <p>データセットのshapeを確認する。</p>
            <pre><code>
image1, label1 = train_set1[0]
image2, label2 = train_set2[0]

print(image1.shape)
print(image2.shape)
            </code></pre>
            <b>データローダー定義</b>
            <pre><code>
# データローダーの定義

# ミニバッチのサイズ指定
batch_size = 100

# 訓練用データローダー
# 訓練用なので、シャッフルをかける
train_loader1 = DataLoader(train_set1, batch_size=batch_size, shuffle=True)

# 検証用データローダー
# 検証時にシャッフルは不要
test_loader1 = DataLoader(test_set1,  batch_size=batch_size, shuffle=False)

# 訓練用データローダー
# 訓練用なので、シャッフルをかける
train_loader2 = DataLoader(train_set2, batch_size=batch_size, shuffle=True)

# 検証用データローダー
# 検証時にシャッフルは不要
test_loader2 = DataLoader(test_set2,  batch_size=batch_size, shuffle=False)
            </code></pre>
            <p>意図したデータができているか、確認する。</p>
            <pre><code>
# train_loader1から1セット取得
for images1, labels1 in train_loader1:
    break

# train_loader2から1セット取得
for images2, labels2 in train_loader2:
    break

# それぞれのshape確認
print(images1.shape)
print(images2.shape)
            </code></pre>
            <b>検証データのイメージ表示</b>
            <p>検証データをtest_loader2から読み取り、先頭50個のイメージを表示してみる。</p>
            <pre><code>
# 正解ラベル定義
classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

# 検証データ最初の50個の表示
show_images_labels(test_loader2, classes, None, None)
            </code></pre>
            <h3>モデル定義（全結合版）</h3>
            <b>学習用パラメータ設定</b>
            <pre><code>
# 入力次元数 今回は3*32*32=3072
n_input = image1.view(-1).shape[0]

# 出力次元数
# 分類先クラス数　今回は10になる
n_output = len(classes)

# 隠れ層のノード数
n_hidden = 128

# 結果確認
print(f'n_input: {n_input}  n_hidden: {n_hidden} n_output: {n_output}')
            </code></pre>
            <pre><code>
# モデルの定義
# 3072入力10出力1隠れ層のニューラルネットワークモデル

class Net(nn.Module):
    def __init__(self, n_input, n_output, n_hidden):
        super().__init__()

        # 隠れ層の定義 (隠れ層のノード数: n_hidden)
        self.l1 = nn.Linear(n_input, n_hidden)

        # 出力層の定義
        self.l2 = nn.Linear(n_hidden, n_output)

        # ReLU関数の定義
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x1 = self.l1(x)
        x2 = self.relu(x1)
        x3 = self.l2(x2)
        return x3
            </code></pre>
            <b>モデルインスタンス生成とGPUの割り当て</b>
            <pre><code>
# モデルインスタンス生成
net = Net(n_input, n_output, n_hidden).to(device)

# 損失関数： 交差エントロピー関数
criterion = nn.CrossEntropyLoss()

# 学習率
lr = 0.01

# 最適化関数: 勾配降下法
optimizer = torch.optim.SGD(net.parameters(), lr=lr)
            </code></pre>
            <pre><code>
# モデルの概要表示

print(net)
            </code></pre>
            <pre><code>
# モデルのサマリー表示

summary(net, (100,3072),depth=1)
            </code></pre>
            <pre><code>
# 損失計算
loss = eval_loss(test_loader1, device, net, criterion)

# 損失の計算グラフ可視化
g = make_dot(loss, params=dict(net.named_parameters()))
display(g)
            </code></pre>
            <h3>結果（全結合版）</h3>
            <b>学習</b>
            <pre><code>
# 乱数初期化
torch_seed()

# モデルインスタンス生成
net = Net(n_input, n_output, n_hidden).to(device)

# 損失関数： 交差エントロピー関数
criterion = nn.CrossEntropyLoss()

# 学習率
lr = 0.01

# 最適化関数: 勾配降下法
optimizer = optim.SGD(net.parameters(), lr=lr)

# 繰り返し回数
num_epochs = 50

# 評価結果記録用
history = np.zeros((0,5))

# 学習
history = fit(net, optimizer, criterion, num_epochs, train_loader1, test_loader1, device, history)
            </code></pre>
            <b>評価</b>
            <pre><code>
# 評価

evaluate_history(history)
            </code></pre>
            <h3>モデル定義（CNN版）</h3>
            <pre><code>
class CNN(nn.Module):
  def __init__(self, n_output, n_hidden):
    super().__init__()
    self.conv1 = nn.Conv2d(3, 32, 3)
    self.conv2 = nn.Conv2d(32, 32, 3)
    self.relu = nn.ReLU(inplace=True)
    self.maxpool = nn.MaxPool2d((2,2))
    self.flatten = nn.Flatten()
    self.l1 = nn.Linear(6272, n_hidden)
    self.l2 = nn.Linear(n_hidden, n_output)

    self.features = nn.Sequential(
        self.conv1,
        self.relu,
        self.conv2,
        self.relu,
        self.maxpool)

    self.classifier = nn.Sequential(
       self.l1,
       self.relu,
       self.l2)

  def forward(self, x):
    x1 = self.features(x)
    x2 = self.flatten(x1)
    x3 = self.classifier(x2)
    return x3
            </code></pre>
            <b>モデルインスタンスの生成</b>
            <pre><code>
# モデルインスタンス生成
net = CNN(n_output, n_hidden).to(device)

# 損失関数： 交差エントロピー関数
criterion = nn.CrossEntropyLoss()

# 学習率
lr = 0.01

# 最適化関数: 勾配降下法
optimizer = torch.optim.SGD(net.parameters(), lr=lr)
            </code></pre>
            <b>モデルの概要表示</b>
            <pre><code>
# モデルの概要表示

print(net)
            </code></pre>
            <pre><code>
# モデルのサマリー表示

summary(net,(100,3,32,32),depth=1)
            </code></pre>
            <pre><code>
# 損失計算
loss = eval_loss(test_loader2, device, net, criterion)

# 損失の計算グラフ可視化
g = make_dot(loss, params=dict(net.named_parameters()))
display(g)
            </code></pre>
            <h3>結果（CNN版）</h3>
            <b>モデル初期化と学習</b>
            <pre><code>
# 乱数初期化
torch_seed()

# モデルインスタンス生成
net = CNN(n_output, n_hidden).to(device)

# 損失関数： 交差エントロピー関数
criterion = nn.CrossEntropyLoss()

# 学習率
lr = 0.01

# 最適化関数: 勾配降下法
optimizer = optim.SGD(net.parameters(), lr=lr)

# 繰り返し回数
num_epochs = 50

# 評価結果記録用
history2 = np.zeros((0,5))

# 学習
history2 = fit(net, optimizer, criterion, num_epochs, train_loader2, test_loader2, device, history2)
            </code></pre>
            <p>モデルインスタンス生成時のパラメータからn_inputがなくなっている。</p>
            <p>これは、全結合版では、先頭の線形関数の重み行列は入力データの件数に合わせてあらかじめ用意しておく必要があり、それがパラメータになっていたが、CNNでは固定長のカーネル行列を用意しておくだけなので、どんな大きさの入力でも受け取れるからである。</p>
            <b>評価</b>
            <pre><code>
# 評価

evaluate_history(history2)
            </code></pre>
            <pre><code>
# 最初の50個の表示

show_images_labels(test_loader2, classes, net, device)
            </code></pre>
        </section>
        <section id="tuning">
            <h2>Tuning</h2>
             <pre><code>

            </code></pre>
            <pre><code>

            </code></pre>
            <pre><code>

            </code></pre>
            <pre><code>

            </code></pre>
            <pre><code>

            </code></pre>
        </section>
        <section id="pretrain">
            <h2>Pretrained Model</h2>
             <pre><code>

            </code></pre>
            <pre><code>

            </code></pre>
            <pre><code>

            </code></pre>
            <pre><code>

            </code></pre>
            <pre><code>

            </code></pre>
        </section>
        <section id="custom">
            <h2>Custom Data</h2>
             <pre><code>

            </code></pre>
            <pre><code>

            </code></pre>
            <pre><code>

            </code></pre>
            <pre><code>

            </code></pre>
            <pre><code>

            </code></pre>
        </section>
    </main>
</body>
</html>
