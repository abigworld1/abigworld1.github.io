<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【論文紹介 | Dicision Transformer: Reinforcement Learning via Sequence Modeling】走る作曲家のAIカフェ</title>
    <link rel="stylesheet" href="style.css"> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1050827580219099"
     crossorigin="anonymous"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
            <h2>目次</h2>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#abstract">Abstract</a></li>
            </ul>
        </section>
        <section id="overview">
            <h2>Overview</h2>
            "<a href="https://arxiv.org/pdf/2106.01345">Dicision Transformer: Reinforcement Learning via Sequence Modeling</a>"は、強化学習（Reinforcement Learning, RL）タスクを解決するために、Transformerモデルを使用するアプローチです。このモデルは、従来の強化学習アルゴリズムと異なり、シーケンスモデリングとして強化学習問題を捉えます。主な特徴として、教師あり学習のように過去の経験データから直接学習し、決定（decision-making）を行います。<br>
            このページでは、その内容を順に追っていきます。
        </section>
        <section id="abstract">
          <h2>Abstract</h2>
            <p>私たちは、強化学習（Reinforcement Learning, RL）をシーケンスモデリング問題として抽象化するフレームワークを紹介します。
                これにより、トランスフォーマーアーキテクチャのシンプルさとスケーラビリティ、そしてGPT-xやBERTといった言語モデリングにおける進展を活用することが可能になります。
                特に、私たちはDecision Transformerというアーキテクチャを提案し、RLの問題を条件付きシーケンスモデリングとして捉えています。
                従来のRLアプローチが価値関数の適合や方策勾配の計算を行うのとは異なり、Decision Transformerは因果関係のあるマスク付きトランスフォーマーを利用して、最適な行動を単に出力します。
                自己回帰モデルを目標とするリターン（報酬）、過去の状態、行動に条件付けることで、Decision Transformerモデルは目標のリターンを達成する将来の行動を生成することができます。
                シンプルでありながら、Decision TransformerはAtari、OpenAI Gym、Key-to-Doorタスクにおいて、最先端のモデルフリーオフラインRLベースラインと同等かそれ以上のパフォーマンスを発揮します。</p>
            <img src="images/decision_transformer1.png" alt="decision_transformer1" class="full-screen-image">
            <p>図1: Decision Transformerのアーキテクチャ。
                状態、行動、およびリターンは、それぞれのモダリティに特化した線形埋め込みに入力され、エピソードのタイムステップを示す位置エンコーディングが追加されます。
                トークンは、因果自己注意マスクを使用して自己回帰的に行動を予測するGPTアーキテクチャに入力されます。</p>
        </section>

