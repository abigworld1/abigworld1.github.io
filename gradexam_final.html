<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【院試対策 | 北海道大学 | 情報科学院 | 情報理工 | 最終チェック】走る作曲家のAIカフェ</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
          <h2>最終チェック</h2>
          <p>「北海道大学大学院情報科学院修士課程入学試験」（令和6年8月実施）の情報理工学コース対策ページです。</p>
            <p>10年分の過去問（2014~2023）を解いた私が、過去に複数回出題されていた問題や狙われそうなポイントをまとめます。</p>
        </section>
        <section>
          <h2>分野別対策</h2>
          <ul>
            <li><a href="gradexam.html">院試対策 ～はじめに～</a></li>
            <li><a href="gradexam_basicmath.html">院試対策 ～基礎数学編～</a></li>
            <li><a href="gradexam_infomath.html">院試対策 ～情報数学編～</a></li>
            <li><a href="gradexam_statistics.html">院試対策 ～確率・統計編～</a></li>
            <li><a href="gradexam_infotheory.html">院試対策 ～情報理論編～</a></li>
            <li><a href="gradexam_algo.html">院試対策 ～アルゴリズムとデータ構造編～</a></li>
            <li><a href="gradexam_ai.html">院試対策 ～人工知能編～</a></li>
            <li><a href="gradexam_cs.html">院試対策 ～コンピュータシステム編～</a></li>
            <li><a href="gradexam_final.html">院試対策 ～最終チェック～</a></li>
          </ul>
        </section>
        <section>
            <h2>基礎数学</h2>
            <h3>停留点</h3>
            <p>停留点を求める問題は、ここ10年で複数回出題されている。</p>
            <p>停留点は関数の変化がなくなる点であるため、\(x\)方向、\(y\)方向ともにその変化量が0、すなわち</p>
            <div class="scroll">
                \begin{align}
                \frac{\partial f(x,y)}{\partial x}=\frac{\partial f(x,y)}{\partial y}=0
                \end{align}
            </div>
            <p>となる点を求めればよい。</p>
            <p>つまり、\(x\)と\(y\)の偏微分を求めて、それらをイコール0にした2式を連立して解けばよい。</p>
            <p>このように停留点は簡単に求まるが、次に、その停留点が極大値なのか、極小値なのか、鞍点なのかを判定せよ、と問われることが多い。</p>
            <p>停留点を判定するには、以下で定義されるヘッセ行列を使う。</p>
            <div class="scroll">
                \begin{align}
                H=
                    \begin{pmatrix}\displaystyle{\frac{\partial^{2} f(x,y)}{\partial x^{2}}}&\displaystyle{\frac{\partial^{2} f(x,y)}{\partial x\partial y}} \\ \\ 
                    \displaystyle{\frac{\partial^{2} f(x,y)}{\partial x\partial y}}&\displaystyle{\frac{\partial^{2} f(x,y)}{\partial y^{2}}}\end{pmatrix}
                \end{align}
            </div>
            <p>したがって、二階偏微分を求める必要がある。</p>
            <p>二階偏微分を求めたら、ヘッセ行列の固有値\(\lambda_{1},\lambda_{2}\)を求めることによって、以下のように判別できる。</p>
            <div class="scroll">
                \begin{align}
                \begin{cases}
                    \lambda_{1}>0,\lambda_{2}>0&\to\,\text{極小点}\\ 
                    \lambda_{1}<0,\lambda_{2}<0&\to\,\text{極大点}\\ 
                    \lambda_{1}\lambda_{2}<0&\to\,\text{鞍点} 
                \end{cases}
                \end{align}
            </div>
            <h3>極座標変換</h3>
            <p>極座標変換ができないと解くのが難しい問題はここ10年で何度も出題されている。</p>
            <p>まずは2次元の極座標変換を説明する。</p>
            <p>2次元の極座標変換では、\(x\)と\(y\)を次のようにおく。</p>
            <div class="scroll">
                \begin{align}
                x = r \cos \theta , \ \ \ y = r \sin \theta 
                \end{align}
            </div>
            <p>このとき、ヤコビアンを計算すると以下のようになる。</p>
            <div class="scroll">
                \begin{align}
                J = & \left| \begin{array}{ccc} \frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\ \frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta} \end{array} \right|
                    \\ = & \left| \begin{array}{ccc} \cos \theta & - r \sin \theta \\ \sin \theta & r \cos \theta \end{array} \right|
                    \\ = & \ r \left( \cos^2 \theta + \sin^2 \theta \right)
                    \\ = & \ r
                \end{align}
            </div>
            <p>したがって、\(dxdy = r \ dr d \theta\)となる。</p>
            <p>次に、3次元の極座標変換を説明する。</p>
            <p>3次元の極座標変換では、\(x\)と\(y\)を次のようにおく。</p>
            <div class="scroll">
                \begin{align}
                x = r \cos \theta \cos \varphi, \quad y = r \cos \theta \sin \varphi, \quad z = r \sin \theta
                \end{align}
            </div>
            <p>このとき、ヤコビアン行列\(J\)は以下のようになる。</p>
            <div class="scroll">
                \begin{align}
                J = \frac{\partial(x, y, z)}{\partial(r, \varphi, \theta)} = 
                \begin{pmatrix}
                \cos \theta \cos \varphi & -r \cos \theta \sin \varphi & -r \sin \theta \cos \varphi \\
                \cos \theta \sin \varphi & r \cos \theta \cos \varphi & -r \sin \theta \sin \varphi \\
                \sin \theta & 0 & r \cos \theta
                \end{pmatrix}
                \end{align}
            </div>
            <p>したがって、ヤコビアン行列の行列式 \( \det(J) \) は次のように計算される：</p>
            <div class="scroll">
                \begin{align}
                \det(J) = r^2 \cos \theta
                \end{align}
            </div>
            <p>以上のことは、\(\cos \theta\)と\(\sin \theta\)を入れ替えても成り立つ。</p>
        </section>
        <section>
            <h2>情報数学</h2>
            <h3>言語</h3>
            <p>言語の中でも、特に正規言語と文脈自由言語について問われることが多い。それぞれの代表例や特徴は必ず覚えておいた方が良い。</p>
            <p>まずは、正規言語を説明する。</p>
            <p>正規言語は、有限オートマトンによって認識される言語であり、正規文法によって生成される。</p>
            <p>正規文法の例を以下に示す。</p>
            <ul>
                <li>\( S \rightarrow aS \)</li>
                <li>\( S \rightarrow bS \)</li>
                <li>\( S \rightarrow \epsilon \) （\(\epsilon\)は空文字を表す）</li>
            </ul>
            <p>この文法は、\(\{a, b\}\)からなる任意の文字列（例えば、\(\epsilon\), \(a\), \(b\), \(aa\), \(ab\), \(ba\), \(bb\) など）を生成する。</p>
            <p>他にも、正規言語の例として、以下があげられる。</p>
            <ul>
                <li>\(L = { a^n \mid n ≥ 0 }\)(空文字列または任意の個数の \(a\) からなる言語。)</li>
                <li>\(L = (ab)^*\)(\(ab\) の繰り返しからなる言語。)</li>
            </ul>
            <p>次に、文脈自由言語を説明する。</p>
            <p>文脈自由言語は、プッシュダウンオートマトンによって認識される言語であり、文脈自由文法によって生成される。</p>
            <p>文脈自由文法は、生成規則が左辺に1つの非終端記号を持ち、右辺が任意の文字列であるような形をしている文法である。</p>
            <p>文脈自由文法の例を以下に示す。</p>
            <ul>
                <li>\( S \rightarrow aSb \)</li>
                <li>\( S \rightarrow \epsilon \)</li>
            </ul>
            <p>この文法は、同数の \(a\) と \(b\) を持つ文字列（例えば、\(\epsilon\), \(ab\), \(aabb\), \(aaabbb\) など）を生成する。</p>
            <p>この言語は、正規文法では表現できないが、文脈自由文法によって生成可能である。</p>
            <p>他にも、文脈自由文法の例として、以下があげられる。</p>
            <ul>
                <li>\(L = { a^n b^n | n ≥ 0 }\)(同じ数の \(a\) と \(b\) からなる言語。)</li>
                <li>\(ww^R | w ∈ {a, b}^*\)(任意の文字列 \(w\) とその逆順 \(w^R\) からなる言語。)</li>
            </ul>
            <p>ついでに、文脈依存文法も説明する。</p>
            <p>文脈依存言語は、線形有界オートマトンによって認識される言語であり、文脈依存文法によって生成される言語である。</p>
            <p>文脈依存文法の生成規則は、形式 \(α A β → α γ β\) を持ち、ここで \(A\) は非終端記号、\(α\)、\(β\)、および \(γ\) は任意の文字列である。</p>
            <p>文脈依存言語の例を以下に示す。</p>
            <ul>
                <li>\(L = { a^n b^n c^n | n ≥ 1 }\)(同じ数の \(a, b, c\) からなる言語。)</li>
            </ul>
        </section>
        <section>
            <h2>確率・統計</h2>
            <h3>一致性</h3>
            <p>一致性とは、ある母数\(\theta\)の推定量\(\hat{\theta}\)が\(\hat{\theta} \xrightarrow{P} \theta\)を満たすことをいい、\(\hat{\theta}\)を一致推定量と呼ぶ。</p>
            <p>ここで、\(\xrightarrow{P}\)は「確率収束する」という意味である。</p>
            <p>標本平均、標本分散、不偏分散の一致性は以下のとおり。</p>
            <ul>
                <li>標本平均</li>
                <ul>
                    <li>大数の弱法則\(\overline{x} \xrightarrow{P} \mu\)から、平均\(\overline{x}\)は\(n\)が大きくなると母平均に近づく。</li>
                </ul>
                <li>標本分散</li>
                <ul>
                    <li>標本分散は以下の式で表される。
                        <div class="scroll">
                        \begin{align}
                        s^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2
                        \end{align}
                        </div>
                    </li>
                    <li>この式は、以下のように変形できる。
                        <div class="scroll">
                        \begin{align}
                        s^2 &= \frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2 \\
                            &= \frac{1}{n} \sum_{i=1}^n ((x_i - \mu) - (\overline{x} - \mu))^2 \\
                            &= \frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2 - (\overline{x} - \mu)^2 \\
                        \end{align}
                        </div>
                    </li>
                    <li>この式の第２項は\((\overline{x} - \mu) \xrightarrow{P} 0\)であるため、\((\overline{x} - \mu)^2 \xrightarrow{P} 0\)となる。</li>
                    <li>また、\(\nu_i = (x_i - \mu)^2\)とおくと、\(\nu_i\)は互いに独立で同じ分布に従う確率変数となり、大数の法則を適用できる。</li>
                    <li>これから、第１項は\(\overline{\nu} \xrightarrow{P} \sigma^2\)となり、\(s^2 \xrightarrow{P} \sigma^2\)がいえる。</li>
                </ul>
                <li>不偏分散</li>
                <ul>
                    <li>不偏分散は以下の式で表される。
                        <div class="scroll">
                        \begin{align}
                        u^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline{x})^2
                        \end{align}
                        </div>
                    </li>
                    <li>\(n \rightarrow \infty\)のとき、無限大に発散する分母について、\(s^2\)と高々１しか差がない\(u^2\)も一致性を持つ。</li>
                </ul>
            </ul>
            <h3>不偏性</h3>
            <p>不偏性とは、推定量\(\hat{\theta}\)の期待値が\(E[\hat{\theta}] = \theta\)と、常に母数に等しくなる性質をいい、その性質を持つ推定量を不偏推定量と呼ぶ。</p>
            <p>これは一致性と異なり、標本の大きさ\(n\)に依存しない基準である。</p>
            <p>不偏推定量でなければ、推定には偏りがあるといい、偏りを\(E[\hat{\theta}] - \theta\)で定義する。</p>
            <p>標本平均、標本分散、不偏分散の不偏性は以下のとおり。</p>
            <ul>
                <li>標本平均</li>
                <ul>
                    <div class="scroll">
                    \begin{align}
                    E[\overline{x}] = \frac{1}{n} \sum_{i=1}^{n} E[x_i] = \frac{1}{n} \sum_{i=1}^{n} \mu = \mu
                    \end{align}
                    </div>
                </ul>
                <li>標本分散、不偏分散</li>
                <ul>
                    <li>偏差平方和を\(T_{xx} = \sum (x_i - \overline{x})^2\)とおくと、
                        <div class="scroll">
                        \begin{align}
                        T_{xx} &= \sum (x_i - \overline{x})^2 \\
                            &= \sum [(x_i - \mu) - (\overline{x} - \mu)]^2 \\
                            &= \sum [(x_i - \mu)^2 - 2 (x_i - \mu)(\overline{x} - \mu) + (\overline{x} - \mu)^2] \\
                            &= \sum (x_i - \mu)^2 - 2 (\overline{x} - \mu) \sum (x_i - \mu) + \sum (\overline{x} - \mu)^2 \\
                            &= \sum (x_i - \mu)^2 - 2 (\overline{x} - \mu) (n \overline{x} - n \mu) + n (\overline{x} - \mu)^2 \\
                            &= \sum (x_i - \mu)^2 - 2 (\overline{x} - \mu) n (\overline{x} - \mu) + n (\overline{x} - \mu)^2 \\
                            &= \sum (x_i - \mu)^2 - n(\overline{x} - \mu)^2 
                        \end{align}
                        </div>
                    となる。</li>
                    <li>ここで期待値を計算すると次の結果が得られる。
                        <div class="scroll">
                        \begin{align}
                        E[T_{xx}] &= \sum E[(x_i - \mu)^2] - n E[(\overline{x} - \mu)^2] \\
                            &= \sum V[x_i] - nV[\overline{x}] \\
                            &= n \sigma^2 - n \frac{\sigma^2}{n} \\
                            &= (n-1) \sigma^2
                        \end{align}
                        </div>
                    </li>
                    <li>したがって、標本分散は不偏性を持たず、不偏分散は不偏性を持つ。</li>
                </ul>
            </ul>
        </section>
        <section>
            <h2>情報理論</h2>
            <h3>色々なエントロピー、相互情報量</h3>
            <p>エントロピーの意味：例えば、\(H(X)\)が大きいとは、\(X\)が各値を取る確率の偏りが小さく、不確実性が大きいことを意味する。</p>
            <p>相互情報量の意味：例えば、\(I(X;Y)\)が大きいとは、\(X, Y\)の片方がわかったときに他方について得られる情報が多く、不確実性が大きく減少することを意味する。</p>
            <p>エントロピー、同時エントロピー、条件付きエントロピー、相互情報量の間に成り立つ関係式は、毎年ほぼ確実に問われるため、暗記はマストである。ベン図を思い浮かべると良い。</p>
            <p>以下に関係式を列挙する。</p>
            <div class="scroll">
            \begin{align}
            H(X,Y) \leq H(X) + H(Y) 
            \end{align}
            </div>
            <div class="scroll">
            \begin{align}
            I(X;Y) = H(X) + H(Y) - H(X, Y)
            \end{align}
            </div>
            <div class="scroll">
            \begin{align}
            I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) 
            \end{align}
            </div>
            <div class="scroll">
            \begin{align}
            H(X|Y) = H(X, Y) - H(Y) 
            \end{align}
            </div>
            <div class="scroll">
            \begin{align}
            H(Y|X) = H(X, Y) - H(X)
            \end{align}
            </div>
            <p>\(H(Y|X)\)は以下の式によって求められる。</p>
            <div class="scroll">
            \begin{align}
            H(Y|X) = - \sum P(X) \sum P(Y|X) \log_2 P(Y|X)
            \end{align}
            </div>
            <p>また、通信路容量の問題もよく出題される。</p>
            <p>無記憶定常通信路の通信路容量\(C\)は</p>
            <div class="scroll">
            \begin{align}
            C = \max_{P_X} I(X;Y)
            \end{align}
            </div>
            <p>によって計算できる。</p>
        </section>
        <section>
            <h2>アルゴリズムとデータ構造</h2>
            <h3>計算量、オーダー表記</h3>
            <p>ここ2年（2022, 2023年）は計算量の問題が出題されているため、オーダー表記はしっかり押さえておくべきである。</p>
            <p>項の強さを強い順に並べると以下のとおりである。</p>
            <ul>
                <li>\(n!, 2^n, n^3, n^2, n \log n, n, \sqrt{n}, \log n\)</li>
            </ul>
        </section>
        <section>
            <h2>人工知能</h2>
            <h3>Q学習</h3>
            <p>Q学習は、ここ10年で二度出題されている（2017, 2023年）。</p>
            <p>問われる内容は簡単なので、基礎的なことは覚えておいて損はない。</p>
            <p>有限マルコフ決定過程とは：</p>
            <ul>
                <li>マルコフ性を満たす環境においてエージェントが意思決定をして状態が確率的に遷移し、状態や行動が有限であるということ。</li>
                <li>環境がマルコフ性を持つとは、過去の環境の履歴すべてが、現在の環境情報に集約されていることを指す。</li>
            </ul>
            <p>方策の例：</p>
            <ul>
                <li>
                    グリーディ方策
                    <div class="scroll">
                    \begin{align}
                    \pi(s, a) = \arg\max\limits_a Q(s, a)
                    \end{align}
                    </div>
                </li>
                <li>
                    ε-グリーディ方策
                    <div class="scroll">
                    \begin{align}
                    \pi(s, a) = 
                    \begin{cases} 
                    \arg\max\limits_a Q(s, a) & \text{with probability } 1 - \epsilon \\
                    \text{random action} & \text{with probability } \epsilon
                    \end{cases}
                    \end{align}
                    </div>
                </li>
                <li>
                    ソフトマックス方策
                    <div class="scroll">
                    \begin{align}
                    \pi(s, a) = \frac{e^{Q(s, a) / \tau}}{\sum\limits_{b} e^{Q(s, b) / \tau}}
                    \end{align}
                    </div>
                </li>
            </ul>
            <p>Q値の更新式：</p>
            <ul>
                <div class="scroll">
                \begin{align}
                Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( r_{t+1} + \gamma \max\limits_a Q(s_{t+1}, a) - Q(s_t, a_t) \right)
                \end{align}
                </div>
            </ul>
            <p>Q値の収束性：</p>
            <ul>
                <li>エージェントが十分に多くのエピソードを経験し、すべての状態-行動ペアを探索することで、Q値は収束し、最適方策が得られる。理論的には、探索が無限に続く場合、適切な学習率と割引率の選択により、Q値は最適な値に収束することが保証されている。</li>
            </ul>
            <h3>ニューラルネットワーク</h3>
            <p>ニューラルネットワークは、人工知能の範囲では最も出題頻度が高い。つまり、最重要ポイントである。</p>
            <p>誤差逆伝播法：ネットワーク内の各重みについて損失関数の勾配を計算する方法。</p>
            <p>最急降下法：その勾配を使って重みを更新する方法。具体的には、以下のように更新される。</p>
            <div class="scroll">
            \begin{align}
            w_{i+1} &= w_i - \alpha \frac{\partial L}{\partial w_i}
            \end{align}
            </div>
            <p>ここで、\(w_i\)は更新する重み、\(\alpha\)は学習率、\(L(\cdot)\)は損失関数である。</p>
        </section>
        <section>
            <h2>コンピュータシステム</h2>
            <h3>補数表現</h3>
            <p>補数表現の問題はほぼ毎年出題される。簡単な問題が多いので、点の稼ぎどころである。</p>
            <p>補数の計算だけでなく、補数を使うことの意味も押さえておくべきである。</p>
            <p>2の補数表現を用いて負の10進数を2進数で表すには、以下の手順を踏めばよい。</p>
            <ol>
                <li>指定のビット数で、対象の10進数を正にした値を2進数で表す。</li>
                <li>ビットを反転させる。</li>
                <li>反転した値に1を加算する。</li>
            </ol>
            <p>最後に1を加算する操作を除けば、1の補数となる。</p>
            <p>1の補数には｢負のゼロ｣が存在するため、負の整数を1の補数で表現するとき、\(n\)ビットの2進数で表現できる整数の範囲は\(- 2^{n-1} + 1\)から\(2^{n-1} - 1\)までである。</p>
            <p>一方、2の補数には負のゼロが存在しないため、表現できる負の数が1つ多くなって、範囲は\(- 2^{n-1}\)から\(2^{n-1} - 1\)までとなる。</p>
            <p>補数を用いる利点としては、符号付き整数の表現に効果的であり、加算器だけで減算が可能になることが挙げられる。これにより、ハードウェアが簡素化され、計算速度が向上する。</p>
        </section>
    </main>
</body>
</html>
