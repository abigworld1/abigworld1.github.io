<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【院試対策 | 北海道大学 | 情報科学院 | 情報理工 | 最終チェック】走る作曲家のAIカフェ</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
          <h2>最終チェック</h2>
          <p>「北海道大学大学院情報科学院修士課程入学試験」（令和6年8月実施）の情報理工学コース対策ページです。</p>
            <p>10年分の過去問（2014~2023）を解いた私が、過去に複数回出題されていた問題や狙われそうなポイントをまとめます。</p>
        </section>
        <section>
          <h2>分野別対策</h2>
          <ul>
            <li><a href="gradexam.html">院試対策 ～はじめに～</a></li>
            <li><a href="gradexam_basicmath.html">院試対策 ～基礎数学編～</a></li>
            <li><a href="gradexam_infomath.html">院試対策 ～情報数学編～</a></li>
            <li><a href="gradexam_statistics.html">院試対策 ～確率・統計編～</a></li>
            <li><a href="gradexam_infotheory.html">院試対策 ～情報理論編～</a></li>
            <li><a href="gradexam_algo.html">院試対策 ～アルゴリズムとデータ構造編～</a></li>
            <li><a href="gradexam_ai.html">院試対策 ～人工知能編～</a></li>
            <li><a href="gradexam_cs.html">院試対策 ～コンピュータシステム編～</a></li>
            <li><a href="gradexam_final.html">院試対策 ～最終チェック～</a></li>
          </ul>
        </section>
        <section>
            <h2>基礎数学</h2>
            <h3>停留点</h3>
            <p>停留点を求める問題は、ここ10年で複数回出題されている。</p>
            <p>停留点は関数の変化がなくなる点であるため、\(x\)方向、\(y\)方向ともにその変化量が0、すなわち</p>
            <div class="scroll">
                \begin{align}
                \frac{\partial f(x,y)}{\partial x}=\frac{\partial f(x,y)}{\partial y}=0
                \end{align}
            </div>
            <p>となる点を求めればよい。</p>
            <p>つまり、\(x\)と\(y\)の偏微分を求めて、それらをイコール0にした2式を連立して解けばよい。</p>
            <p>このように停留点は簡単に求まるが、次に、その停留点が極大値なのか、極小値なのか、鞍点なのかを判定せよ、と問われることが多い。</p>
            <p>停留点を判定するには、以下で定義されるヘッセ行列を使う。</p>
            <div class="scroll">
                \begin{align}
                H=
                    \begin{pmatrix}\displaystyle{\frac{\partial^{2} f(x,y)}{\partial x^{2}}}&\displaystyle{\frac{\partial^{2} f(x,y)}{\partial x\partial y}} \\ \\ 
                    \displaystyle{\frac{\partial^{2} f(x,y)}{\partial x\partial y}}&\displaystyle{\frac{\partial^{2} f(x,y)}{\partial y^{2}}}\end{pmatrix}
                \end{align}
            </div>
            <p>したがって、二階偏微分を求める必要がある。</p>
            <p>二階偏微分を求めたら、ヘッセ行列の固有値\(\lambda_{1},\lambda_{2}\)を求めることによって、以下のように判別できる。</p>
            <div class="scroll">
                \begin{align}
                \begin{cases}
                    \lambda_{1}>0,\lambda_{2}>0&\to\,\text{極小点}\\ 
                    \lambda_{1}<0,\lambda_{2}<0&\to\,\text{極大点}\\ 
                    \lambda_{1}\lambda_{2}<0&\to\,\text{鞍点} 
                \end{cases}
                \end{align}
            </div>
            <h3>極座標変換</h3>
            <p>極座標変換ができないと解くのが難しい問題はここ10年で何度も出題されている。</p>
            <p>まずは2次元の極座標変換を説明する。</p>
            <p>2次元の極座標変換では、\(x\)と\(y\)を次のようにおく。</p>
            <div class="scroll">
                \begin{align}
                x = r \cos \theta , \ \ \ y = r \sin \theta 
                \end{align}
            </div>
            <p>このとき、ヤコビアンを計算すると以下のようになる。</p>
            <div class="scroll">
                \begin{align}
                J = & \left| \begin{array}{ccc} \frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\ \frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta} \end{array} \right|
                    \\ = & \left| \begin{array}{ccc} \cos \theta & - r \sin \theta \\ \sin \theta & r \cos \theta \end{array} \right|
                    \\ = & \ r \left( \cos^2 \theta + \sin^2 \theta \right)
                    \\ = & \ r
                \end{align}
            </div>
            <p>したがって、\(dxdy = r \ dr d \theta\)となる。</p>
            <p>次に、3次元の極座標変換を説明する。</p>
            <p>3次元の極座標変換では、\(x\)と\(y\)を次のようにおく。</p>
            <div class="scroll">
                \begin{align}
                x = r \cos \theta \cos \varphi, \quad y = r \cos \theta \sin \varphi, \quad z = r \sin \theta
                \end{align}
            </div>
            <p>このとき、ヤコビアン行列\(J\)は以下のようになる。</p>
            <div class="scroll">
                \begin{align}
                J = \frac{\partial(x, y, z)}{\partial(r, \varphi, \theta)} = 
                \begin{pmatrix}
                \cos \theta \cos \varphi & -r \cos \theta \sin \varphi & -r \sin \theta \cos \varphi \\
                \cos \theta \sin \varphi & r \cos \theta \cos \varphi & -r \sin \theta \sin \varphi \\
                \sin \theta & 0 & r \cos \theta
                \end{pmatrix}
                \end{align}
            </div>
            <p>したがって、ヤコビアン行列の行列式 \( \det(J) \) は次のように計算される：</p>
            <div class="scroll">
                \begin{align}
                \det(J) = r^2 \cos \theta
                \end{align}
            </div>
            <p>以上のことは、\(\cos \theta\)と\(\sin \theta\)を入れ替えても成り立つ。</p>
        </section>
        <section>
            <h2>情報数学</h2>
            <h3>言語</h3>
            <p>言語の中でも、特に正規言語と文脈自由言語について問われることが多い。それぞれの代表例や特徴は必ず覚えておいた方が良い。</p>
            <p>まずは、正規言語を説明する。</p>
            <p>正規言語は、有限オートマトンによって認識される言語であり、正規文法によって生成される。</p>
            <p>正規文法の例を以下に示す。</p>
            <ul>
                <li>\( S \rightarrow aS \)</li>
                <li>\( S \rightarrow bS \)</li>
                <li>\( S \rightarrow \epsilon \) （\(\epsilon\)は空文字を表す）</li>
            </ul>
            <p>この文法は、\(\{a, b\}\)からなる任意の文字列（例えば、\(\epsilon\), \(a\), \(b\), \(aa\), \(ab\), \(ba\), \(bb\) など）を生成する。</p>
            <p>他にも、正規言語の例として、以下があげられる。</p>
            <ul>
                <li>\(L = { a^n \mid n ≥ 0 }\)(空文字列または任意の個数の \(a\) からなる言語。)</li>
                <li>\(L = (ab)^*\)(\(ab\) の繰り返しからなる言語。)</li>
            </ul>
            <p>次に、文脈自由言語を説明する。</p>
            <p>文脈自由言語は、プッシュダウンオートマトンによって認識される言語であり、文脈自由文法によって生成される。</p>
            <p>文脈自由文法は、生成規則が左辺に1つの非終端記号を持ち、右辺が任意の文字列であるような形をしている文法である。</p>
            <p>文脈自由文法の例を以下に示す。</p>
            <ul>
                <li>\( S \rightarrow aSb \)</li>
                <li>\( S \rightarrow \epsilon \)</li>
            </ul>
            <p>この文法は、同数の \(a\) と \(b\) を持つ文字列（例えば、\(\epsilon\), \(ab\), \(aabb\), \(aaabbb\) など）を生成する。</p>
            <p>この言語は、正規文法では表現できないが、文脈自由文法によって生成可能である。</p>
            <p>他にも、文脈自由文法の例として、以下があげられる。</p>
            <ul>
                <li>\(L = { a^n b^n | n ≥ 0 }\)(同じ数の \(a\) と \(b\) からなる言語。)</li>
                <li>\(ww^R | w ∈ {a, b}^*\)(任意の文字列 \(w\) とその逆順 \(w^R\) からなる言語。)</li>
            </ul>
            <p>ついでに、文脈依存文法も説明する。</p>
            <p>文脈依存言語は、線形有界オートマトンによって認識される言語であり、文脈依存文法によって生成される言語である。</p>
            <p>文脈依存文法の生成規則は、形式 \(α A β → α γ β\) を持ち、ここで \(A\) は非終端記号、\(α\)、\(β\)、および \(γ\) は任意の文字列である。</p>
            <p>文脈依存言語の例を以下に示す。</p>
            <ul>
                <li>\(L = { a^n b^n c^n | n ≥ 1 }\)(同じ数の \(a, b, c\) からなる言語。)</li>
            </ul>
        </section>
        <section>
            <h2>確率・統計</h2>
            <h3>一致性</h3>
            <p>一致性とは、ある母数\(\theta\)の推定量\(\hat{\theta}\)が\(\hat{\theta} \xrightarrow{P} \theta\)を満たすことをいい、\(\hat{\theta}\)を一致推定量と呼ぶ。</p>
            <p>ここで、\(\xrightarrow{P}\)は「確率収束する」という意味である。</p>
            <p>標本平均、標本分散、不偏分散の一致性は以下のとおり。</p>
            <ul>
                <li>標本平均</li>
                <ul>
                    <li>大数の弱法則\(\overline{x} \xrightarrow{P} \mu\)から、平均\(\overline{x}\)は\(n\)が大きくなると母平均に近づく。</li>
                </ul>
                <li>標本分散</li>
                <ul>
                    <li>標本分散は以下の式で表される。
                        <div class="scroll">
                        \begin{align}
                        s^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2
                        \end{align}
                        </div>
                    </li>
                    <li>この式は、以下のように変形できる。
                        <div class="scroll">
                        \begin{align}
                        s^2 &= \frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2 \\
                            &= \frac{1}{n} \sum_{i=1}^n ((x_i - \mu) - (\overline{x} - \mu))^2 \\
                            &= \frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2 - (\overline{x} - \mu)^2 \\
                        \end{align}
                        </div>
                    </li>
                    <li>この式の第２項は\((\overline{x} - \mu) \xrightarrow{P} 0\)であるため、\((\overline{x} - \mu)^2 \xrightarrow{P} 0\)となる。</li>
                    <li>また、\(\nu_i = (x_i - \mu)^2\)とおくと、\(\nu_i\)は互いに独立で同じ分布に従う確率変数となり、大数の法則を適用できる。</li>
                    <li>これから、第１項は\(\overline{\nu} \xrightarrow{P} \sigma^2\)となり、\(s^2 \xrightarrow{P} \sigma^2\)がいえる。</li>
                </ul>
                <li>不偏分散</li>
                <ul>
                    <li>不偏分散は以下の式で表される。
                        <div class="scroll">
                        \begin{align}
                        u^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline{x})^2
                        \end{align}
                        </div>
                    </li>
                    <li>\(n \rightarrow \infty\)のとき、無限大に発散する分母について、\(s^2\)と高々１しか差がない\(u^2\)も一致性を持つ。</li>
                </ul>
            </ul>
            <h3>不偏性</h3>
            <p>不偏性とは、推定量\(\hat{\theta}\)の期待値が\(E[\hat{\theta}] = \theta\)と、常に母数に等しくなる性質をいい、その性質を持つ推定量を不偏推定量と呼ぶ。</p>
            <p>これは一致性と異なり、標本の大きさ\(n\)に依存しない基準である。</p>
            <p>不偏推定量でなければ、推定には偏りがあるといい、偏りを\(E[\hat{\theta}] - \theta\)で定義する。</p>
            <p>標本平均、標本分散、不偏分散の不偏性は以下のとおり。</p>
            <ul>
                <li>標本平均</li>
                <ul>
                    <div class="scroll">
                    \begin{align}
                    E[\overline{x}] = \frac{1}{n} \sum_{i=1}^{n} E[x_i] = \frac{1}{n} \sum_{i=1}^{n} \mu = \mu
                    \end{align}
                    </div>
                </ul>
                <li>標本分散、不偏分散</li>
                <ul>
                    <li>偏差平方和を\(T_{xx} = \sum (x_i - \overline{x})^2\)とおくと、
                        <div class="scroll">
                        \begin{align}
                        T_{xx} &= \sum (x_i - \overline{x})^2 \\
                            &= \sum [(x_i - \mu) - (\overline{x} - \mu)]^2 \\
                            &= \sum [(x_i - \mu)^2 - 2 (x_i - \mu)(\overline{x} - \mu) + (\overline{x} - \mu)^2] \\
                            &= \sum (x_i - \mu)^2 - 2 (\overline{x} - \mu) \sum (x_i - \mu) + \sum (\overline{x} - \mu)^2 \\
                            &= \sum (x_i - \mu)^2 - 2 (\overline{x} - \mu) (n \overline{x} - n \mu) + n (\overline{x} - \mu)^2 \\
                            &= \sum (x_i - \mu)^2 - 2 (\overline{x} - \mu) n (\overline{x} - \mu) + n (\overline{x} - \mu)^2 \\
                            &= \sum (x_i - \mu)^2 - n(\overline{x} - \mu)^2 
                        \end{align}
                        </div>
                    となる。</li>
                    <li>ここで期待値を計算すると次の結果が得られる。
                        <div class="scroll">
                        \begin{align}
                        E[T_{xx}] &= \sum E[(x_i - \mu)^2] - n E[(\overline{x} - \mu)^2] \\
                            &= \sum V[x_i] - nV[\overline{x}] \\
                            &= n \sigma^2 - n \frac{\sigma^2}{n} \\
                            &= (n-1) \sigma^2
                        \end{align}
                        </div>
                    </li>
                    <li>したがって、標本分散は不偏性を持たず、不偏分散は不偏性を持つ。</li>
                </ul>
            </ul>
        </section>
    </main>
</body>
</html>
