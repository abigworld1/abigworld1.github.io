<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【論文紹介 | Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation】走る作曲家のAIカフェ</title>
    <link rel="stylesheet" href="style.css"> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1050827580219099"
     crossorigin="anonymous"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
            <h2>目次</h2>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#abstract">Abstract</a></li>
                <li><a href="#conclusion">Conclusion</a></li>
            </ul>
        </section>
        <section id="overview">
            <h2>Overview</h2>
            "<a href="https://arxiv.org/pdf/1604.06057">Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation</a>" は、h-DQN (階層型Deep Q-Network) を提案しています。このモデルは、異なる時間スケールで動作する階層的な価値関数を統合することで、内発的に動機づけられた探索行動を促し、報酬が遅延する環境において効率的な探索を実現します。<br>
            このページでは、その内容を順に追っていきます。
        </section>
        <section id="abstract">
          <h2>Abstract</h2>
            <ul>
                <li>スパースなフィードバックの環境で行動を学ぶことは、強化学習において大きな課題である。</li>
                <li>探索が不十分で、エージェントがロバストな方策を学習できないことが多い。</li>
                <li>内発的に動機づけられたエージェントは、新たな行動を探索できる。</li>
                <li>h-DQN（階層型DQN）は、異なる時間スケールで動作する階層的な行動価値関数と、内発的動機づけを用いた深層強化学習を統合するフレームワークである。</li>
                <li>上位レベルのQ関数は、内発的目標に基づいた方策を学習し、下位レベルの関数は与えられた目標を達成するための基本的な行動に基づいた方策を学習する。</li>
                <li>提案手法の有効性を、非常にスパースで遅延したフィードバックが存在する2つの問題で実証する：（1）複雑で離散的な確率的意思決定プロセス、（2）古典的なATARIゲーム「モンテズマの復讐」。</li>
            </ul>
        </section>
        <section>
            <h2>Conclusion</h2>
            <ul>
                <li>h-DQNを提案し、異なる時間スケールで動作する階層的な価値関数を持つフレームワークを構築した。</li>
                <li>価値関数を時間的に分解することで、エージェントが内発的に動機づけられた行動を行い、遅延報酬のある環境で効率的な探索が可能になる。</li>
                <li>エンティティと関係の空間で内発的動機をパラメータ化することにより、時間的に拡張された探索を行うエージェントを構築する有望な手段を提供できる。</li>
                <li>将来的には、h-DQNにおける目標の代替的なパラメータ化の可能性も探求する予定である。</li>
                <li>現在のフレームワークには、以下のような欠けている要素がある：（1）生のピクセルデータからオブジェクトを自動的に分離する機能、（2）短期記憶機能。</li>
                <li>vanilla DQNで学習された状態抽象化は、構造化や十分な構成性が不足している。</li>
                <li>最近の研究では、深層生成モデルを使用してピクセルデータからオブジェクトや姿勢、位置などの多様な要因を分離する方法が検討されている。</li>
                <li>本研究が、h-DQNと画像生成モデルの組み合わせの動機づけとなることを期待する。</li>
                <li>長期的な依存関係に対応するために、エージェントが過去の目標、行動、表現の履歴を保存する必要がある。</li>
                <li>最近の研究では、強化学習と再帰型ネットワークを組み合わせた手法も検討されている。</li>
                <li>より複雑な非マルコフ環境に対応するためには、柔軟なエピソード記憶モジュールを取り入れることが必要である。</li>
            </ul>
        </section>
        <section>
            <h2>Model</h2>
            <ul>
                <li>状態 \( s \in S \)、行動 \( a \in A \)、および遷移関数 \( T : (s, a) \rightarrow s' \) で表されるマルコフ決定過程（MDP）を考える。</li>
                <li>この枠組みで動作するエージェントは外部環境から状態 \( s \) を受け取り、行動 \( a \) を取ることで新たな状態 \( s' \) へ遷移する。</li>
                <li>外因的報酬関数を \( F : (s) \rightarrow R \) と定義する。</li>
                <li>エージェントの目標は、この関数を長期にわたって最大化することである。</li>
                <li>例）エージェントの生存時間やゲームのスコアなど。</li>
                <li>MDPにおける効果的な探索は、良い制御方策を学習するうえで重要な課題である。</li>
                <li>\(\varepsilon\)-greedyのような手法は局所的な探索には有効だ、状態空間の異なる領域を探索する動機をエージェントに与えることはできない。</li>
                <li>この問題に対処するために、目標 \( g \in G \) の概念を利用し、エージェントに内発的な動機を与える。</li>
                <li>エージェントは、外因的な累積報酬和を最大化するために、目標の順序を設定し、それを達成することに集中する。</li>
                <li>各目標 \( g \) に対する方策 \( \pi_g \) を定義するために、オプションの時間的抽象化を利用する。</li>
                <li>エージェントは、これらのオプション方策を学習すると同時に、追従するべき最適な目標の順序も学習する。</li>
                <li>各 \( \pi_g \) を学習するために、エージェントには批評者（クリティック）があり、エージェントが目標を達成できるかどうかに基づいて内因的な報酬を提供する。</li>
                <li>図1に示すように、エージェントはコントローラとメタコントローラからなる2段階の階層構造を使用する。</li>
                <img src="images/h-dqn1.png" alt="h-dqn1" class="responsive-image">
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
        </section>
    </main>
</body>
</html>
