<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【LLM入門】走る作曲家のAIカフェ</title>
    <link rel="stylesheet" href="style.css"> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1050827580219099"
     crossorigin="anonymous"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
            <h2>目次</h2>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#source">Source</a></li>
                <li><a href="#into">Introduction</a></li>
                <li><a href="#transformer">Transformer</a></li>
            </ul>
        </section>
        <section id="overview">
            <h2>Overview</h2>
            大規模言語モデルは、大規模なテキストデータを使って訓練された大規模なテキストデータを使って訓練された大規模なパラメータで構成されるニューラルネットワークです。<br>
        </section>
        <section id="source">
            <h2>Source</h2>
            <p>以下の講義・書籍・サイトを参考にしました。</p>
            <ul>
                <li><a href="https://www.amazon.co.jp/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E5%85%A5%E9%96%80-%E5%B1%B1%E7%94%B0-%E8%82%B2%E7%9F%A2/dp/4297136333/ref=sr_1_1?__mk_ja_JP=%E3%82%AB%E3%82%BF%E3%82%AB%E3%83%8A&crid=2NRUXU6OPORG4&dib=eyJ2IjoiMSJ9.msAfTiVGwyLdpxEClh6ohw5tXQIDiC4gRKh9jgYqHnA5gLSAUQdrvLkJ7dAVQHu4Sz5PDvQVPKbV5EzTJ_9yxxHB3Ju_MNdyWkNODlgdqYPwMmGuu2ezB7HMSl1rn2kKB4etPeHj7vkHg7bfsyPJgItIHX_jc2DyprYE2V0XCnlZYUDWzWfva356UfhBuuLCmy1lOWeYtB1YA1KKQXBbW3Tm2uyNmDZEUXW0N2WO6F6k5T95Qqx9ZnKPuDiZub9o51S6HfFnqne7wYUqCB--mD4aXnAyjfB54ltsdsglqpU.Z3NIrXIdpEAf0ttW3STkkxFVSSNkdzTNDasiPYmYLlc&dib_tag=se&keywords=%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB+%E5%85%A5%E9%96%80&qid=1726382190&sprefix=%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB+%E5%85%A5%E9%96%80%2Caps%2C189&sr=8-1">大規模言語モデル入門</a></li>
                <li><a href="https://github.com/ghmagazine/llm-book">ghmagazine_llm-book_ 「大規模言語モデル入門」（2023）と「大規模言語モデル入門Ⅱ〜生成型LLMの実装と評価」（2024）のGitHubリポジトリ.html</a></li>
            </ul>     
        </section>
        <section id="intro">
          <h2>Introduction</h2>
          <h3>文書分類</h3>
          <p>文書分類（document classification）：テキストを予め定められたラベルに分類するタスク。</p>
          <p>感情分析（sentiment analysis）：テキストから読み取れる感情を検出する文書分類。</p>
          <h3>自然言語推論</h3>
          <p>自然言語推論（natural language inference; NLI）：２つのテキストの論理関係を予測するタスク。言語モデルの意味理解能力を評価するために使用される。</p>
          <ul>
            <li>entailment：含意</li>
            <li>contradiction：矛盾</li>
            <li>neutral：中立</li>
          </ul>
          <h3>意味的類似度計算</h3>
          <p>意味的類似度計算（semantic textual similarity; STS）：２つのテキストの意味が似ている度合をスコアとして予測するタスク。情報検索や、複数テキストの内容の整合性を確認する際に役立つ。</p>
          <p>文埋め込み（sentence embedding）モデルから得られるテキストのコサイン類似度を、意味的類似度とみなすことができる。</p>
          <h3>固有表現認識</h3>
          <p>固有表現認識（named entity recognition; NER）：テキストに含まれる固有表現を抽出するタスク。</p>
          <h3>要約生成</h3>
          <p>要約生成（summarization generation）：比較的長い文章から短い要約を生成するタスク。</p>
          <h3>transformersの基本的な使い方</h3>
          <p>transformersで大規模言語モデルを使った開発を行う際には、基本的にAuto Classesというクラス群を使う。</p>
          <p>transformersでは多くの種類のモデルが提供されているが、Auto Classesを使うと、その中から適切な実装を自動的に選択してくれる。</p>
          <p>Auto Classesでは、モデルを表すAutoModelと入力テキストを分割するAutoTokenizerを主に使う。</p>
          <p>これらのクラスにはfrom_pretrainedというメソッドが用意されていて、このメソッドにHugging Face Hubのモデルの名称や、モデルの保存されているフォルダを渡すことで、クラスのインスタンスを作成する。</p>
          <p>大規模言語モデルを含む多くの自然言語処理のモデルでは、テキストを細かい単位に分割してからモデルに入力する。</p>
          <p>モデルが扱う基本的な単位をトークン（token）、トークンに分割する処理をトークナイゼーション（tokenization）、トークン単位に分割する実装をトークナイザ（tokenizer）と呼ぶ。</p>
          <h3>単語埋め込みとニューラルネットワークの基礎</h3>
          <b>単語埋め込み（word embedding）</b>
          <p>word2vec：単語の意味を表現したベクトルを大規模なテキストから学習できることを示したニューラルネットワーク。
          ある単語の意味は、周辺に出現する単語によって表せると考える分布仮説（distributional hypothesis）に基づいて設計されている。</p>
          <p>コーパス（corpus）：自然言語処理に用いるために構築されたテキストのこと。</p>
          <p>skip-gram：word2vecのモデルの１つ。文中の単語を順に処理していき、単語からその周辺単語を予測できるように単語と周辺単語の関係を学習する。</p>
          <p>語彙（vocabulary）：モデルで扱うすべての単語の集合のこと。</p>
          <p>word2vecでは、単語の予測確率はソフトマックス関数（softmax function）を用いて計算される。</p>
          <p>線形変換を行う層を、線形層（linear layer）または全結合層（fully-connected layer）と呼ぶ。</p>
          <p>単語列を使った訓練は、負の対数尤度（negative log-likelihood）または交差エントロピー（cross entropy）を最小化することで行われる。</p>
          <p>この損失関数を最小化するには、中央単語の埋め込みと周辺単語の埋め込みの内積を最大化する必要がある。</p>
          <p>こうした訓練を大規模コーパスを用いて行うことで、単語のベクトル空間上で、関連性の高い単語同士は近くに配置されていく。</p>
          <p>自己教師あり学習：入力から自動的に予測するラベルを生成して学習を行なう方式。
          <p>word2vecの登場以降、文脈を考慮した単語埋め込みである文脈化単語埋め込み（contextualized word embedding）を大規模なコーパスから自己教師あり学習で獲得するモデルが提案された。</p>
          <p>こうしたモデルでは、単語に対して１つの埋め込みが割り当てられるword2vecとは異なり、入力テキストの周辺の文脈を加味して動的に単語埋め込みが計算される。</p>
          <p>文脈化単語埋め込みを計算するTransformerを大規模コーパスでの自己教師あり学習で事前学習し、そのモデルを下流タスクのデータセットを使って微調整して解く方法が、自然言語処理の標準的な手法になっている。</p>
          <p>こうした事前学習した大規模なニューラルネットワークは、大規模言語モデルや事前学習済み言語モデル（pre-trained language model; PLM）のように呼ばれる。</p>
        </section>
        <section id="transformer">
            <h2>Transformer</h2>
            <p>Transformerは、2017年にGoogleが提案したニューラルネットワーク。</p>
            <p>入力には、単語よりも細かい単位であるサブワード（sub-word）や文字を使うことが一般的。</p>
            <h3>概要</h3>
            <p>Transformerには、エンコーダ・デコーダ（encoder-decoder）、エンコーダのみ、デコーダのみの3種類の構成が存在する。</p>
            <p>エンコーダのみで使う場合は入力に対する文脈化トークン埋め込みを出力するモデル、デコーダのみの場合は入力から次のトークンを予測するモデルとして使う。</p>
            <h3>エンコーダ</h3>
            <p>単純なエンコーダのみのTransformerについて。</p>
            <p>エンコーダは入力トークン列に対応する文脈化トークン埋め込みを出力する。</p>
            <p>Transformerでは、入力トークン埋め込みに対して、多段的に文脈情報を付与する。</p>
            <p>この多段的な処理によって、低い位置にある層では表層的、中間にある層では文法的、高い位置にある層では意味的というように、複雑で抽象的な文脈を捉えられるようになると考えられている。</p>
            <h3>入力トークンの埋め込み</h3>
            <p>Transformerでは、語彙Ｖに含まれるすべてのトークンに対して、Ｄ次元の入力トークン埋め込みを付与する。</p>
        </section>
