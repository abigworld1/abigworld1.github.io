<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【LLM入門】走る作曲家のAIカフェ</title>
    <link rel="stylesheet" href="style.css"> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1050827580219099"
     crossorigin="anonymous"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
            <h2>目次</h2>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#source">Source</a></li>
                <li><a href="#into">Introduction</a></li>
                <li><a href="#transformer">Transformer</a></li>
            </ul>
        </section>
        <section id="overview">
            <h2>Overview</h2>
            大規模言語モデルは、大規模なテキストデータを使って訓練された大規模なテキストデータを使って訓練された大規模なパラメータで構成されるニューラルネットワークです。<br>
        </section>
        <section id="source">
            <h2>Source</h2>
            <p>以下の講義・書籍・サイトを参考にしました。</p>
            <ul>
                <li><a href="https://www.amazon.co.jp/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E5%85%A5%E9%96%80-%E5%B1%B1%E7%94%B0-%E8%82%B2%E7%9F%A2/dp/4297136333/ref=sr_1_1?__mk_ja_JP=%E3%82%AB%E3%82%BF%E3%82%AB%E3%83%8A&crid=2NRUXU6OPORG4&dib=eyJ2IjoiMSJ9.msAfTiVGwyLdpxEClh6ohw5tXQIDiC4gRKh9jgYqHnA5gLSAUQdrvLkJ7dAVQHu4Sz5PDvQVPKbV5EzTJ_9yxxHB3Ju_MNdyWkNODlgdqYPwMmGuu2ezB7HMSl1rn2kKB4etPeHj7vkHg7bfsyPJgItIHX_jc2DyprYE2V0XCnlZYUDWzWfva356UfhBuuLCmy1lOWeYtB1YA1KKQXBbW3Tm2uyNmDZEUXW0N2WO6F6k5T95Qqx9ZnKPuDiZub9o51S6HfFnqne7wYUqCB--mD4aXnAyjfB54ltsdsglqpU.Z3NIrXIdpEAf0ttW3STkkxFVSSNkdzTNDasiPYmYLlc&dib_tag=se&keywords=%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB+%E5%85%A5%E9%96%80&qid=1726382190&sprefix=%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB+%E5%85%A5%E9%96%80%2Caps%2C189&sr=8-1">大規模言語モデル入門</a></li>
                <li><a href="https://github.com/ghmagazine/llm-book">ghmagazine_llm-book_ 「大規模言語モデル入門」（2023）と「大規模言語モデル入門Ⅱ〜生成型LLMの実装と評価」（2024）のGitHubリポジトリ.html</a></li>
            </ul>     
        </section>
        <section id="intro">
          <h2>Introduction</h2>
          <h3>文書分類</h3>
          <p>文書分類（document classification）：テキストを予め定められたラベルに分類するタスク。</p>
          <p>感情分析（sentiment analysis）：テキストから読み取れる感情を検出する文書分類。</p>
          <h3>自然言語推論</h3>
          <p>自然言語推論（natural language inference; NLI）：２つのテキストの論理関係を予測するタスク。言語モデルの意味理解能力を評価するために使用される。</p>
          <ul>
            <li>entailment：含意</li>
            <li>contradiction：矛盾</li>
            <li>neutral：中立</li>
          </ul>
          <h3>意味的類似度計算</h3>
          <p>意味的類似度計算（semantic textual similarity; STS）：２つのテキストの意味が似ている度合をスコアとして予測するタスク。情報検索や、複数テキストの内容の整合性を確認する際に役立つ。</p>
          <p>文埋め込み（sentence embedding）モデルから得られるテキストのコサイン類似度を、意味的類似度とみなすことができる。</p>
          <h3>固有表現認識</h3>
          <p>固有表現認識（named entity recognition; NER）：テキストに含まれる固有表現を抽出するタスク。</p>
          <h3>要約生成</h3>
          <p>要約生成（summarization generation）：比較的長い文章から短い要約を生成するタスク。</p>
          <h3>transformersの基本的な使い方</h3>
          <p>transformersで大規模言語モデルを使った開発を行う際には、基本的にAuto Classesというクラス群を使う。</p>
          <p>transformersでは多くの種類のモデルが提供されているが、Auto Classesを使うと、その中から適切な実装を自動的に選択してくれる。</p>
          <p>Auto Classesでは、モデルを表すAutoModelと入力テキストを分割するAutoTokenizerを主に使う。</p>
          <p>これらのクラスにはfrom_pretrainedというメソッドが用意されていて、このメソッドにHugging Face Hubのモデルの名称や、モデルの保存されているフォルダを渡すことで、クラスのインスタンスを作成する。</p>
          <p>大規模言語モデルを含む多くの自然言語処理のモデルでは、テキストを細かい単位に分割してからモデルに入力する。</p>
          <p>モデルが扱う基本的な単位をトークン（token）、トークンに分割する処理をトークナイゼーション（tokenization）、トークン単位に分割する実装をトークナイザ（tokenizer）と呼ぶ。</p>
          <h3>単語埋め込みとニューラルネットワークの基礎</h3>
          <b>単語埋め込み（word embedding）</b>
          <p>word2vec：単語の意味を表現したベクトルを大規模なテキストから学習できることを示したニューラルネットワーク。
          ある単語の意味は、周辺に出現する単語によって表せると考える分布仮説（distributional hypothesis）に基づいて設計されている。</p>
          <p>コーパス（corpus）：自然言語処理に用いるために構築されたテキストのこと。</p>
          <p>skip-gram：word2vecのモデルの１つ。文中の単語を順に処理していき、単語からその周辺単語を予測できるように単語と周辺単語の関係を学習する。</p>
          <p>語彙（vocabulary）：モデルで扱うすべての単語の集合のこと。</p>
          <p>word2vecでは、単語の予測確率はソフトマックス関数（softmax function）を用いて計算される。</p>
          <p>線形変換を行う層を、線形層（linear layer）または全結合層（fully-connected layer）と呼ぶ。</p>
          <p>単語列を使った訓練は、負の対数尤度（negative log-likelihood）または交差エントロピー（cross entropy）を最小化することで行われる。</p>
          <p>この損失関数を最小化するには、中央単語の埋め込みと周辺単語の埋め込みの内積を最大化する必要がある。</p>
          <p>こうした訓練を大規模コーパスを用いて行うことで、単語のベクトル空間上で、関連性の高い単語同士は近くに配置されていく。</p>
          <p>自己教師あり学習：入力から自動的に予測するラベルを生成して学習を行なう方式。
          <p>word2vecの登場以降、文脈を考慮した単語埋め込みである文脈化単語埋め込み（contextualized word embedding）を大規模なコーパスから自己教師あり学習で獲得するモデルが提案された。</p>
          <p>こうしたモデルでは、単語に対して１つの埋め込みが割り当てられるword2vecとは異なり、入力テキストの周辺の文脈を加味して動的に単語埋め込みが計算される。</p>
          <p>文脈化単語埋め込みを計算するTransformerを大規模コーパスでの自己教師あり学習で事前学習し、そのモデルを下流タスクのデータセットを使って微調整して解く方法が、自然言語処理の標準的な手法になっている。</p>
          <p>こうした事前学習した大規模なニューラルネットワークは、大規模言語モデルや事前学習済み言語モデル（pre-trained language model; PLM）のように呼ばれる。</p>
        </section>
        <section id="transformer">
            <h2>Transformer</h2>
            <p>Transformerは、2017年にGoogleが提案したニューラルネットワーク。</p>
            <p>入力には、単語よりも細かい単位であるサブワード（sub-word）や文字を使うことが一般的。</p>
            <h3>概要</h3>
            <p>Transformerには、エンコーダ・デコーダ（encoder-decoder）、エンコーダのみ、デコーダのみの3種類の構成が存在する。</p>
            <p>エンコーダのみで使う場合は入力に対する文脈化トークン埋め込みを出力するモデル、デコーダのみの場合は入力から次のトークンを予測するモデルとして使う。</p>
            <h3>エンコーダ</h3>
            <p>単純なエンコーダのみのTransformerについて。</p>
            <p>エンコーダは入力トークン列に対応する文脈化トークン埋め込みを出力する。</p>
            <p>Transformerでは、入力トークン埋め込みに対して、多段的に文脈情報を付与する。</p>
            <p>この多段的な処理によって、低い位置にある層では表層的、中間にある層では文法的、高い位置にある層では意味的というように、複雑で抽象的な文脈を捉えられるようになると考えられている。</p>
            <h3>入力トークンの埋め込み</h3>
            <p>Transformerでは、語彙Vに含まれるすべてのトークンに対して、D次元の入力トークン埋め込みを付与する。</p>
            <p>例えば、トークンwに対する入力トークン埋め込みを\(\mathbf{e}_w\)とすると、各トークンに対応する5個の入力トークン埋め込みで構成される埋め込み列\(\mathbf{e_{こたつ}}, \mathbf{e_{で}}, \mathbf{e_{みかん}}, \mathbf{e_{を}}, \mathbf{e_{食べる}}\)
            が後述する位置符号と加算されてエンコーダに入力される。</p>
            <p>入力トークン埋め込みには、トークンについての静的な情報が保持されていて、この埋め込みに対して多層のブロックを通じて動的に文脈情報が付与されていく。</p>
            <p>このモデルに含まれるすべての入力トークン埋め込みは、各行が個別のトークンに対応する\(|V| \times D\)次元の入力トークン埋め込み行列\(\mathbf{E} = [\mathbf{e_1}, \mathbf{e_2}, \cdots, e_{|V|}]^T\)で表すことができる。</p>
            <h3>位置符号</h3>
            <p>上述の入力トークン埋め込みは、トークンの順序や位置に関する情報を含んでいない。</p>
            <p>また、Transformerのブロックはトークンの順序を考慮しないため、例えば、「こたつでみかんを食べる」と「みかんでこたつを食べる」は同一の入力とされてしまう。</p>
            <p>これを解決するために、入力トークン埋め込みに対して位置符号（position encoding）を付加する。</p>
            <p>位置符号は正弦関数（sinusoidal function）を使ってトークン列の中でのトークンの位置iに対応するD次元の位置符号\(\mathbf{p_i}\)は、そのj番目の要素を\(p_{i, j}\)とすると、\(k \in {0, 1, \cdots, \frac{D}{2} - 1}\)に対して以下のように計算される。</p>
            <div class="scroll">
                \begin{align}
                p_{i, 2k+1} = \sin (\frac{i}{10000^{2k/D}}) \\
                p_{i, 2k+2} = \cos (\frac{i}{10000^{2k/D}}) 
                \end{align}
            </div>
            <p>つまり、次元が大きくなるにつれて波長が\(2 \pi\)から約\(10000 \cdot 2 \pi\)まで大きくなっていく。</p>
            <p>位置iのトークン\(w_i\)の入力トークン埋め込みを\(\mathbf{e_{w_i}}\)とすると、モデルの入力埋め込み\(\mathbf{x_i}\)は以下のように計算される。</p>
            <div class="scroll">
                \begin{align}
                \mathbf{x_i} = \sqrt{D} \mathbf{e_{w_i}} + \mathbf{p_i}
                \end{align}
            </div>
            <p>したがって、入力トークン数がNなら、モデルには\(\mathbf{x_1}, \mathbf{x_2}, \cdots, \mathbf{x_N}\)が入力される。</p>
            <p>位置符号同士の内積の値は、位置が離れるにつれて減衰していく。</p>
            <p>位置符号のこの特徴は、近いトークン同士の方が遠いトークン同士よりも意味的・文法的な関連度が高くなりやすいという言語の特性を学習するのに役立つと考えられる。</p>
            <p>位置符号は、位置に対して一意に埋め込みが決まるため、位置符号に関連する訓練時に学習されるパラメータはない。</p>
            <p>位置符号を使わずに、単純に位置を表す埋め込みである位置埋め込み（position Embedding）をモデルのパラメータとして学習する方法も頻繁に用いられる。</p>
            <p>Transformerの最大トークン長をKとおくと、位置埋め込みは、K×D次元の行列\(\mathbf{P} = [\mathbf{p_1}, \mathbf{p_2}, \cdots, \mathbf{p_K}]^T\)で表すことができる。</p>
            <h3>自己注意機構</h3>
            <p>入力トークン埋め込みに対して文脈の情報を付与していくために、トークンの重要度を加味しながら文脈化を担う仕組みが自己注意機構（self-attention）である。</p>
            <p>この機構は、埋め込み列を入力として受け取り、それらを相互に参照して、新しい埋め込み列を計算する。</p>
            <p>Transformerで採用されているキー・クエリ・バリュー注意機構（key-query-value attention mechanism）では、入力された埋め込みに対して、キー、クエリ、バリューの3つの異なる埋め込みを計算する。</p>
            <p>この注意機構では、キーとバリューを1対1で紐づける形式のデータ構造が採用されている。</p>
            <p>このデータ構造のキーとクエリを比較することで、ソフトな連想配列のような仕組みを実現している。</p>
            <p>クエリ埋め込みとすべてのキー埋め込みの関連性を測るスコアが計算され、そのスコアをもとに、バリュー埋め込み全体が重み付きで足しあわされて、出力が計算される。</p>
            <p>この仕組みによって、入力全体を考慮しつつ、関連性の高いトークンから優先的に情報を取得できるようになっている。</p>
            <p>クエリ埋め込み、キー埋め込み、バリュー埋め込みを計算するための3つのD×D次元の行列\(\mathbf{W_q}\)、\(\mathbf{W_k}\)、\(\mathbf{W_v}\)が含まれる。</p>
            <p>これらの行列を訓練時に学習することで、重要度を加味した文脈化ができるようになる。</p>
            <p>ここで、D次元の埋め込み列\(\mathbf{h_1}, \mathbf{h_2}, \cdots, \mathbf{h_N}\)に対して、自己注意機構を適用することを考えると、\(\mathbf{h_i}\)に対するクエリ埋め込み\(\mathbf{q_i}\)、キー埋め込み\(\mathbf{k_i}\)、バリュー埋め込み\(\mathbf{v_i}\)は次のように計算される。</p>
            <div class="scroll">
                \begin{align}
                \mathbf{q_i} &= \mathbf{W_q} \mathbf{h_i} \\
                \mathbf{k_i} &= \mathbf{W_k} \mathbf{h_i} \\
                \mathbf{v_i} &= \mathbf{W_v} \mathbf{h_i} 
                \end{align}
            </div>
            <p>i番目のトークンから見たj番目のトークンの関連性スコア\(s_{ij}\)は、内積を用いて以下のように計算される。</p>
            <div class="scroll">
                \begin{align}
                s_{ij} = \frac{\mathbf{q_i}^T \mathbf{k_j}}{\sqrt{D}}
                \end{align}
            </div>
            <p>ここで、分母の\(\sqrt{D}\)は、次元Dが増えるのにともなって、内積の絶対値が大きな値になりすぎることを防ぐことで、訓練を安定化させるために導入されている。</p>
            <p>出力埋め込み\(\mathbf{o_i}\)は、関連性スコア\(s_{ij}\)をソフトマックス関数を用いて正規化した重み\(\alpha_{ij}\)によるバリュー埋め込みの重み付き和になる。</p>
            <div class="scroll">
                \begin{align}
                \alpha_{ij} &= \frac{\exp(s_{ij})}{\sum_{j'=1}^{N} \exp(s_{ij'})} \\
                \mathbf{o_i} &= \sum_{j=1}^{N} \alpha_{ij} \mathbf{v_{j}}
                \end{align}
            </div>
            <p>上式のように、重み\(\alpha_{ij}\)を使ってすべてのトークンのバリュー埋め込みの重み付き和を計算することで、すべてのトークンから重要度を加味しつつ必要な情報を集めて文脈化が行われる。</p>
            <h3>マルチヘッド注意機構</h3>
            <p>Transformerでは、キー・クエリ・バリュー注意機構の表現力をさらに高めるために、この注意機構を同時に複数適用するマルチヘッド注意機構（multi-head attention）が採用されている。</p>
            <p>複数の注意機構を同時に適用することで、複数の観点から文脈化を行うことができる。</p>
            <p>マルチヘッド注意機構では、D次元の埋め込み\(\mathbf{h_i}\)に対して、M個の注意機構を同時に適用する。</p>
            <p>ここで、MはDの約数である必要がある。\(\mathbf{h_i}\)に対応する\(m \in {1, 2, \cdots, M}\)番目の注意機構の埋め込みは以下のように計算される。</p>
            <div class="scroll">
                \begin{align}
                \mathbf{q_i^{(m)}} &= \mathbf{W_q^{(m)}} \mathbf{h_i} \\
                \mathbf{k_i^{(m)}} &= \mathbf{W_k^{(m)}} \mathbf{h_i} \\
                \mathbf{v_i^{(m)}} &= \mathbf{W_v^{(m)}} \mathbf{h_i} 
                \end{align}
            </div>
            <p>\(\mathbf{W_q^{(m)}}, \mathbf{W_k^{(m)}}, \mathbf{W_v^{(m)}}\)は、m番目のヘッド（head）に対応する\(\frac{D}{M} \times D\)の行列であり、したがって\(\mathbf{q_i^{(m)}}, \mathbf{k_i^{(m)}}, \mathbf{v_i^{(m)}}\)は、\(\frac{D}{M}\)次元のベクトルになる。</p>
            <p>このM個のヘッドがそれぞれ異なる観点から文脈化を行う。</p>
            <p>各ヘッドの出力埋め込み\(\mathbf{o_i^{(m)}}\)は、単一のヘッドによる注意機構と同様に以下のように表される。</p>
             <div class="scroll">
                \begin{align}
                s_{ij}^{(m)} &= \frac{\mathbf{q_i}^{(m)T} \mathbf{k_j}^{(m)}}{\sqrt{D/M}} \\
                \alpha_{ij}^{(m)} &= \frac{\exp(s_{ij}^{(m)})}{\sum_{j'=1}^{N} \exp(s_{ij'}^{(m)})} \\
                \mathbf{o_i}^{(m)} &= \sum_{j=1}^{N} \alpha_{ij}^{(m)} \mathbf{v_{j}^{(m)}}
                \end{align}
            </div>
            <p>マルチヘッド注意機構の出力は、M個の出力埋め込みを連結して計算される。</p>
            <div class="scroll">
                \begin{align}
                \mathbf{o_i} = \mathbf{W_o}
                \begin{bmatrix}
                \mathbf{o_i^{(1)}} \\
                \cdots \\
                \mathbf{o_i^{(M)}}
                \end{bmatrix}
                \end{align}
            </div>
            <p>ここで、\(\mathbf{W_o}\)は、D×D次元の行列である。</p>
            <h3>フィードフォワード層</h3>
            <p>フィードフォワード層（feed-forward layer）は、2層の順伝播型ニューラルネットワーク（feed-forward neural network）または多層パーセプトロン（mutilayer perceptron; MLP）と同じ構造である。</p>
            <p>フィードフォワード層への入力ベクトルを\(\mathbf{u_i}\)とすると、出力ベクトル\(\mathbf{z_i}\)は以下の式で求められる。</p>
            <div class="scroll">
                \begin{align}
                \mathbf{z_i} = \mathbf{W_2} f (\mathbf{W_1} \mathbf{u_i} + \mathbf{b_1}) + \mathbf{b_2}
                \end{align}
            </div>
            <p>\(\mathbf{W_1}, \mathbf{W_2}\)は、それぞれ\(D_f \times D\)次元の行列、\(\mathbf{b_1}, \mathbf{b_2}\)は、それぞれ\(D_f, D\)次元のベクトルで、\(f(\cdot)\)は、活性化関数（activation function）である。</p>
            <p>ここで、フィードフォワード層はすべての位置の入力ベクトルを使う注意機構とは異なり、入力された位置のベクトルのみに閉じて計算される。</p>
            <p>活性化関数としては、提案当初のTransformerでは正規化線形ユニット（rectified linear unit; ReLU）が用いられている。</p>
            <p>また、大規模言語モデルでは、正規化線形関数よりも滑らかであり経験的に良い収束性能を発揮するガウス誤差線形ユニット（gaussian error linear unit; GELU）が標準的に用いられている。</p>
            <p>活性化関数は、入力ベクトルの要素単位で適用される。もしTransformerにフィードフォワード層がないと、単に入力を線形変換して重み付きで足し合わせるだけになってしまい、表現力が著しく低くなってしまう。</p>
            <p>提案時のTransformerでは、入力次元D=512に対して、中間層の次元は4倍の\(D_f = 2048\)が使われている。この結果、フィードフォワード層に含まれるパラメータ数は、Transformer全体のパラメータ数の約2/3を占めている。</p>
            <p>フィードフォワード層は、文脈に関する情報をその豊富なパラメータの中に記憶していて、入力された文脈に対して関連する情報を付加する役割を果たしている。</p>
            <h3>残差結合</h3>
            <p>残差結合（residual connection）はTransformerの訓練を安定させるための仕組みである。</p>
            <p>エンコーダ構成のTransformerでは、各ブロックに二つの残差結合を適用した層（マルチヘッド注意機構とフィードフォワード層）がある。</p>
            <p>このため、L個のブロックを含んだモデルでは、入力から2L個の残差結合を適用した層を通って出力が計算される。</p>
            <p>ここで、\(k \in {1, 2, \cdots, 2L}\)に対して、\(k\)番目の残差結合を適用する前の元の層の処理を\(\mathcal{F}^{(k)} (\mathbf{X})\)、層への入力ベクトル列を行列表記して\(\mathbf{X^{(k)}} = [\mathbf{x_1^{(k)}}, \mathbf{x_2^{(k)}}, \cdots, \mathbf{x_N^{(k)}},]^T\)とする。</p>
            <p>残差結合を適用した層の出力（k+1番目の層の入力）\(\mathbf{X^{(k+1)}}\)は元々の層の出力\(\mathcal{F}^{(k)} (\mathbf{X}^{(k)})\)に入力\(\mathbf{X^{(k)}}\)を加算して計算される。</p>
            <div class="scroll">
                \begin{align}
                \mathbf{X^{(k+1)}} = \mathcal{F}^{(k)} (\mathbf{X}^{(k)}) + \mathbf{X^{(k)}}
                \end{align}
            </div>
            <p>この式を変形すると、以下のようになる。</p>
            <div class="scroll">
                \begin{align}
                \mathcal{F}^{(k)} (\mathbf{X}^{(k)}) = \mathbf{X^{(k+1)}} - \mathbf{X^{(k)}}
                \end{align}
            </div>
            <p>この式から、\(\mathcal{F}^{(k)} (\mathbf{X}^{(k)})\)は出力と入力の残差のみを捉えればよくなることが分かる。これが残差結合という名前の由来である。</p>
            <p>この式を使ってエンコーダ構成のTransformerの出力\(\mathbf{X^{(2L+1)}}\)を展開すると、以下のようになる。</p>
            <div class="scroll">
                \begin{align}
                \mathbf{X^{(2L+1)}} = \mathbf{X^{(1)}} + \mathcal{F}^{(1)} (\mathbf{X}^{(1)}) + \mathcal{F}^{(2)} (\mathbf{X}^{(2)}) + \cdots + \mathcal{F}^{(2L)} (\mathbf{X}^{(2L)})
                \end{align}
            </div>
            <p>Transformerの出力は、入力埋め込みと2L個の層の出力を足し合わせたものである。</p>
            <p>入力埋め込み\(\mathbf{X^{(1)}}\)に対して、マルチヘッド注意機構とフィードフォワード層を通じて、静的な埋め込みでは捉えられない文脈情報を順に付与していくことで、文脈化トークン埋め込みが計算される。</p>
            <p>また、損失の計算に使われる出力に対して、構成要素を直接接続する構造をとることで、深層ニューラルネットワークにおける勾配消失・爆発問題を防ぎ、訓練を安定化させている。</p>
            <h3>層正規化</h3>
            <p>層正規化（layer normalization）は、過剰に大きい値によって訓練が不安定になることを防ぐために、ベクトルの値を正規化する仕組みである。</p>
            <p>まず、層正規化へのD次元の入力ベクトルを\(\mathbf{x}\)とおき、ベクトルの要素の平均\(\mu_\mathbf{x}\)と標準偏差\(\sigma_\mathbf{x}\)を求める。</p>
            <div class="scroll">
                \begin{align}
                \mu_\mathbf{x} &= \frac{1}{D} \sum_{i=1}^{D} x_i \\
                \sigma_\mathbf{x} &= \sqrt{\frac{1}{D} \sum_{i=1}^{D} (x_i - \mu_\mathbf{x})^2}
                \end{align}
            </div>
            <p>層正規化関数layernorm(\(\mathbf{x}\))のk番目の要素は、以下のように求められる。</p>
            <div class="scroll">
                \begin{align}
                layernorm(\mathbf{x})_k = g_k \frac{x_k - \mu_{\mathbf{x}}}{\sigma_{\mathbf{x}} + \epsilon} + b_k
                \end{align}
            </div>
            <p>\(g_k\)と\(b_k\)は、ゲインベクトル\(\mathbf{g}\)とバイアスベクトル\(\mathbf{b}\)のk番目の要素である。</p>
            <p>この二つのベクトルは、層正規化の表現力を向上するために導入されているが、\(\mathbf{g} = 1, \mathbf{b} = 0\)のようにすることで無効にすることもできる。</p>
            <h3>ドロップアウト</h3>
            <p>ドロップアウト（dropout）は、訓練データセットに対してモデルが過適合することを防ぐための正則化の仕組みである。</p>
            <p>ドロップアウトは、訓練中にモデルが特定の特徴に依存しすぎないようにするために、確率\(1 - p_{keep}\)で入力されたベクトルの要素を欠落（0に設定）させる。</p>
            <p>ドロップアウトの挙動は訓練時と推論時で異なる。</p>
            <p>D次元の入力ベクトル\(\mathbf{x}\)に対してドロップアウトを適用することを考える。</p>
            <p>訓練時のドロップアウトの出力\(\hat{\mathbf{x}}^{train}\)のj番目の要素は確率\(p_{keep}\)で１、確率\(1 - p_{keep}\)で0を出力するベルヌーイ分布Bernoulli(\(p_{keep}\))から値\(r_j\)をサンプリングすることで以下のように計算される。</p>
            <div class="scroll">
                \begin{align}
                r_j &\sim Bernoulli(p_{keep})
                \hat{x}_j^{train} &= r_j x_j
                \end{align}
            </div>
            <p>推論時には値を欠落せずにすべての特徴をモデルが利用できるようにする。</p>
            <p>また、訓練時の出力のスケールに近づけるため、入力を\(p_{keep}\)倍して\(\hat{\mathbf{x}}^{infer}\)を計算する。</p>
            <div class="scroll">
                \begin{align}
                \hat{\mathbf{x}}^{infer} = p_{keep} \mathbf{x}
                \end{align}
            </div>
            <p>ベクトル[1.0, 2.0, 3.0, 4.0, 5.0]に対して\(p_{keep} = 0.8\)でドロップアウトを適用すると、訓練時の出力は各要素が20%の確率で0に設定されて、[1.0, 0.0, 3.0, 4.0, 5.0]のようになり、推論時の出力は入力ベクトルが0.8倍されて[0.8, 1.6, 2.4, 3.2, 4.0]のようになる。</p>
            <p>Transformerでは、以下の場所にドロップアウトが適用されている。</p>
            <ul>
                <li>入力埋め込み</li>
                <li>マルチヘッド注意機構の重み</li>
                <li>残差結合による加算を適用する前のマルチヘッド注意機構の出力</li>
                <li>残差結合による加算を適用する前のフィードフォワード層の出力</li>
            </ul>
            <h3>交差注意機構</h3>
            <p>デコーダのブロックに含まれる二つの注意機構の1つ目は、自己注意機構、2つ目は、交差注意機構（cross-attention）である。</p>
            <p>自己注意機構と同じく、交差注意機構でもキー・クエリ・バリュー注意機構が使われる。</p>
            <p>原言語の文「こたつでみかんをたべる」の翻訳において、「I eat a mandarin at the」の6トークンを既に生成し、「katatsu」予測することを考える。</p>
            <p>このとき、デコーダは、既に生成した目的言語の情報「I eat a mandarin at the」を自己注意機構、原言語の情報「こたつでみかんを食べる」を交差注意機構を使って取得して、次のトークンを予測する。</p>
            <p>交差注意機構のクエリには、デコーダの埋め込み列、キーとバリューには、エンコーダの出力埋め込み列が使われる。</p>
            <p>D次元のデコーダの埋め込み列\(\mathbf{h_1}, \mathbf{h_2}, \cdots, \mathbf{h_N}\)とエンコーダの出力埋め込み列\(\mathbf{z_1}, \mathbf{z_2}, \cdots, \mathbf{z_M},\)に対して、交差注意機構を適用することを考える。</p>
            <p>デコーダのトークン列の位置iに対応するクエリ埋め込みは、以下のように計算される。</p>
            <div class="scroll">
                \begin{align}
                \mathbf{q_i} = \mathbf{W_q} \mathbf{h_i}
                \end{align}
            </div>
            <p>エンコーダのトークン列の位置jに対応するキー埋め込み、バリュー埋め込みは以下のように計算される。</p>
            <div class="scroll">
                \begin{align}
                \mathbf{q_i} = \mathbf{W_q} \mathbf{h_i} \\
                \mathbf{k_i} = \mathbf{W_k} \mathbf{z_i} \\
                \mathbf{v_i} = \mathbf{W_v} \mathbf{z_i}
                \end{align}
            </div>
            <p>これ以外の交差注意機構の処理は、自己注意機構と同じである。</p>
            <p>具体的には、デコーダのi番目のトークンから見たエンコーダのj番目のトークンの関連性スコアは\(s_{ij}\)は、以下のように計算される。</p>
            <div class="scroll">
                \begin{align}
                s_{ij} = \frac{\mathbf{q_i}^T \mathbf{k_j}}{\sqrt{D}}
                \end{align}
            </div>
            <p>次に、バリュー埋め込みを重み付きで加算することで、重要度を加味しつつ、エンコーダのトークンの情報を使った文脈化を行う。</p>
            <div class="scroll">
                \begin{align}
                \alpha_{ij} &= \frac{\exp(s_{ij})}{\sum_{j'=1}^{M} \exp(s_{ij'})} \\
                \mathbf{o_i} &= \sum_{j=1}^{M} \alpha_{ij} \mathbf{v_{j}}
                \end{align}
            </div>
            <h3>トークン出力分布の計算</h3>
            <p>原言語のトークン列\(u_1, u_2, \cdots, u_M\)と目的言語の生成済みのトークン列\(w_1, w_2, \cdots, w_i\)に対して、次の目的言語のトークン\(w_{i+1}\)を出力することを考える。</p>
            <p>このとき、\(w_{i+1}\)の確率分布はデコーダの位置iに対応する出力埋め込み\(\mathbf{h_i}\)を使って、以下のように計算される。</p>
            <div class="scroll">
                \begin{align}
                P(w_{i+1} | u_1, u_2, \cdots, u_M, w_1, w_2, \cdots, w_i) = softmax_{w_{i+1}} (\mathbf{E h_{i}})
                \end{align}
            </div>
            <p>ここで、\(\mathbf{E}\)は入力トークン埋め込み行列である。</p>
            <p>上式において\(w_1\)を予測することを考えると、\(w_0\)ｙは存在しないため、予測に必要な\(\mathbf{h_0}\)を計算できないことがわかる。</p>
            <p>機械翻訳のように1トークン目の予測を行う必要がある場合には、<code><s></code>のようなテキストの先頭であることを表す擬似的なトークンを\(w_0\)としてデコーダの入力トークン列に追加する。</p>
            <p>このとき、\(w_0\)は入力トークン列のみに使われ、出力には含まれない。</p>
        </section>
    </main>
</body>
</html>
