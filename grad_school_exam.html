<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【研究ノート】走る作曲家のAIカフェ - 院試</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
          <h2>目次</h2>
          <ul>
            <li><a href="#overview">概要</a></li>
            <li><a href="#strategy">戦略</a></li>
            <li><a href="#basic-math">基礎数学</a></li>
            <li><a href="#info-math">情報数学</a></li>
            <li><a href="#probability-statistics">確率・統計</a></li>
            <li><a href="#algorithms-data-structures">アルゴリズムとデータ構造</a></li>
            <li><a href="#artificial-intelligence">人工知能</a></li>
          </ul>
        </section>
        <section id="overview">
          <h2>概要</h2>
          このページでは、「北海道大学大学院情報科学院修士課程入学試験」（令和6年8月実施）の対策を行います。過去問から傾向を掴み、必要な知識や要点をまとめていきます。
        </section>
        <section id="strategy">
          <h2>戦略</h2>
          院試は以下の内容で構成されます。
          <ul>
            <li>専門科目1: 基礎数学、情報数学、【選択】確率・統計、【選択】情報理論（4問のうち基礎数学と情報数学を含む3問を解答）</li>
            <li>専門科目2: アルゴリズムとデータ構造、人工知能、コンピュータシステム、応用数学（4問のうち2問を選択し解答）</li>
          </ul>
          俗に、専門科目1では「情報理論」が、専門科目2では「アルゴリズムとデータ構造」と「コンピュータシステム」が簡単だと言われています。
          しかし、今後の自分の研究に役立てたいので、専門科目1では「確率・統計」を、専門科目2では「アルゴリズムとデータ構造」と「人工知能」を選択しようと思います。
        </section>
        <section id="basic-math">
          <h2>基礎数学</h2>
            <h3>線形写像</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://www.momoyama-usagi.com/entry/math-linear-algebra13#1-3">うさぎでもわかる線形代数　第13羽　線形写像（後編）　核空間・像空間　線形写像の全射・単射について _ 工業大学生ももやまのうさぎ塾.html</a></p>
            <ul>
                <li>集合\(V\)から集合\(W\)への写像があるとする。</li>
                <ul>
                    <li><b>全射</b>：\(W\)の要素すべてが\(V\)のどれかの要素と対応しているような写像。</li>
                    <li><b>単射</b>：\(W\)のある要素に写すための\(V\)の要素は１つしかない（２つ以上存在しない）写像。</li>
                    <li><b>全単射</b>：全射の条件、単射の条件の両方を満たすような写像。\(V\)の要素と\(W\)の要素は１対１の関係になる。</li>
                </ul>
                <li>\(f(x) = Ax\)を満たす行列\(A\)（表現行列）が正則であることと、\(f\)が全単射であることは同値である。</li>
            </ul>
            <h3>線形独立・線形従属</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://www.momoyama-usagi.com/entry/math-linear-algebra06#google_vignette">うさぎでもわかる線形代数　第06羽　1次独立・1次従属 _ 工業大学生ももやまのうさぎ塾.html</a></p>
            <ul>
                <li>線形独立</li>
                <ul>
                    <li>ベクトルの集合が線形独立であるとは、集合内のどのベクトルも他のベクトルの線形結合として表せないことを意味する。
                        具体的には、ベクトル\(v_{1}, v_{2}, ..., v_{n}\)が線形独立であるとは、次の方程式が成り立つときである：
                        <div class="scroll">
                        \begin{align}
                        c_{1} v_{1} + c_{2} v_{2} + ... + c_{n} v_{n} = 0
                        \end{align}
                        </div>
                    </li>
                    <li>ここで、すべてのスカラー\(c_{i}\) が0である場合に限り成り立つ。つまり、
                        <div class="scroll">
                        \begin{align}
                        c_{1} = c_{2} = ... = c_{n} = 0
                        \end{align}
                        </div>
                    </li>
                </ul>
                <li>線形従属</li>
                <ul>
                    <li>一方、ベクトルの集合が線形従属であるとは、少なくとも一つのベクトルが他のベクトルの線形結合として表せることを意味する。
                        つまり、ベクトル\(v_{1}, v_{2}, ..., v_{n}\)が線形従属であるとは、次の方程式が成り立つときである：
                        <div class="scroll">
                        \begin{align}
                        c_{1} v_{1} + c_{2} v_{2} + ... + c_{n} v_{n} = 0
                        \end{align}
                        </div>
                    </li>
                    <li>ここで、少なくとも一つの\(c_{i}\)が0ではない（つまり、非自明な解が存在する）場合である。</li>
                </ul>
                <li>具体例</li>
                <div class="example">
                    <b>線形独立の例</b>
                    <p>ベクトル\(v_{1} = (1, 0)\) と \(v_{2} = (0, 1)\) は線形独立である。なぜなら、</p>
                        <div class="scroll">
                        \begin{align}
                        c_{1} (1, 0) + c_{2} (0, 1) = (0, 0)
                        \end{align}
                        </div>                  
                    <p>が成り立つのは \(c_{1} = 0\)かつ \(c_{2} = 0\) のときだけだからである。</p>
                </div>
                <div class="example">
                    <b>線形従属の例</b>
                    <p>ベクトル\(v_{1} = (1, 2)\) と \(v_{2} = (2, 4)\)は線形従属である。なぜなら、</p>
                        <div class="scroll">
                        \begin{align}
                        2 (1, 2) - 1 (2, 4) = (0, 0)
                        \end{align}
                        </div>                      
                    <p>という関係があり、非自明な解 \(c_{1} = 2, c_{2} = -1\) が存在するからである。</p>
                </div>                
            </ul>
            <h3>二変数間の停留点</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://pictblog.com/twoparadiffer">【微積分】二変数関数の停留点(極大点・極小点・鞍点).html</a></p>
            <ul>
                <li>二変数関数\(f(x, y)\)の停留点\((a_x, a_y)\)を求めることを考える。</li>
                <li>関数の変化がなくなる点が停留点であるため、停留点では\(x\)方向、\(y\)方向ともにその変化量が0、すなわち
                    <div class="scroll">
                    \begin{align}
                    \frac{\partial f(x,y)}{\partial x}=\frac{\partial f(x,y)}{\partial y}=0
                    \end{align}
                    </div>
                を満たす点が停留点となる。</li>
                <li>停留点\((a_x, a_y)\)を求めたら、次はその種類をヘッセ行列により判定する。</li>
                <li>ヘッセ行列は次の式で表される。
                    <div class="scroll">
                    \begin{align}
                    \mathsf{H}=
                        \begin{pmatrix}\displaystyle{\frac{\partial^{2} f(x,y)}{\partial x^{2}}}&\displaystyle{\frac{\partial^{2} f(x,y)}{\partial x\partial y}} \\ \\ 
                        \displaystyle{\frac{\partial^{2} f(x,y)}{\partial x\partial y}}&\displaystyle{\frac{\partial^{2} f(x,y)}{\partial y^{2}}}\end{pmatrix}
                    \end{align}
                    </div>
                </li>
                <li>ヘッセ行列に停留点の座標\((a_x, a_y)\)を代入する。
                    <div class="scroll">
                    \begin{align}
                    \mathsf{H}=
                        \mathsf{H}_{(a_{x},a_{y})}
                        =\begin{pmatrix}\displaystyle{\left.\frac{\partial^{2} f(x,y)}{\partial x^{2}}\right|_{x=a_{x},y=a_{y}}}
                        &\displaystyle{\left.\frac{\partial^{2} f(x,y)}{\partial x\partial y}\right|_{x=a_{x},y=a_{y}}} \\ \\ 
                        \displaystyle{\left.\frac{\partial^{2} f(x,y)}{\partial x\partial y}\right|_{x=a_{x},y=a_{y}}}
                        &\displaystyle{\left.\frac{\partial^{2} f(x,y)}{\partial y^{2}}\right|_{x=a_{x},y=a_{y}}}\end{pmatrix}
                    \end{align}
                    </div>
                </li>
                <li>この行列の固有値を\(\lambda_{1},\lambda_{2}\)とすると、次のように停留点の種類を判別できる。
                    <div class="scroll">
                    \begin{align}
                    \begin{cases}
                        \lambda_{1}>0,\lambda_{2}>0&\to\,\text{極小点}\\ 
                        \lambda_{1}<0,\lambda_{2}<0&\to\,\text{極大点}\\ 
                        \lambda_{1}\lambda_{2}<0&\to\,\text{鞍点} 
                    \end{cases}
                    \end{align}
                    </div>
                </li>
                <li>具体例</li>
                <div class="example">
                    <b>\( f(x, y) = -3x^2 + 2xy - 4y^3 \)の停留点を調べる。</b>
                    <p>一階偏微分を求める。</p>
                        <div class="scroll">
                        \begin{align}
                        f_x = \frac{\partial f}{\partial x} = -6x + 2y \\
                        f_y = \frac{\partial f}{\partial y} = 2x - 12y^2
                        \end{align}
                        </div>
                    <p>これらの一階偏微分を0に設定して連立方程式を解く。</p>
                        <div class="scroll">
                        \begin{align}
                        -6x + 2y = 0 \quad \Rightarrow \quad y = 3x \\
                        2x - 12(3x)^2 = 0 \quad \Rightarrow \quad 2x - 108x^2 = 0 \quad \Rightarrow \quad x(2 - 108x) = 0
                        \end{align}
                        </div>
                    <p>よって、 \( x = 0 \) または \( x = \frac{1}{54} \) である。</p>
                    <p>それぞれの場合に対応する \( y \) を求める。</p>
                        <div class="scroll">
                        \begin{align}
                        x = 0 \quad \Rightarrow \quad y = 0 \\
                        x = \frac{1}{54} \quad \Rightarrow \quad y = \frac{1}{18}
                        \end{align}
                        </div>
                    <p>停留点は \( (0, 0) \) と \( \left( \frac{1}{54}, \frac{1}{18} \right) \) である。</p>
                    <p>次に、二階偏微分を求める。</p>
                        <div class="scroll">
                        \begin{align}
                        f_{xx} = \frac{\partial^2 f}{\partial x^2} = -6 \\
                        f_{xy} = \frac{\partial^2 f}{\partial x \partial y} = 2 \\
                        f_{yy} = \frac{\partial^2 f}{\partial y^2} = -24y
                        \end{align}
                        </div>
                    <p>ヘッセ行列 \( H \) は以下の通りである：</p>
                        <div class="scroll">
                        \begin{align}
                        H = \begin{bmatrix}
                            f_{xx} & f_{xy} \\
                            f_{xy} & f_{yy}
                        \end{bmatrix}
                        = \begin{bmatrix}
                            -6 & 2 \\
                            2 & -24y
                        \end{bmatrix}
                        \end{align}
                        </div>
                    <p>ヘッセ行列の判別式 \( D \) を求める。</p>
                        <div class="scroll">
                        \begin{align}
                        D = f_{xx} f_{yy} - (f_{xy})^2
                        \end{align}
                        </div>
                    <p>停留点 \( (0, 0) \) の場合、</p>
                        <div class="scroll">
                        \begin{align}
                        f_{xx}(0, 0) = -6, \quad f_{yy}(0, 0) = 0, \quad f_{xy}(0, 0) = 2 \\
                        D(0, 0) = (-6)(0) - (2)^2 = -4
                        \end{align}
                        </div>
                    <p>\( D < 0 \) なので、 \( (0, 0) \) は鞍点である。</p>
                    <p>停留点 \( \left( \frac{1}{54}, \frac{1}{18} \right) \) の場合</p>
                        <div class="scroll">
                        \begin{align}
                        f_{xx}\left( \frac{1}{54}, \frac{1}{18} \right) = -6, \quad f_{yy}\left( \frac{1}{54}, \frac{1}{18} \right) = -\frac{4}{3}, \quad f_{xy}\left( \frac{1}{54}, \frac{1}{18} \right) = 2 \\
                        D\left( \frac{1}{54}, \frac{1}{18} \right) = (-6) \left( -\frac{4}{3} \right) - (2)^2 = 8 - 4 = 4
                        \end{align}
                        </div>
                    <p>\( D > 0 \) かつ \( f_{xx} < 0 \) なので、 \( \left( \frac{1}{54}, \frac{1}{18} \right) \) は極大点である。</p>
                </div>
            </ul>
            <h3>極座標変換</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://www.momoyama-usagi.com/entry/math-analysis25">うさぎでもわかる解析　Part25　極座標変換を用いた2重積分の求め方 _ 工業大学生ももやまのうさぎ塾.html</a></p>
            <ul>
                <li>\(x\)と\(y\)を次のようにおく。
                    <div class="scroll">
                    \begin{align}
                    x = r \cos \theta , \ \ \ y = r \sin \theta \\ \left( r \geqq 0, \ \ 0 \leqq \theta \leqq 2 \pi \right)
                    \end{align}
                    </div>
                </li>
                <li>このとき、ヤコビアンを計算すると以下のようになる。
                    <div class="scroll">
                    \begin{align}
                    J = & \left| \begin{array}{ccc} \frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\ \frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta} \end{array} \right|
                        \\ = & \left| \begin{array}{ccc} \cos \theta & - r \sin \theta \\ \sin \theta & r \cos \theta \end{array} \right|
                        \\ = & \ r \left( \cos^2 \theta + \sin^2 \theta \right)
                        \\ = & \ r
                    \end{align}
                    </div>
                </li>
                <li>よって、\(dxdy = r \ dr d \theta\)となる。</li>
                <li>具体例</li>
                <div class="example">
                    <b>\(\int _{-1}^1\int _{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}e^{\sqrt{x^2+y^2}}dydx\)を計算する。</b>
                    <p>極座標変換を行う。</p>
                        <div class="scroll">
                        \begin{align}
                        \int _{-1}^1\int _{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}e^{\sqrt{x^2+y^2}}dydx = \int _{0}^{2\pi}\int _{0}^{1}e^{r}rdrd\theta
                        \end{align}
                        </div>                  
                    <p>内側の積分をまず計算する。</p>
                        <div class="scroll">
                        \begin{align}
                        \int _{0}^{1}re^{r}dr
                        \end{align}
                        </div>        
                    <p>この積分は部分積分を用いて解く。<br>部分積分の公式： \(\int u dv = uv - \int v du\)</p>
                    <ul>
                        <li>\(u = r \Rightarrow du = dr\)</li>
                        <li>\(dv = e^r dr \Rightarrow v = e^r\)</li>
                    </ul>
                        <div class="scroll">
                        \begin{align}
                        \int r e^r dr = r e^r - \int e^r dr = r e^r - e^r
                        \end{align}
                        </div> 
                    <p>これを\(0\)から\(1\)の範囲で評価する：</p>
                        <div class="scroll">
                        \begin{align}
                        \left[ r e^r - e^r \right]_{0}^{1} = (1 e^1 - e^1) - (0 e^0 - e^0) = (e - e) - (0 - 1) = 1
                        \end{align}
                        </div> 
                    <p>したがって、内側の積分の結果は \(1\) である。</p>
                    <p>次に外側の積分を計算する：</p>
                        <div class="scroll">
                        \begin{align}
                        \int _{0}^{2\pi} 1 d\theta = \int _{0}^{2\pi} d\theta = 2\pi
                        \end{align}
                        </div> 
                    <p>したがって、元の2重積分の結果は：</p>
                        <div class="scroll">
                        \begin{align}
                        \int _{-1}^1\int _{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}e^{\sqrt{x^2+y^2}}dydx = 2\pi
                        \end{align}
                        </div> 
                </div>
            </ul>
            <h3>双曲線関数(cosh, sinh, tanh)</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://manabitimes.jp/math/617">双曲線関数(sinh,cosh,tanh)の意味・性質・楽しい話題まとめ _ 高校数学の美しい物語.html</a></p>
            <ul>
                <li>双曲線関数は、以下のように定義される。</li>
                <ul>
                    <li>\(\cosh x = \frac{e^x+e^{-x}}{2}\)</li>
                    <li>\(\sinh x = \frac{e^x-e^{-x}}{2}\)</li>
                    <li>\(\tanh x = \frac{\sinh x}{\cosh x} = \frac{e^x-e^{-x}}{e^x+e^{-x}}\)</li>
                </ul>
                <li>双曲線関数について、以下の関係式が成り立つ。</li>
                <ul>
                    <li>\(\cosh^2 x - \sinh^2 x = 1\)</li>
                    <li>\(1 - \tanh^2 x = \frac{1}{\cosh^2 x}\)</li>
                    <li>\(1 - \frac{1}{\tanh^2 x} = -\frac{1}{\sinh^2 x}\)</li>
                </ul>
                <li>双曲線関数を微分すると、以下のようになる。</li>
                <ul>
                    <li>\((\cosh x)'=\sinh x\)</li>
                    <li>\((\sinh x)'=\cosh x\)</li>
                    <li>\((\tanh x)'=\frac{1}{\cosh^2 x}\)</li>
                </ul>
                <li>双曲線関数を積分すると、以下のようになる。</li>
                <ul>
                    <li>\(\int \cosh x \, dx = \sinh x + C\)</li>
                    <li>\(\int \sinh x \, dx = \cosh x + C\)</li>
                    <li>\(\int \tanh x \, dx = log( \cosh x) + C\)</li>
                </ul>
                <li>具体例</li>
                <div class="example">
                    <p>開区間 \(I := [-1, 1]\) で定義された、無限回微分可能な実数値関数全体からなる集合 \(C^∞(I)\) は、
                    通常の関数の和と定数倍に関して線形空間（ベクトル空間）をなす。また、任意の \(f(x), g(x) ∈ C^∞(I)\) に対して</p>
                    <p>
                        <div class="scroll">
                        \begin{align}
                        \langle f(x), g(x) \rangle := \int_{-1}^{1} f(x) g(x) \, dx
                        \end{align}
                        </div>
                        と定義すると、\(\langle f(x), g(x) \rangle\) は \(C^{\infty}(I)\) の内積となる。
                        
                        自然対数の底を \(e\) とし、
                        <div class="scroll">
                        \begin{align}
                        \cosh x := \frac{e^x + e^{-x}}{2}, \quad \sinh x := \frac{e^x - e^{-x}}{2}
                        \end{align}
                        </div>
                        という二つの関数を定義する。
                        更に、上で定義された関数の集合 \(F(I)\) を
                    </p>               
                    <p>
                        <div class="scroll">
                        \begin{align}
                        F(I) := \{a \cosh x + b \sinh x | a, b ∈ \mathbb{R}\}
                        \end{align}
                        </div><br>
                        によって定義する。ここで、\(\mathbb{R}\) は実数全体からなる集合を表す。以下の設問に答えよ。
                    </p>
                
                    <ol>
                        <li><b>\(F(I)\) が \(C^∞(I)\) の部分集合となることを証明せよ。</b></li>
                        <ul>
                            <li>\(F(I)\) が \(C^∞(I)\) の部分集合であることを証明するには、すべての\(x\)について、\(x \in F(I)\)ならば\(x \in C^∞(I)\)であることを示せばよい。</li>
                            <li>したがって、\( F(I) \) の任意の要素\(x\)が無限回微分可能であることを示す。具体的には、\( F(I) \) の任意の要素は次の形をしている：
                            <div class="scroll">\[ f(x) = a \cosh x + b \sinh x \]</div></li>
                            <li>ここで、\( a \) および \( b \) は実数定数である。この関数の導関数は、次のように計算される：
                            <div class="scroll">\[ f'(x) = a \sinh x + b \cosh x \]</div></li>
                            <li>さらに、二階導関数は：
                            <div class="scroll">\[ f''(x) = a \cosh x + b \sinh x \]</div></li>
                            <li>つまり、\( f(x) \) の任意の階の導関数は再び \( a \cosh x + b \sinh x \) の形になる。したがって、\( f(x) \) は無限回微分可能であり、\( C^\infty(I) \) に属する。</li>
                            <li>よって、\( F(I) \subseteq C^\infty(I) \) が成り立つことが示された。</li>
                        </ul>
                        <li><b>\(F(I)\) が \(C^∞(I)\) の線形部分空間となることを証明せよ。</b></li>
                        <ul>
                            <li>線形部分空間であるためには、以下の条件を満たす必要がある：</li>
                            <ol>
                                <li>関数の加法が閉じていること。</li>
                                <li>関数のスカラー倍が閉じていること。</li>
                            </ol>
                            <li>まず、任意の \( f_1(x), f_2(x) \in F(I) \) を取り、それぞれ次のように表す：
                            <div class="scroll">\[ f_1(x) = a_1 \cosh x + b_1 \sinh x \]</div>
                            <div class="scroll">\[ f_2(x) = a_2 \cosh x + b_2 \sinh x \]</div></li>
                            <li>これらの和を計算する：
                            <div class="scroll">\begin{align} f_1(x) + f_2(x) &= (a_1 \cosh x + b_1 \sinh x) + (a_2 \cosh x + b_2 \sinh x) \\
                            &= (a_1 + a_2) \cosh x + (b_1 + b_2) \sinh x \end{align}</div></li>
                            <li>ここで、\( a_1 + a_2 \) および \( b_1 + b_2 \) も実数であるため、\( f_1(x) + f_2(x) \in F(I) \) であることがわかる。したがって、加法は閉じている。</li>
                            <li>次に、任意の \( f(x) \in F(I) \) とスカラー \( c \in \mathbb{R} \) を取り、それぞれ次のように表す：
                            <div class="scroll">\[ f(x) = a \cosh x + b \sinh x \]</div></li>
                            <li>これのスカラー倍を計算する：</li>
                            <div class="scroll">\begin{align} c f(x) &= c (a \cosh x + b \sinh x)  \\
                            &= (ca) \cosh x + (cb) \sinh x \end{align}</div></li>
                            <li>ここで、\( ca \) および \( cb \) も実数であるため、\( c f(x) \in F(I) \) であることがわかる。したがって、スカラー倍も閉じている。</li>
                            <li>これにより、\( F(I) \) が \( C^\infty(I) \) の線形部分空間であることが示された。</li>
                        </ul>
                        <li><b>\(\cosh x\)と\(\sinh x\)が、上で定義した内積に関して直交することを証明せよ。</b></li>
                        <ul>
                            <li>内積を計算する：
                                <div class="scroll">\begin{align} \langle \cosh x, \sinh x \rangle = \int_{-1}^{1} \cosh(x) \sinh(x) \, dx \end{align}</div>
                            </li>
                            <li>定義により、\( \cosh(x) = \frac{e^x + e^{-x}}{2} \)、\( \sinh(x) = \frac{e^x - e^{-x}}{2} \) である。これを使って積を計算する。
                                <div class="scroll">\begin{align} \cosh(x) \sinh(x) = \left(\frac{e^x + e^{-x}}{2}\right)\left(\frac{e^x - e^{-x}}{2}\right) = \frac{e^{2x} - e^{-2x}}{4} \end{align}</div>
                            </li>
                            <li>これを内積の定義に従って積分する。
                                <div class="scroll">\begin{align} \langle \cosh x, \sinh x \rangle = \int_{-1}^{1} \frac{e^{2x} - e^{-2x}}{4} \, dx \end{align}</div>
                            </li>
                            <li>積分を計算する。
                                <div class="scroll">\begin{align}
                                \frac{1}{4} \int_{-1}^{1} e^{2x} \, dx - \frac{1}{4} \int_{-1}^{1} e^{-2x} \, dx 
                                &= \frac{1}{4} \left[ \frac{e^{2x}}{2} \right]_{-1}^{1} - \frac{1}{4} \left[ -\frac{e^{-2x}}{2} \right]_{-1}^{1} \\
                                &= \frac{1}{4} \left( \frac{e^2 - e^{-2}}{2} \right) - \frac{1}{4} \left( \frac{e^{-2} - e^2}{2} \right) \\
                                &= \frac{1}{8} (e^2 - e^{-2}) - \frac{1}{8} (e^{-2} - e^2) \\
                                &= \frac{1}{8} (e^2 - e^{-2} - e^{-2} + e^2) = \frac{1}{8} (2e^2 - 2e^{-2}) = 0 \end{align}</div>
                            </li>
                            <li>したがって、\( \cosh x \) と\( \sinh x \) は直交することが示された。</li>
                        </ul>
                        <li><b>関数の組\(\{\cosh x, \sinh x\}\)が\(F(I)\)の基底となることを証明せよ。</b></li>
                        <ul>
                            <li>関数の組 \( \{ \cosh x, \sinh x \} \) が基底であるための条件：
                                <ol>
                                    <li>一次独立であること。</li>
                                    <li>\( F(I) \) を生成すること。</li>
                                </ol>
                            </li>
                    
                            <li>1. 一次独立性の証明
                                <ul>
                                    <li>関数 \( \cosh x \)と \( \sinh x \)が一次独立であることを示すには、任意の定数 \( a \)および\( b \)に対して次の関係式が成り立つ場合に\( a = 0 \) および\( b = 0 \) であることを証明する：
                                        <div class="scroll">\begin{align} a \cosh x + b \sinh x = 0 \end{align}</div>
                                    </li>
                                    <li>この式は全ての \( x \) に対して成り立つと仮定する。特に、\( x = 0 \)のとき、
                                        \( \cosh(0) = 1 \) および \( \sinh(0) = 0 \) なので、
                                        <div class="scroll">\begin{align} a \cosh(0) + b \sinh(0) = a \cdot 1 + b \cdot 0 = a = 0 \end{align}</div>
                                    </li>
                                    <li>次に、\( x = 1 \) のとき、
                                        \( \cosh(1) \) および \( \sinh(1) \) なので、
                                        <div class="scroll">\begin{align} a \cosh(1) + b \sinh(1) = 0 \end{align}</div>
                                    </li>
                                    <li>既に \( a = 0 \) であることを示しているので、残りの式は次のようになる：
                                        <div class="scroll">\begin{align} b \sinh(1) = 0 \end{align}</div>
                                    </li>
                                    <li>ここで、\( \sinh(1) \neq 0 \) であるため、
                                        <div class="scroll">\begin{align} b = 0 \end{align}</div>
                                    </li>
                                    <li>したがって、\( \cosh x \) と \( \sinh x \) は一次独立である。</li>
                                </ul>
                            </li>
                    
                            <li>2. 生成性の証明
                                <ul>
                                    <li>\( F(I) \) が \( \{ \cosh x, \sinh x \} \) によって生成されることを示すには、任意の関数 \( f(x) \in F(I) \) が \( \cosh x \) および \( \sinh x \) の線形結合として表現できることを示す必要がある。</li>
                                    <li>\( F(I) \) の定義によれば、任意の関数 \( f(x) \in F(I) \) は次の形式で表現できる：
                                        <div class="scroll">\begin{align} f(x) = a \cosh x + b \sinh x \end{align}</div>
                                        ここで、\( a \) および \( b \) は実数である。
                                    </li>
                                    <li>任意の \( f(x) \in F(I) \) を取ると、\( f(x) \) は常にこの形式で表現できる。これにより、\( \{ \cosh x, \sinh x \} \) が \( F(I) \) を生成することが示された。</li>
                                </ul>
                            </li>
                        </ul>
                        <li><b>微分が、\(F(I)\)から\(F(I)\)への線形写像となることを証明せよ。</b></li>
                        <ul>
                            <li>微分が線形写像であるためには、次の2つの条件を満たす必要がある。</li>
                            <ol>
                                <li>線形性の保持：
                                    <div class="scroll">\begin{align} D(af + bg) = aD(f) + bD(g)\end{align}</div>
                                    ここで、\( f, g \in F(I) \) であり、\( a, b \in \mathbb{R} \) である。
                                </li>
                                <li>関数 \( f(x) = a \cosh x + b \sinh x \in F(I) \) に対する微分が再び \( F(I) \) に属すること。</li>
                            </ol>
                            <li>まず、線形性の保持を確認する。
                                <br>
                                <div class="scroll">\( D(af + bg) = D(a \cosh x + b \sinh x) = aD(\cosh x) + bD(\sinh x) \)</div>
                            </li>
                            <li>次に、微分を計算する。
                                <div class="scroll">\begin{align}
                                    D(\cosh x) = \sinh x \\
                                    D(\sinh x) = \cosh x \end{align}</div>
                            </li>
                            <li>したがって、
                                <div class="scroll">\begin{align} D(a \cosh x + b \sinh x) = a \sinh x + b \cosh x \in F(I) \end{align}</div>
                            </li>
                            <li>以上より、微分が \( F(I) \)から \( F(I) \) への線形写像であることが示された。</li>
                        </ul>
                    </ol>
                </div>
            </ul>
        </section>
        <section id="info-math">
          <h2>情報数学</h2>
            <h3>形式言語</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://xtech.nikkei.com/it/article/COLUMN/20070618/275020/">第2回　形式言語とオートマトン---「文」のルールを知り，機械に解釈させる _ 日経クロステック（xTECH）.html</a></p>
            <ul>
                <li><b>正規言語 (Regular Language)</b></li>
                <ul>
                    <li>正規言語は、有限オートマトン（有限状態機械）によって認識される言語である。</li>
                    <li>正規文法によって生成される。</li>
                    <li>決定性有限オートマトン（DFA）や非決定性有限オートマトン（NFA）で認識可能である。</li>
                    <li>\(a \rightarrow b\)において、\(b\)が終端記号、または終端記号の後に非終端記号を付ける。</li>
                    <li>例：\(L = { a^n | n ≥ 0 }\)(空文字列または任意の個数の \(a\) からなる言語。)</li>
                    <li>例：\(L = (ab)^*\)(\(ab\) の繰り返しからなる言語。)</li>
                </ul>
                <li><b>文脈自由言語 (Context-Free Language, CFL)</b></li>
                <ul>
                    <li>文脈自由言語は、文脈自由文法（CFG）によって生成される言語である。</li>
                    <li>文脈自由文法は、1つの非終端記号が文字列に置き換えられる生成規則を持つ。</li>
                    <li>プッシュダウンオートマトン（PDA）で認識可能である。</li>
                    <li>スタックを使用して構造を保持するため、より複雑な構造を認識できる。</li>
                    <li>\(a \rightarrow b\)において、\(a\)が非終端記号１つだけから成る。</li>
                    <li>例：\(L = { a^n b^n | n ≥ 0 }\)(同じ数の \(a\) と \(b\) からなる言語。)</li>
                    <li>例：\(ww^R | w ∈ {a, b}^*\)(任意の文字列 \(w\) とその逆順 \(w^R\) からなる言語。)</li>
                </ul>
                <li><b>文脈依存言語 (Context-Sensitive Language, CSL)</b></li>
                <ul>
                    <li>文脈依存言語は、文脈依存文法（CSG）によって生成される言語である。</li>
                    <li>文脈依存文法の生成規則は、形式 \(α A β → α γ β\) を持ち、ここで \(A\) は非終端記号、\(α\)、\(β\)、および \(γ\) は任意の文字列である。</li>
                    <li>線形有界オートマトン（LBA）で認識可能である。</li>
                    <li>メモリが有限で、入力の長さに比例して制約される。</li>
                    <li>\(a \rightarrow b\)において、\(b\)の長さが\(a\)の長さ以上である。</li>
                    <li>例：\(L = { a^n b^n c^n | n ≥ 1 }\)(同じ数の \(a, b, c\) からなる言語。)</li>
                </ul>
                <li><b>クリーネ閉包</b></li>
                <ul>
                    <li>アルファベット \(Σ = {a, b}\) に対して、そのクリーネ閉包（Kleene closure）、またはクリーネスター（Kleene star）とは、\(Σ\) から生成されるすべての有限長の文字列の集合を指す。</li>
                    <li>記号としては \(Σ*\) と表現される。</li>
                    <li>具体的には、\(Σ*\) は以下のようなすべての文字列を含む集合である：
                        <ul>
                            <li>空文字列\(ε\)</li>
                            <li>\(a\) と \(b\) の任意の組み合わせ</li>
                            <li>文字列の長さはゼロ以上</li>
                        </ul>
                    </li>
                    <li>つまり、\(Σ*\) は次のような集合である：
                        <ul>
                            <li>\(ε\)（空文字列）</li>
                            <li>長さ1の文字列： \(a, b\)</li>
                            <li>長さ2の文字列： \(aa, ab, ba, bb\)</li>
                            <li>長さ3の文字列： \(aaa, aab, aba, abb, baa, ...\)</li>
                            <li>など、任意の有限長のすべての組み合わせ</li>
                        </ul>
                    </li>
                </ul>
                <li><b>ポンピング定理</b></li>
                <ul>
                    <li>任意の正規言語 \(L\) について、あるポンピング長 \(p\)（自然数）が存在して、\(L\) のすべての文字列 \(s\) が以下の条件を満たす場合に限り、\(s \in L\) である：
                        <ul>
                            <li>\(s\) の長さは \(p\) 以上である。</li>
                            <li>\(s\) は \(s = xyz\) という形に分割できる。</li>
                            <li>\(|xy| \leq p\)。</li>
                            <li>\(y \neq \epsilon\)。</li>
                            <li>任意の非負整数 \(i\) に対して、文字列 \(xy^iz \in L\) である。</li>
                        </ul>
                    </li>
                    <li>具体例</li>
                    <div class="example">
                        <b>言語 \(L = \{0^n 1^n \mid n \geq 0\}\) が正規言語でないことをポンピング定理を使って証明する。</b>
                        <p>仮に、言語 \(L = \{0^n 1^n \mid n \geq 0\}\) が正規言語であると仮定する。すると、ポンピング長 \(p\) が存在するはずである。</p>
                        <ol>
                            <li>\(L\) から \(s\) を選ぶ。ここで、\(s = 0^p 1^p\) とする。この文字列 \(s\) の長さは \(2p\) で、ポンピング補題の条件を満たす。</li>
                            <li>ポンピング補題に従い、\(s\) を \(s = xyz\) の形に分割する。ここで、\(|xy| \leq p\) であり、\(y \neq \epsilon\) である。</li>
                        </ol>
                        <p>\(|xy| \leq p\) なので、\(x\) と \(y\) はすべて \(0\) から構成される。具体的には、\(x = 0^a\)、\(y = 0^b\)、\(z = 0^{p-a-b} 1^p\) という形になる。ここで、\(a\) と \(b\) は非負整数であり、\(a + b \leq p\)、かつ \(b > 0\) である。</p>
                    
                        <ol start="3">
                            <li>ポンピング補題により、任意の非負整数 \(i\) に対して、文字列 \(xy^iz\) が \(L\) に属するはずである。</li>
                        </ol>
                        <p>しかし、\(i \neq 1\) の場合を考える。</p>
                        <ul>
                            <li>\(i = 0\) のとき、
                                <div class="scroll">
                                \(xy^0z = xz = 0^a 0^{p-a-b} 1^p = 0^{p-b} 1^p\)
                                </div>
                            である。ここで、\(b > 0\) なので、この文字列は \(0\) の数と \(1\) の数が一致せず、\(L\) に属さない。</li>
                            <li>\(i = 2\) のとき、
                                <div class="scroll">
                                \(xy^2z = xyyz = 0^a 0^{2b} 0^{p-a-b} 1^p = 0^{p+b} 1^p\)
                                </div>
                            である。ここでも、\(b > 0\) なので、この文字列も \(0\) の数と \(1\) の数が一致せず、\(L\) に属さない。</li>
                        </ul>
                        <p>これらの例から、\(xy^iz\) が \(L\) に属さない \(i\) が存在することが示される。したがって、ポンピング補題の条件を満たさないため、仮定に矛盾する。</p>
                        <p>したがって、言語 \(L = \{0^n 1^n \mid n \geq 0\}\) は正規言語ではないことが証明された。</p>
                    </div>
                </ul>
            </ul>
        </section>
        <section id="probability-statistics">
          <h2>確率・統計</h2>
            <p>参考にした本は以下のとおり。</p>
            <p><a href="https://www.amazon.co.jp/%E6%94%B9%E8%A8%82%E7%89%88-%E6%97%A5%E6%9C%AC%E7%B5%B1%E8%A8%88%E5%AD%A6%E4%BC%9A%E5%85%AC%E5%BC%8F%E8%AA%8D%E5%AE%9A-%E7%B5%B1%E8%A8%88%E6%A4%9C%E5%AE%9A2%E7%B4%9A%E5%AF%BE%E5%BF%9C-%E7%B5%B1%E8%A8%88%E5%AD%A6%E5%9F%BA%E7%A4%8E-%E7%94%B0%E4%B8%AD%E8%B1%8A/dp/4489022271/ref=sr_1_3?adgrpid=118645167349&dib=eyJ2IjoiMSJ9.tqheIVwzNsAlWRtDMXpk4NQuS-Y3KADO2zuJVrYBspygQ-wuTir-4Zh8ZccMsf6hif_83PZ9kySPkkoITDdWUa6ipD1MaPRi0Pvb8Fsxyoen0-IcBaseJIM5lZbG_2f-Bmy9i4NWn4GJlyD2nWgtvjYngUr26gaISrg2sSB7CBeIDGkgl9iHQpriMgPZCYevDn16ax_7dg_j2YfDOTEsEUUnGkaOOcuYJtUGSL1REs-viAjaz9CfdZaetJeMn8edfB5RF27GSp4DK5Im-q-9yKoe3FTymhvb6z2cCOblnC0.AYS8Y7i56awkZav04EPeM51WBXaA4QbTF7kHfUD_MQE&dib_tag=se&hvadid=666001842902&hvdev=c&hvqmt=b&hvtargid=kwd-536488505477&hydadcr=27299_14701689&jp-ad-ap=0&keywords=%E7%B5%B1%E8%A8%88%E6%A4%9C%E5%AE%9A2%E7%B4%9A&qid=1722681801&sr=8-3">改訂版　日本統計学会公式認定　統計検定２級対応　統計学基礎</a></p>
            <h3>チェビシェフの不等式</h3>
            <ul>
                <li>期待値\(\mu\)、分散\(\sigma\)をもつ確率分布に従う確率変数\(X\)があるとする。</li>
                <li>このとき、任意の\(k\)に対して次の不等式が成り立つ。
                    <div class="scroll">
                    \begin{align}
                    P(|X-\mu| \geq k\sigma) \leq 1/k^2
                    \end{align}
                    </div>
                </li>
                <li>\(\epsilon = k\sigma\)とおくと、次の式に変形できる。
                    <div class="scroll">
                    \begin{align}
                    P(|X-\mu|\geq\epsilon)\leq\frac{\sigma^2}{\epsilon^2} \\
                    P(|X-\mu|<\epsilon)\geq\frac{1-\sigma^2}{\epsilon^2}
                    \end{align}
                    </div>
                </li>
            </ul>
            <h3>大数の法則</h3>
            <ul>
                <li>大数の法則は多数回の試行の結果として得られたデータの平均（標本平均）や相対度数が、試行回数\(n\)を大きくするとき、確率分布の平均（母平均）や生起確率に近づくことを保証する理論的根拠となっている定理である。</li>
                <li>\(X_1, X_2, ..., X_n\)が互いに独立に平均\(\mu\)、分散\(\sigma^2\)の同一の確率分布に従うとする。</li>
                <li>このとき、平均\(\overline{X}=(X_1+X_2+...+X_n)/n\)の期待値と分散は\(E[\overline{X}]=\mu, V[\overline{X}]=\frac{\sigma^2}{n}\)となる。</li>
                <li>\(\overline{X}\)の場合についてチェビシェフの不等式を求めると
                    <div class="scroll">
                    \begin{align}
                    P(|\overline{X}-\mu|<\epsilon)\geq\frac{1-\sigma^2}{n\epsilon^2}
                    \end{align}
                    </div>
                となり、\(n\)を大きくするとき次の性質を持つことがいえる。</li>
                <li><b>定理（大数の弱法則）</b></li>
                <ul>
                    <li>\(X_1, X_2, ..., X_n\)が互いに独立に平均\(\mu\)、分散\(\sigma^2\)の同一の確率分布に従うとき、任意の\(\epsilon\)に対して
                        <div class="scroll">
                        \begin{align}
                        \lim_{n \to \infty} P(|\overline{X}-\mu|<\epsilon)=1
                        \end{align}
                        </div>
                    が成り立つ。</li>
                    <li>これを標本平均\(\overline{X}\)は母平均\(\mu\)に確率収束するといい、\(\overline{X} \xrightarrow{P} \mu\)と表す。</li>
                    <li>これより、標本平均\(\overline{X}\)は\(n\)を無限に大きくするとき期待値（母平均）\(\mu\)に近づくことがわかる。</li>
                </ul>
            </ul>
            <h3>一致性</h3>
            <ul>
                <li>ある母数\(\theta\)の推定量\(\hat{\theta}\)が\(\hat{\theta} \xrightarrow{P} \theta\)を満たすことを<b>一致性</b>と呼び、\(\hat{\theta}\)を<b>一致推定量</b>と呼ぶ。</li>
                <li><b>標本平均</b></li>
                <ul>
                    <li>大数の弱法則\(\overline{x} \xrightarrow{P} \mu\)から、平均\(\overline{x}\)は\(n\)が大きくなると母平均に近づく。</li>
                </ul>
                <li><b>標本分散</b></li>
                <ul>
                    <li>標本分散は以下の式で表される。
                        <div class="scroll">
                        \begin{align}
                        s^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2
                        \end{align}
                        </div>
                    </li>
                    <li>この式は、以下のように変形できる。
                        <div class="scroll">
                        \begin{align}
                        s^2 &= \frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2 \\
                            &= \frac{1}{n} \sum_{i=1}^n ((x_i - \mu) - (\overline{x} - \mu))^2 \\
                            &= \frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2 - (\overline{x} - \mu)^2 \\
                        \end{align}
                        </div>
                    </li>
                    <li>この式の第２項は\((\overline{x} - \mu) \xrightarrow{P} 0\)であるため、\((\overline{x} - \mu)^2 \xrightarrow{P} 0\)となる。</li>
                    <li>また、\(\nu_i = (x_i - \mu)^2\)とおくと、\(\nu_i\)は互いに独立で同じ分布に従う確率変数となり、大数の法則を適用できる。</li>
                    <li>これから、第１項は\(\overline{\nu} \xrightarrow{P} \sigma^2\)となり、\(s^2 \xrightarrow{P} \sigma^2\)がいえる。</li>
                </ul>
                <li><b>不偏分散</b></li>
                <ul>
                    <li>不偏分散は以下の式で表される。
                        <div class="scroll">
                        \begin{align}
                        u^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline{x})^2
                        \end{align}
                        </div>
                    </li>
                    <li>\(n \rightarrow \infty\)のとき、無限大に発散する分母について、\(s^2\)と高々１しか差がない\(u^2\)も一致性を持つ。</li>
                </ul>
            </ul>
            <h3>不偏性</h3>
            <ul>
                <li>推定量\(\hat{\theta}\)の分布は母数\(\theta\)によって定められるが、その期待値が\(E[\hat{\theta}] = \theta\)と、常に母数に等しくなる性質を<b>不偏性</b>と呼び、その性質を持つ推定量を<b>不偏推定量</b>と呼ぶ。</li>
                <li>これは一致性と異なり、標本の大きさ\(n\)に依存しない基準である。</li>
                <li>不偏推定量でなければ、推定には<b>偏り</b>があるといい、偏りを\(E[\hat{\theta}] - \theta\)で定義する。</li>
                <li><b>標本平均</b></li>
                <ul>
                    <li>
                        <div class="scroll">
                        \begin{align}
                        E[\overline{x}] = \frac{1}{n} \sum_{i=1}^{n} E[x_i] = \frac{1}{n} \sum_{i=1}^{n} \mu = \mu
                        \end{align}
                        </div>
                    </li>
                </ul>
                <li><b>分散</b></li>
                <ul>
                    <li>偏差平方和を\(T_{xx} = \sum (x_i - \overline{x})^2\)とおくと、
                        <div class="scroll">
                        \begin{align}
                        T_{xx} &= \sum (x_i - \overline{x})^2 \\
                            &= \sum [(x_i - \mu) - (\overline{x} - \mu)]^2 \\
                            &= \sum (x_i - \mu)^2 - n(\overline{x} - \mu)^2 
                        \end{align}
                        </div>
                    となる。</li>
                    <li>ここで期待値を計算すると次の結果が得られる。
                        <div class="scroll">
                        \begin{align}
                        E[T_{xx}] &= \sum E[(x_i - \mu)^2] - n E[(\overline{x} - \mu)^2] \\
                            &= \sum V[x_i] - nV[\overline{x}] \\
                            &= n \sigma^2 - n \frac{\sigma^2}{n} \\
                            &= (n-1) \sigma^2
                        \end{align}
                        </div>
                    </li>
                    <li>したがって、標本分散は不偏性を持たず、不偏分散は不偏性を持つ。</li>
                </ul>
                <h3>条件付き確率とベイズの定理</h3>
                <p>参考にしたサイトは以下のとおり。</p>
                <p><a href="https://mathphysnote.com/bayes-theorem/">条件付き確率とベイズの定理 〜検査結果の陽性が正しい確率を求める〜 - とある数物研究者の覚書.html</a></p>
                <ul>
                    <li>事象\(B\)が起こったときの事象\(A\)が起こる条件付き確率\(P(A|B)\)を以下のように定義する。
                        <div class="scroll">
                        \begin{align}
                        P(A|B) = \frac{P(A \cap B)}{P(B)}
                        \end{align}
                        </div>
                    </li>
                    <li>次のベイズの定理は、\(P(B|A)\)が分かっている状態で\(P(A|B)\)を求めたいときに有効である。
                        <div class="scroll">
                        \begin{align}
                        P(A|B) = \frac{P(A)P(B|A)}{P(B)}
                        \end{align}
                        </div>
                    </li>
                    <li>この式は、条件付き確率の定義を用いて、\(P(A \cap B)\)を二通りで表すことによって導くことができる。</li>
                    <li>具体例</li>
                    <div class="example">
                        <b>検査結果の陽性が正しい確率</b>
                        <p>ある病気の検査結果について、以下の事実が分かっている。</p>
                        <ol>
                            <li>この検査の真陽性率（感度; True positive）は90%である。</li>
                            <li>この検査の真陰性率（特異度; True negative）は80%である。</li>
                            <li>全体の人口の5%がこの病気に罹患している。</li>
                        </ol>
                        <p>検査で陽性と判定された場合、実際に病気に罹患している確率を求めたい。</p>
                        
                        <div class="scroll">
                        <table>
                            <tr>
                                <th colspan="2"></th>
                                <th colspan="2">検査結果</th>
                            </tr>
                            <tr>
                                <th colspan="2"></th>
                                <th>陽性（事象B）</th>
                                <th>陰性</th>
                            </tr>
                            <tr>
                                <th rowspan="2">病気に<br>罹っているか</th>
                                <th>罹っている（事象A）</th>
                                <td>a</td>
                                <td>b</td>
                            </tr>
                            <tr>
                                <th>罹っていない</th>
                                <td>c</td>
                                <td>d</td>
                            </tr>
                        </table>
                        </div>
                        <p>表のように事象\(A, B\)を定めると、求めたい確率は\(P(A|B)\)となる。</p>
                        <p>１つ目の条件より、\(P(B|A) = 0.90\)である。</p>
                        <p>２つ目の条件より、\(P(B^c|A^c) = 0.80\)、すなわち、\(P(B|A^c) = 1.00 - 0.80 = 0.20\)である。</p>
                        <p>３つ目の条件より、\(P(A) = 0.05\)、すなわち、\(P(A^c) = 0.95\)である。</p>
                        <p>また、\(P(B)\)は以下のように展開できる。</p>
                        <div class="scroll">
                        \begin{align}
                        P(B) &= B \cap \Omega \\
                            &= B \cap (A \cup A^c) \\
                            &= (B \cap A) \cup (B \cap A^c)
                        \end{align}
                        </div>
                        <p>この式に、条件付き確率の式\(P(B|A) = \frac{P(B \cap A)}{P(A)}\)を用いると、ベイズの式は以下のように変形できる。</p>
                        <div class="scroll">
                        \begin{align}
                        P(A|B) &= \frac{P(A)P(B|A)}{P(B)} \\
                            &= \frac{P(A)P(B|A)}{P(A)P(B|A) + P(A^c)P(B|A^c)}
                        \end{align}
                        </div>
                        <p>したがって、</p>
                        <div class="scroll">
                        \begin{align}
                        P(A|B) &= \frac{0.05 \times 0.90}{0.05 \times 0.90 + 0.95 \times 0.20} \simeq 0.19
                        \end{align}
                        </div>
                        <p>となる。</p>
                </div>
                </ul>
            </ul>
            <h3>累積分布関数、確率密度関数、期待値、分散</h3>
            <ul>
                <li>具体例</li>
                    <div class="example">
                        <b>連続一様分布の問題</b>
                        <p>確率変数 <strong>X</strong> が区間 <strong>(0, 1)</strong> での連続一様分布に従うとき、以下の小問に答えよ。</p>
                        <ol>
                            <li>
                                <p><strong>X</strong> の累積分布関数、確率密度関数、期待値、分散をそれぞれ答えよ。</p>
                                <ol>
                                    <li><strong>累積分布関数 (CDF)</strong>:</li>
                                    <div class="scroll">
                                        \begin{align}
                                        F_X(x) = \begin{cases} 
                                        0 & \text{if } x < 0 \\
                                        x & \text{if } 0 \le x \le 1 \\
                                        1 & \text{if } x > 1 
                                        \end{cases}
                                        \end{align}
                                    </div>
                                    <li><strong>確率密度関数 (PDF)</strong>:</li>
                                    <div class="scroll">
                                        \begin{align}
                                        f_X(x) = \begin{cases} 
                                        1 & \text{if } 0 \le x \le 1 \\
                                        0 & \text{otherwise}
                                        \end{cases}
                                        \end{align}
                                    </div>
                                    <li><strong>期待値</strong>:</li>
                                    <div class="scroll">
                                        \begin{align}
                                        E[X] = \int_0^1 x \cdot f_X(x) \, dx = \int_0^1 x \, dx = \left[ \frac{x^2}{2} \right]_0^1 = \frac{1}{2}
                                        \end{align}
                                    </div>
                                    <li><strong>分散</strong>:</li>
                                    <div class="scroll">
                                        \begin{align}
                                        \mathrm{Var}(X) = E[X^2] - (E[X])^2
                                        \end{align}
                                    </div>
                                    <div class="scroll">
                                        \begin{align}
                                        E[X^2] = \int_0^1 x^2 \cdot f_X(x) \, dx = \int_0^1 x^2 \, dx = \left[ \frac{x^3}{3} \right]_0^1 = \frac{1}{3}
                                        \end{align}
                                    </div>
                                    <div class="scroll">
                                        \begin{align}
                                        \mathrm{Var}(X) = \frac{1}{3} - \left( \frac{1}{2} \right)^2 = \frac{1}{3} - \frac{1}{4} = \frac{1}{12}
                                        \end{align}
                                    </div>
                                </ol>
                            </li>
                            <li>
                                <p>\(Y = -2 \ln(1 - X)\) とおく。</p>
                                <ol>
                                    <li><strong>累積分布関数 (CDF)</strong>:</li>
                                    <div class="scroll">
                                        \begin{align}
                                        F_Y(y) = P(Y \le y) = P(-2 \ln(1 - X) \le y)
                                        \end{align}
                                    </div>
                                    <div class="scroll">
                                        \begin{align}
                                        -2 \ln(1 - X) \le y \implies \ln(1 - X) \ge -\frac{y}{2} \implies 1 - X \ge e^{-\frac{y}{2}} \implies X \le 1 - e^{-\frac{y}{2}}
                                        \end{align}
                                    </div>
                                    <div class="scroll">
                                        \begin{align}
                                        F_Y(y) = P(X \le 1 - e^{-\frac{y}{2}}) = F_X(1 - e^{-\frac{y}{2}}) = 1 - e^{-\frac{y}{2}}
                                        \end{align}
                                    </div>
                                    <li><strong>確率密度関数 (PDF)</strong>:</li>
                                    <div class="scroll">
                                        \begin{align}
                                        f_Y(y) = \frac{d}{dy} F_Y(y) = \frac{d}{dy} (1 - e^{-\frac{y}{2}}) = \frac{1}{2} e^{-\frac{y}{2}}
                                        \end{align}
                                    </div>
                                    <li><strong>期待値</strong>:</li>
                                    <div class="scroll">
                                        \begin{align}
                                        E[Y] = \int_0^\infty y \cdot f_Y(y) \, dy = \int_0^\infty y \cdot \frac{1}{2} e^{-\frac{y}{2}} \, dy
                                        \end{align}
                                    </div>
                                    <div class="scroll">
                                        \begin{align}
                                        \text{置換 } u = \frac{y}{2} \text{ を行うと、}
                                        \end{align}
                                    </div>
                                    <div class="scroll">
                                        \begin{align}
                                        E[Y] = \int_0^\infty 2u \cdot e^{-u} \, du = 2 \int_0^\infty u e^{-u} \, du
                                        \end{align}
                                    </div>
                                    <div class="scroll">
                                        \begin{align}
                                        \int_0^\infty u e^{-u} \, du \text{ はガンマ関数の結果から } 1 \text{ であるため、}
                                        \end{align}
                                    </div>
                                    <div class="scroll">
                                        \begin{align}
                                        E[Y] = 2 \cdot 1 = 2
                                        \end{align}
                                    </div>
                                    <li><strong>分散</strong>:</li>
                                    <div class="scroll">
                                        \begin{align}
                                        \mathrm{Var}(Y) = E[Y^2] - (E[Y])^2
                                        \end{align}
                                    </div>
                                    <div class="scroll">
                                        \begin{align}
                                        E[Y^2] = \int_0^\infty y^2 \cdot f_Y(y) \, dy = \int_0^\infty y^2 \cdot \frac{1}{2} e^{-\frac{y}{2}} \, dy
                                        \end{align}
                                    </div>
                                    <div class="scroll">
                                        \begin{align}
                                        \text{置換 } u = \frac{y}{2} \text{ を行うと、}
                                        \end{align}
                                    </div>
                                    <div class="scroll">
                                        \begin{align}
                                        E[Y^2] = \int_0^\infty 4u^2 \cdot e^{-u} \, du = 4 \int_0^\infty u^2 e^{-u} \, du
                                        \end{align}
                                    </div>
                                    <div class="scroll">
                                        \begin{align}
                                        \int_0^\infty u^2 e^{-u} \, du \text{ はガンマ関数の結果から } 2 \text{ であるため、}
                                        \end{align}
                                    </div>
                                    <div class="scroll">
                                        \begin{align}
                                        E[Y^2] = 4 \cdot 2 = 8
                                        \end{align}
                                    </div>
                                    <div class="scroll">
                                        \begin{align}
                                        \mathrm{Var}(Y) = 8 - (2)^2 = 8 - 4 = 4
                                        \end{align}
                                    </div>
                                </ol>
                            </li>
                        </ol>                       
            </ul>
        </section>
        <section id="algorithms-data-structures">
          <h2>アルゴリズムとデータ構造</h2>
            <h3>計算量、オーダー表記</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://www.momoyama-usagi.com/entry/calc-order">プログラムの計算量、オーダー表記 O( ) の求め方のまとめ _ 工業大学生ももやまのうさぎ塾.html</a></p>
            <ul>
                <li>項の強さを弱い順に左から並べると、以下のとおりである。</li>
                <ul>
                    <li>\(logn, \sqrt{n}, n, nlogn, n^2, n^3, 2^n, n!\)</li>
                </ul>
            </ul>
            <h3>最小全域木</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://www.momoyama-usagi.com/entry/math-risan13">うさぎでもわかる離散数学（グラフ理論）　第13羽　最小全域木の求め方（クラスカル法・プリム法） _ 工業大学生ももやまのうさぎ塾.html</a></p>
            <ul>
                <li><b>全域木</b>とは、もとのグラフのすべての点を含み、さらに選んだ辺が木となっているようなグラフである。</li>
                <li><b>最小全域木</b>とは、選んだ辺の重みの合計が一番小さい全域木のことである。</li>
                <li>最小全域木を求めるアルゴリズム①「<b>クラスカル法</b>」は以下のとおりである。</li>
                <ol>
                    <li>重みが小さい順に辺をチェックする。</li>
                    <li>チェックした辺を追加しても閉路ができなければ辺を追加する。追加して閉路ができてしまう場合は無視する。</li>
                    <li>すべての辺をチェックし終わったときに追加された辺が最小全域木である。</li>
                </ol>
                <li>最小全域木を求めるアルゴリズム②「<b>プリム法</b>」は以下のとおりである。</li>
                <ol>
                    <li>スタートの点を１つ決める（どこでも良い）。</li>
                    <li>スタート点から辿れる辺の中で最も重みが小さい辺を追加する（最も重みが小さい辺が複数ある場合は、重みが小さいどの辺を選んでも良い）。</li>
                    <li>追加した辺につながっているすべての点から辿れる辺かつ追加しても閉路とならない辺の中から最も重みが小さい辺を追加する。</li>
                    <li>追加できる辺がなくなったとき、最小全域木である。</li>
                </ol>
            </ul>
            <h3>ダイクストラ法</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://algo-logic.info/dijkstra/">ダイクストラ法による単一始点最短経路を求めるアルゴリズム _ アルゴリズムロジック.html</a></p>
            <ul>
                <li>ダイクストラ法は、ネットワークにおいて、すべての頂点\(v\)に対して、指定された頂点\(s\)から頂点\(v\)への最短路長\(D(v)\)を求めるアルゴリズムである。</li>
                <li>アルゴリズムは以下のとおりである。</li>
                <ol>
                    <li>始点\(s\)を「既に最短距離が確定した頂点」、他の頂点を「まだ最短距離が確定していない頂点」とする。</li>
                    <li>以下をすべての頂点の最短距離が確定するまで繰り返す。</li>
                    <ol>
                        <li>すべての「既に最短距離が確定した頂点\(u\)」から「まだ最短距離が確定していない頂点\(v\)」へ伸びるすべての辺\(e=(u, v)\)について、「\(v\)と\(D(v)\)の候補」をまとめておく。</li>
                        <li>候補の中から、\(D(v)\)が最小のものを選択し、\(v\)を「既に最短距離が確定した頂点」に加える。</li>
                    </ol>
                </ol>
            </ul>
            <h3>単純グラフ</h3>
            <ul>
                <li>具体例</li>
                <div class="example">
                    <b>問題</b>
                        <p>\(n\)個の頂点と\(m\)個の辺からなる無向グラフ \(G(V, E)\) について、以下の問いに答えよ。なお、ループ（自己閉路）も多重辺も含まないグラフを単純グラフという。</p>
                        <ol>
                            <li>
                                <p>以下の定義を答えよ。</p>
                                <ol>
                                    <li><strong>頂点の次数</strong></li>
                                    <p>頂点の次数とは、その頂点に接続している辺の数のことを指す。</p>
                                    <li><strong>完全グラフ</strong></li>
                                    <p>完全グラフとは、任意の2つの異なる頂点の間に辺が存在するグラフのことを指す。 \( n \) 頂点の完全グラフは \( K_n \) と表され、その辺の数は \( \frac{n(n-1)}{2} \) である。</p>
                                </ol>
                            </li>
                            <li>
                                <p>単純グラフ \(G(V, E)\) のすべての頂点の次数が \( \frac{n-1}{2} \) 以上であるとき、 \(G(V, E)\) は連結であることを示せ。</p>
                                <p>証明:</p>
                                <ol>
                                    <li>連結でないと仮定して、背理法によって証明する。</li>
                                    <li>つまり、\(G\)を\(G_1\)と\(G_2\)に分離可能だと仮定する。</li>
                                    <li>このとき、\(G_1\)に含まれる頂点 \(v\) は \( \frac{n-1}{2} \) 個以上の頂点と接続されている。</li>
                                    <li>\(v\) と接続されていない\(G_2\)に含まれる頂点の数は \( n - 1 - \frac{n-1}{2} = \frac{n-1}{2} \) 以下である。</li>
                                    <li>頂点数が \( \frac{n-1}{2} \) 以下の頂点群における最大次数は\( \frac{n-1}{2} - 1 \)以下である。</li>
                                    <li>したがって、\(G_2\)に含まれる頂点の次数が\( \frac{n-1}{2} \) 以上にならず、矛盾する。</li>
                                </ol>
                            </li>
                            <li>
                                <p>辺の数が \( m = \frac{(n-1)(n-2)}{2} \) であるような、連結ではない単純グラフが存在することを示せ。</p>
                                <p>証明:</p>
                                <ol>
                                    <li>１個の頂点だけが連結でない単純グラフでは、連結な\(n-1\)個の頂点から２頂点を選ぶと辺が存在するため、辺の数が\( m = \frac{(n-1)(n-2)}{2} \)となる。</li>
                                </ol>
                            </li>
                            <li>
                                <p>単純グラフ <strong>G(V, E)</strong> の辺の数が \( m \geq \frac{(n-1)(n-2)}{2} + 1 \) であるとき、 <strong>G(V, E)</strong> は連結であることを示せ。</p>
                                <p>証明:</p>
                                <ol>
                                    <li>\( m = \frac{(n-1)(n-2)}{2} \) のとき、グラフは１個の頂点だけが連結でない単純グラフであることを示した。</li>
                                    <li>しかし、\( m \geq \frac{(n-1)(n-2)}{2} + 1 \) であれば、少なくとも1つの追加の辺が存在する。</li>
                                    <li>この追加の辺は、連結でない１個の頂点を結びつける役割を果たすため、グラフ全体が連結になる。</li>
                                    <li>したがって、\( G(V, E) \) は連結であることが示された。</li>
                                </ol>
                            </li>
                        </ol>
                </div>
            </ul>
        </section>
        <section id="artificial-intelligence">
          <h2>人工知能</h2>
            <h3>概説1</h3>
            <ul>
                  <li>近年のAI（人工知能）技術の発展は目を見張るものがある。</li>
                  <li>ここでは、このAI技術の発展を特に、言語・知識系の研究分野と、その成果を応用した対話システムに注目して概観する。</li>
                  <li>ニューラルネットワーク中のどの部分（特定の単語など）に注目するかを動的に決定する<b>アテンション機構</b>が考案され、言語の出力系列生成の品質向上につながった。</li>
                  <li>ニューラルネットワーク中のどの部分（特定の単語など）に注目するかを動的に決定する<b>アテンション機構</b>が考案され、言語の出力系列生成の品質向上につながった。</li>
                  <li>この<b>アテンション機構</b>を最大限に生かした新しい深層学習モデルとして、2017 年に<b>トランスフォーマー</b>が Google から発表された。</li>
                  <li><b>トランスフォーマー</b>は、RNN(Recurrent Neural Network)やCNN(Convolutional Neural Network)を使わずに、<b>アテンション機構</b>で構成した深層学習モデルである。</li>
                  <li>RNNやCNNより計算量が抑えられ、訓練が容易で、並列処理もしやすく、複数の言語現象を効率良く扱えて、文章中の長距離の<b>依存関係</b>も考慮しやすいといった特長を持つ。</li>
                  <li>ニューラルネットワークを用いた自然言語処理で高い精度を達成するには、大量の訓練データが必要だが、さまざまなタスクのおのおのについて大量の訓練データを用意することは容易なことではない。</li>
                  <li>そこでまず、さまざまなタスクに共通的な汎用性の高いモデルを大量のラベルなしデータで<b>事前学習(Pre-Training)</b>しておき、それをベースに個別のタスクごとに少量のラベル付きデータでの<b>追加学習(Fine Tuning)</b>を行うというアプローチが取られるようになった。</li>
                  <li>この<b>事前学習(Pre-Training)</b>で作られた<b>トランスフォーマー</b>型の深層学習モデルは、2018年にGoogle から発表された<b>BERT</b>以降、自然言語処理においてスタンダードになった。</li>
                  <li>言語モデルの規模を表すパラメータ数は、<b>BERT</b>の場合、3.4 億個であったが、2020年にOpenAIから発表された<b>GPT-3(Generative Pretrained Transformer)</b>では、事前学習に45TB のデータを用い、モデルのパラメータ数は1750億個となった。</li>
                  <li>これらは大規模なパラメータを持つことから<b>大規模言語モデル(Large Language Model: LLM)</b>と呼ばれるが、高い汎用性を示すことから<b>基盤モデル(Foundation Model)</b>とも呼ばれるようになった。</li>
                  <li>また、<b>GPT-3(Generative Pretrained Transformer)</b>においては、それまでのGPTアーキテクチャと同様に後続の系列を予測する自己回帰型の自己教師あり学習が用いられた。</li>
                  <li>タスクごとの<b>追加学習(Fine Tuning)</b>をせずとも、最初に入力する系列にタスクの記述や事例を含めることを意味する<b>プロンプト</b>により複数のタスクに対応することを<b>ゼロショット</b>と呼び、言語モデルの汎用的な活用が開拓された。</li>
                  <li>さらに、2022 年 11 月末に OpenAI から<b>ChatGPT</b>がWeb 公開された。</li>
                  <li>OpenAI が2020 年 6月に発表したGPT-3.5 に人間のフィードバックを用いた強化学習の一つである<b>RLHF(Reinforcement Learning from Human Feedback)</b>を加え、対話システムとして<b>追加学習(Fine Tuning)</b>されたものだということである。</li>
                  <li>入力された質問に対してまるで人間が書いたかのような自然な文章で説明を返し、用途に応じたテキスト（論文・電子メール・<b>プログラムコード</b>など）の作成にも活用できる。</li>
                  <li>しかし、その性能に驚く一方、誤った内容をもっともらしく回答するケースも見られ（特に<b>計算や演繹推論</b>で間違うケースが見られる）、誤って信じてしまうリスクやそれが悪用されるリスクが懸念される。</li>
            </ul>
            <h3>概説2</h3>
            <ul>
                  <li>ディープラーニングは2012年に開催されたILSVRC (ImageNet Large Scale Visual Recognition Challenge) と呼ばれる画像認識コンテストで顕著な成果を収めたことで一躍有名になった。</li>
                  <li>ディープラーニングは、<b>ニューラルネットワーク</b>の層を3層以上に多層に積み重ね、<b>オートエンコーダ</b>などの手法を用いて<b>勾配消失問題</b>を克服したもので、<b>ジェフリー・ヒントン</b>教授の研究チームが提唱した。</li>
                  <li>現在、課題別に応じて様々な構造を持つモデルが提案されている。</li>
                  <li>例えば、画像認識によく用いられるモデルとして、<b>ResNet</b>や<b>AlexNet</b>などの<b>畳み込み</b>ネットワークが有名である。</li>
                  <li>基本的な<b>畳み込み</b>ネットワークでは、<b>畳み込み</b>層、<b>プーリング</b>層、<b>全結合</b>層を経て入力画像の認識を行う。</li>
                  <li>その他、時系列データに対して長期・短期の時間依存性を学習することができる<b>LSTM (Long Short Term Memory)</b>というモデルもある。</li>
                  <li><b>LSTM</b>は内部にループ構造を持つ<b>リカレント</b>ネットワークの応用であり、<b>忘却ゲート</b>層などを導入することで飛躍的に性能が伸びた。</li>
                  <li>応用面では、囲碁の対決においてDeepMind 社が開発した<b>AlphaGo</b>が2017年に当時世界チャンピオンであった柯潔（カ・ケツ）を打ち負かしたことなどが記憶に新しい。</li>
                  <li>このモデルは<b>方策</b>ネットワーク、<b>価値</b>ネットワークからなり、過去の棋譜をベースに<b>教師あり学習</b>を行ったあと、自己対戦による<b>強化学習</b>を行うことによって学習が行われる。</li>
                  <li>ディープラーニングの実行には膨大な計算が必要であるが、計算時間を短縮するために<b>GPU (Graphics Processing Unit)</b>を利用した<b>TensorFlow</b>や<b>PyTorch</b>などのライブラリが公開されており、これらのライブラリを使うことで効率よくモデルの開発が可能となった。</li>
            </ul>
            <h3>Q学習</h3>
            <ul>
                <li><b style="color:brown">状態遷移が有限マルコフ決定過程であるとはどういうことか。</b></li>
                <ul>
                    <li>マルコフ性を満たす環境においてエージェントが意思決定をして状態が確率的に遷移し、状態や行動が有限であるということ。</li>
                    <li>環境がマルコフ性を持つとは、過去の環境の履歴すべてが、現在の環境情報に集約されていることを指す。</li>
                </ul>
                <li><b style="color:brown">式を用いて方策\(\pi(s, a)\)の例を示せ。</b></li>
                <ul>
                    <li>
                        グリーディ方策
                        <div class="scroll">
                        \begin{align}
                        \pi(s, a) = \arg\max\limits_a Q(s, a)
                        \end{align}
                        </div>
                    </li>
                    <li>
                        ε-グリーディ方策
                        <div class="scroll">
                        \begin{align}
                        \pi(s, a) = 
                        \begin{cases} 
                        \arg\max\limits_a Q(s, a) & \text{with probability } 1 - \epsilon \\
                        \text{random action} & \text{with probability } \epsilon
                        \end{cases}
                        \end{align}
                        </div>
                    </li>
                    <li>
                        ソフトマックス方策
                        <div class="scroll">
                        \begin{align}
                        \pi(s, a) = \frac{e^{Q(s, a) / \tau}}{\sum\limits_{b} e^{Q(s, b) / \tau}}
                        \end{align}
                        </div>
                    </li>
                </ul>
                <li><b style="color:brown">エージェントが状態\(s_t\)において取った行動を\(a_t\)とするとき、行動価値\(Q(s_t, a_t)\)の更新を式で表せ。</b></li>
                <ul>
                    <li>
                        <div class="scroll">
                        \begin{align}
                        Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( r_{t+1} + \gamma \max\limits_a Q(s_{t+1}, a) - Q(s_t, a_t) \right)
                        \end{align}
                        </div>
                    </li>
                </ul>
                <li><b style="color:brown">すべての状態と行動の組が無限に繰り返されて行動価値が更新されるとき、最終的に行動価値はどのような値に収束するか。</b></li>
                <ul>
                    <li>真の最適行動価値関数に収束する。</li>
                </ul>
            </ul>
            <h3>ニューラルネットワーク</h3>
            <ul>
                <li><b style="color:brown">ニューラルネットワークの教師あり学習における学習プロセスを説明せよ。</b></li>
                <ul>
                    <li>入力層に入力信号を与えて順方向に中間層の出力を計算し、出力層からの出力を計算する。</li>
                    <li>出力層からの出力と教師信号との誤差から<b>損失関数</b>の値を計算する。</li>
                    <li><b>誤差逆伝播法</b>を用いて出力層から入力層に向かう逆方向に各ユニット間の<b>重み</b>に対する<b>勾配</b>を計算する。</li>
                    <li><b>損失関数</b>の値を最小化するために、<b>最急降下法</b>を適用して各ユニット間の<b>重み</b>を更新する。</li>
                    <li>終了条件を満たすまで以上を繰り返す。</li>
                </ul>
                <li><b style="color:brown">上記の学習プロセスにおいて更新する重みを\(w_i\)、学習率を\(\alpha\)、損失関数を\(L(\cdot)\)とするとき、偏微分を用いて最急降下法による重みの更新式を示せ。</b></li>
                <ul>
                    <li>
                        <div class="scroll">
                        \begin{align}
                        w_{i+1} &= w_i - \alpha \frac{\partial L}{\partial w_i}
                        \end{align}
                        </div>
                    </li>
                </ul>
                <li><b style="color:brown">活性化関数にシグモイド関数がよく使われるのはなぜか。</b></li>
                <ul>
                    <li>シグモイド関数は非線形な関数であり、ニューラルネットワークに複雑なパターンや関係を学習する能力を持たせることができるから。</li>
                </ul>
                <li><b style="color:brown">最急降下法は使用するサンプルの数により、バッチ最急降下法と確率的最急降下法がある。その違い、およびそれぞれメリットとデメリットを簡単に説明せよ。</b></li>
                <ul>
                    <li>バッチ最急降下法では、すべてのトレーニングデータを使用して勾配を計算するため、各ステップの勾配は安定していて、収束は滑らかだが、計算コストが高く、メモリの使用量も多い。</li>
                    <li>一方、確率的最急降下法ではトレーニングデータからランダムに選んだ1つのサンプルを用いるため、計算コストが低いが、収束が不安定である。</li>
                </ul>
            </ul>
            <h3>サポートベクターマシン</h3>
            <ul>
                <li><b style="color:brown">サポートベクターマシン（SVM）は、パターン識別用の教師あり機械学習の方法であり、局所解収束の問題がないという長所がある。
                特に、<b>マージン最大化</b>という手法で汎化能力も高め、現在知られている方法としては、優秀なパターン識別能力を持つとされている。
                また、<b>カーネルトリック</b>という巧妙な方法を用いることにより、応用範囲が格段に広がっている。</b></li>
                <li><b style="color:brown">マージン最大化について説明せよ。</b></li>
                <ul>
                    <li>SVMは、データポイントを識別するために<b>識別境界面</b>を見つける。
                        この識別境界面は、データポイント間の<b>マージン</b>を最大化するように配置される。
                        <b>マージン</b>とは、識別境界面から最も近いデータポイントまでの「距離」を指し、この距離を最大化することで、分類の信頼性が高まる。</li>
                </ul>
                <li><b style="color:brown">カーネルトリックについて説明せよ。</b></li>
                <ul>
                    <li>SVMは、データが<b>低次元</b>空間で<b>線形分離</b>できない場合、カーネルトリックを使用してデータを<b>高次元</b>空間にマッピングする。
                        これにより、<b>高次元</b>空間ではデータが<b>線形分離</b>可能となり、SVMは効果的に分類を行うことができる。</li>
                </ul>
                <li><b style="color:brown">以下の式(a)と(b)は、SVMにおいて解を得る際の目的関数と制約条件を表している。
                <div class="scroll">
                        \begin{align}
                        \text{(a) } & \min_{\mathbf{w}, w_0} \frac{1}{2} \|\mathbf{w}\|^2 \\
                        \text{(b) } & y_i (\mathbf{w} \cdot \mathbf{x}_i + w_0) \geq 1 \quad i = 1, \ldots, N
                        \end{align}
                        </div>
                ここで、太字の記号はベクトルを表し、\(N\)は訓練データ数、\(x_i\)は\(i\)番目の訓練データを表すベクトル、\(y_i\)は\(i\)番目の訓練データに与えられた正解情報（正例では1、不例では-1）、\(w\)は重みパラメータのベクトル、\(w_0\)はバイアスパラメータとする。    
                </b></li>
                <li><b style="color:brown">式(b)で等号が成り立つのはどのような場合かを\(x_i\)を用いて説明せよ。</b></li>
                <ul>
                    <li>式(b)で等号が成り立つのは、訓練データ \(\mathbf{x}_i\) がマージンの境界上にある場合である。</li>
                    <li>つまり、\(\mathbf{x}_i\) がサポートベクターであるときに成り立つ。</li>
                </ul>
            </ul>
            <h3>K近傍法</h3>
            <ul>
                <li><b style="color:brown">２クラス分類におけるK近傍法は、分類したいデータ点\(\mathbf{x}\)が与えられたとき、訓練データ中で\(\mathbf{x}\)から最も近いK個のデータ点（K近傍と呼ぶ）を探し、それらの中の多数派が属するクラスを\(\mathbf{x}\)のクラスとする。
                ２次元平面乗の５つのデータ点からなる訓練データ\(\mathbf{x}_1 = (1, 0)\), \(\mathbf{x}_2 = (1, 2)\), \(\mathbf{x}_3 = (3, 3)\), \(\mathbf{x}_4 = (4, 3)\), \(\mathbf{x}_5 = (4, 5)\)において、\(\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3\)はクラス1に、\(\mathbf{x}_4, \mathbf{x}_5\)はクラス2に属しているとする。
                新しいデータ点\(\mathbf{x} = (2, 3)\)をどちらのクラスに分類するか、\(K=3\)としたK近傍法により決定せよ。
                ただし、データ点間の距離はユークリッド距離で測ることとする。</b></li>
                <ul>
                    <li>まず、ユークリッド距離を使用して、\(\mathbf{x} = (2, 3)\) から各訓練データ点までの距離を計算する。</li>
                    <ul>
                        <div class="scroll">
                        <li>\(\mathbf{x}_1 = (1, 0)\): \(\sqrt{(2-1)^2 + (3-0)^2} = \sqrt{1 + 9} = \sqrt{10}\)</li>
                        <li>\(\mathbf{x}_2 = (1, 2)\): \(\sqrt{(2-1)^2 + (3-2)^2} = \sqrt{1 + 1} = \sqrt{2}\)</li>
                        <li>\(\mathbf{x}_3 = (3, 3)\): \(\sqrt{(2-3)^2 + (3-3)^2} = \sqrt{1 + 0} = \sqrt{1}\)</li>
                        <li>\(\mathbf{x}_4 = (4, 3)\): \(\sqrt{(2-4)^2 + (3-3)^2} = \sqrt{4 + 0} = \sqrt{4}\)</li>
                        <li>\(\mathbf{x}_5 = (4, 5)\): \(\sqrt{(2-4)^2 + (3-5)^2} = \sqrt{4 + 4} = \sqrt{8}\)</li>
                        </div>
                    </ul>
                    <li>次に、最も近いK個 (K=3) のデータ点を見つける。</li>
                    <ul>
                        <li>\(\sqrt{1}\) (最も近い) → \(\mathbf{x}_3\) (クラス1)</li>
                        <li>\(\sqrt{2}\) (次に近い) → \(\mathbf{x}_2\) (クラス1)</li>
                        <li>\(\sqrt{4}\) (3番目に近い) → \(\mathbf{x}_4\) (クラス2)</li>
                    </ul>
                    <li>多数決によって、新しいデータ点 \(\mathbf{x} = (2, 3)\) はクラス1に分類される。</li>
                </ul>
                <li><b style="color:brown">近傍のサイズKを大きくして訓練データと同数にした場合、任意の点はどのように分類されるかを考察せよ。</b></li>
                <ul>
                    <li>近傍のサイズKを訓練データと同数の5にした場合、すべての訓練データ点が考慮される。
                        この場合、クラス1のデータ点が3つ、クラス2のデータ点が2つ存在する。</li>
                    <ul>
                        <li>クラス1: \(\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3\)</li>
                        <li>クラス2: \(\mathbf{x}_4, \mathbf{x}_5\)</li>
                    </ul>
                    <li>K=5では常にクラス1のデータ点が多数派となるため、任意の新しい点はクラス1に分類される。
                        したがって、この方法では訓練データのクラス比率が分類に強く影響を与えることになる。</li>
                </ul>
                <li><b style="color:brown">K近傍法に代表されるノンパラメトリック法を、線形識別モデルに代表されるパラメトリック法と比較した場合の問題点について、必要な計算量と記憶量の点から考察せよ。</b></li>
                <ul>
                    <li>ノンパラメトリック法は、訓練時の計算量は少ないが、予測時の計算量が大きくなる。</li>
                    <li>また、ノンパラメトリック法はデータセット全体を保存する必要があるため、記憶量が大きくなる。</li>
                </ul>
            </ul>
        </section>
    </main>
</body>
</html>
