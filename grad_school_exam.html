<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【研究ノート】走る作曲家のAIカフェ - 院試</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
          <h2>目次</h2>
          <ul>
            <li><a href="#overview">概要</a></li>
            <li><a href="#strategy">戦略</a></li>
            <li><a href="#basic-math">基礎数学</a></li>
            <li><a href="#info-math">情報数学</a></li>
            <li><a href="#probability-statistics">確率・統計</a></li>
            <li><a href="#algorithms-data-structures">アルゴリズムとデータ構造</a></li>
            <li><a href="#artificial-intelligence">人工知能</a></li>
          </ul>
        </section>
        <section id="overview">
          <h2>概要</h2>
          このページでは、「北海道大学大学院情報科学院修士課程入学試験」（令和6年8月実施）の対策を行います。過去問から傾向を掴み、必要な知識や要点をまとめていきます。
        </section>
        <section id="strategy">
          <h2>戦略</h2>
          院試は以下の内容で構成されます。
          <ul>
            <li>専門科目1: 基礎数学、情報数学、【選択】確率・統計、【選択】情報理論（4問のうち基礎数学と情報数学を含む3問を解答）</li>
            <li>専門科目2: アルゴリズムとデータ構造、人工知能、コンピュータシステム、応用数学（4問のうち2問を選択し解答）</li>
          </ul>
          俗に、専門科目1では「情報理論」が、専門科目2では「アルゴリズムとデータ構造」と「コンピュータシステム」が簡単だと言われています。
          しかし、今後の自分の研究に役立てたいので、専門科目1では「確率・統計」を、専門科目2では「アルゴリズムとデータ構造」と「人工知能」を選択しようと思います。
        </section>
        <section id="basic-math">
          <h2>基礎数学</h2>
            <h3>線形写像</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://www.momoyama-usagi.com/entry/math-linear-algebra13#1-3">うさぎでもわかる線形代数　第13羽　線形写像（後編）　核空間・像空間　線形写像の全射・単射について _ 工業大学生ももやまのうさぎ塾.html</a></p>
            <ul>
                <li>集合\(V\)から集合\(W\)への写像があるとする。</li>
                <ul>
                    <li><b>全射</b>：\(W\)の要素すべてが\(V\)のどれかの要素と対応しているような写像。</li>
                    <li><b>単射</b>：\(W\)のある要素に写すための\(V\)の要素は１つしかない（２つ以上存在しない）写像。</li>
                    <li><b>全単射</b>：全射の条件、単射の条件の両方を満たすような写像。\(V\)の要素と\(W\)の要素は１対１の関係になる。</li>
                </ul>
                <li>\(f(x) = Ax\)を満たす行列\(A\)（表現行列）が正則であることと、\(f\)が全単射であることは同値である。</li>
            </ul>
            <h3>線形独立・線形従属</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://www.momoyama-usagi.com/entry/math-linear-algebra06#google_vignette">うさぎでもわかる線形代数　第06羽　1次独立・1次従属 _ 工業大学生ももやまのうさぎ塾.html</a></p>
            <ul>
                <li>線形独立</li>
                <ul>
                    <li>ベクトルの集合が線形独立であるとは、集合内のどのベクトルも他のベクトルの線形結合として表せないことを意味する。
                        具体的には、ベクトル\(v_{1}, v_{2}, ..., v_{n}\)が線形独立であるとは、次の方程式が成り立つときである：
                        <div class="scroll">
                        \begin{align}
                        c_{1} v_{1} + c_{2} v_{2} + ... + c_{n} v_{n} = 0
                        \end{align}
                        </div>
                    </li>
                    <li>ここで、すべてのスカラー\(c_{i}\) が0である場合に限り成り立つ。つまり、
                        <div class="scroll">
                        \begin{align}
                        c_{1} = c_{2} = ... = c_{n} = 0
                        \end{align}
                        </div>
                    </li>
                </ul>
                <li>線形従属</li>
                <ul>
                    <li>一方、ベクトルの集合が線形従属であるとは、少なくとも一つのベクトルが他のベクトルの線形結合として表せることを意味する。
                        つまり、ベクトル\(v_{1}, v_{2}, ..., v_{n}\)が線形従属であるとは、次の方程式が成り立つときである：
                        <div class="scroll">
                        \begin{align}
                        c_{1} v_{1} + c_{2} v_{2} + ... + c_{n} v_{n} = 0
                        \end{align}
                        </div>
                    </li>
                    <li>ここで、少なくとも一つの\(c_{i}\)が0ではない（つまり、非自明な解が存在する）場合である。</li>
                </ul>
                <li>具体例</li>
                <div class="example">
                    <b>線形独立の例</b>
                    <p>ベクトル\(v_{1} = (1, 0)\) と \(v_{2} = (0, 1)\) は線形独立である。なぜなら、</p>
                        <div class="scroll">
                        \begin{align}
                        c_{1} (1, 0) + c_{2} (0, 1) = (0, 0)
                        \end{align}
                        </div>                  
                    <p>が成り立つのは \(c_{1} = 0\)かつ \(c_{2} = 0\) のときだけだからである。</p>
                </div>
                <div class="example">
                    <b>線形従属の例</b>
                    <p>ベクトル\(v_{1} = (1, 2)\) と \(v_{2} = (2, 4)\)は線形従属である。なぜなら、</p>
                        <div class="scroll">
                        \begin{align}
                        2 (1, 2) - 1 (2, 4) = (0, 0)
                        \end{align}
                        </div>                      
                    <p>という関係があり、非自明な解 \(c_{1} = 2, c_{2} = -1\) が存在するからである。</p>
                </div>                
            </ul>
        </section>
        <section id="info-math">
          <h2>情報数学</h2>
            <h3>二変数間の停留点</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://pictblog.com/twoparadiffer">【微積分】二変数関数の停留点(極大点・極小点・鞍点).html</a></p>
            <ul>
                <li>二変数関数\(f(x, y)\)の停留点\((a_x, a_y)\)を求めることを考える。</li>
                <li>関数の変化がなくなる点が停留点であるため、停留点では\(x\)方向、\(y\)方向ともにその変化量が0、すなわち
                    <div class="scroll">
                    \begin{align}
                    \frac{\partial f(x,y)}{\partial x}=\frac{\partial f(x,y)}{\partial y}=0
                    \end{align}
                    </div>
                を満たす点が停留点となる。</li>
                <li>停留点\((a_x, a_y)\)を求めたら、次はその種類をヘッセ行列により判定する。</li>
                <li>ヘッセ行列は次の式で表される。
                    <div class="scroll">
                    \begin{align}
                    \mathsf{H}=
                        \begin{pmatrix}\displaystyle{\frac{\partial^{2} f(x,y)}{\partial x^{2}}}&\displaystyle{\frac{\partial^{2} f(x,y)}{\partial x\partial y}} \\ \\ 
                        \displaystyle{\frac{\partial^{2} f(x,y)}{\partial x\partial y}}&\displaystyle{\frac{\partial^{2} f(x,y)}{\partial y^{2}}}\end{pmatrix}
                    \end{align}
                    </div>
                </li>
                <li>ヘッセ行列に停留点の座標\((a_x, a_y)\)を代入する。
                    <div class="scroll">
                    \begin{align}
                    \mathsf{H}=
                        \mathsf{H}_{(a_{x},a_{y})}
                        =\begin{pmatrix}\displaystyle{\left.\frac{\partial^{2} f(x,y)}{\partial x^{2}}\right|_{x=a_{x},y=a_{y}}}
                        &\displaystyle{\left.\frac{\partial^{2} f(x,y)}{\partial x\partial y}\right|_{x=a_{x},y=a_{y}}} \\ \\ 
                        \displaystyle{\left.\frac{\partial^{2} f(x,y)}{\partial x\partial y}\right|_{x=a_{x},y=a_{y}}}
                        &\displaystyle{\left.\frac{\partial^{2} f(x,y)}{\partial y^{2}}\right|_{x=a_{x},y=a_{y}}}\end{pmatrix}
                    \end{align}
                    </div>
                </li>
                <li>この行列の固有値を\(\lambda_{1},\lambda_{2}\)とすると、次のように停留点の種類を判別できる。
                    <div class="scroll">
                    \begin{align}
                    \begin{cases}
                        \lambda_{1}>0,\lambda_{2}>0&\to\,\text{極小点}\\ 
                        \lambda_{1}<0,\lambda_{2}<0&\to\,\text{極大点}\\ 
                        \lambda_{1}\lambda_{2}<0&\to\,\text{鞍点} 
                    \end{cases}
                    \end{align}
                    </div>
                </li>
            </ul>
            <h3>極座標変換</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://www.momoyama-usagi.com/entry/math-analysis25">うさぎでもわかる解析　Part25　極座標変換を用いた2重積分の求め方 _ 工業大学生ももやまのうさぎ塾.html</a></p>
            <ul>
                <li>\(x\)と\(y\)を次のようにおく。
                    <div class="scroll">
                    \begin{align}
                    x = r \cos \theta , \ \ \ y = r \sin \theta \\ \left( r \geqq 0, \ \ 0 \leqq \theta \leqq 2 \pi \right)
                    \end{align}
                    </div>
                </li>
                <li>このとき、ヤコビアンを計算すると以下のようになる。
                    <div class="scroll">
                    \begin{align}
                    J = & \left| \begin{array}{ccc} \frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\ \frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta} \end{array} \right|
                        \\ = & \left| \begin{array}{ccc} \cos \theta & - r \sin \theta \\ \sin \theta & r \cos \theta \end{array} \right|
                        \\ = & \ r \left( \cos^2 \theta + \sin^2 \theta \right)
                        \\ = & \ r
                    \end{align}
                    </div>
                </li>
                <li>よって、\(dxdy = r \ dr d \theta\)となる。</li>
            </ul>
        </section>
        <section id="probability-statistics">
          <h2>確率・統計</h2>
          <ul>
              <li></li>
              <li></li>
              <li></li>
              <li></li>
          </ul>
        </section>
        <section id="algorithms-data-structures">
          <h2>アルゴリズムとデータ構造</h2>
          <ul>
              <li></li>
              <li></li>
              <li></li>
              <li></li>
          </ul>
        </section>
        <section id="artificial-intelligence">
          <h2>人工知能</h2>
            <h3>概説1</h3>
            <ul>
                  <li>近年のAI（人工知能）技術の発展は目を見張るものがある。</li>
                  <li>ここでは、このAI技術の発展を特に、言語・知識系の研究分野と、その成果を応用した対話システムに注目して概観する。</li>
                  <li>ニューラルネットワーク中のどの部分（特定の単語など）に注目するかを動的に決定する<b>アテンション機構</b>が考案され、言語の出力系列生成の品質向上につながった。</li>
                  <li>ニューラルネットワーク中のどの部分（特定の単語など）に注目するかを動的に決定する<b>アテンション機構</b>が考案され、言語の出力系列生成の品質向上につながった。</li>
                  <li>この<b>アテンション機構</b>を最大限に生かした新しい深層学習モデルとして、2017 年に<b>トランスフォーマー</b>が Google から発表された。</li>
                  <li><b>トランスフォーマー</b>は、RNN(Recurrent Neural Network)やCNN(Convolutional Neural Network)を使わずに、<b>アテンション機構</b>で構成した深層学習モデルである。</li>
                  <li>RNNやCNNより計算量が抑えられ、訓練が容易で、並列処理もしやすく、複数の言語現象を効率良く扱えて、文章中の長距離の<b>依存関係</b>も考慮しやすいといった特長を持つ。</li>
                  <li>ニューラルネットワークを用いた自然言語処理で高い精度を達成するには、大量の訓練データが必要だが、さまざまなタスクのおのおのについて大量の訓練データを用意することは容易なことではない。</li>
                  <li>そこでまず、さまざまなタスクに共通的な汎用性の高いモデルを大量のラベルなしデータで<b>事前学習(Pre-Training)</b>しておき、それをベースに個別のタスクごとに少量のラベル付きデータでの<b>追加学習(Fine Tuning)</b>を行うというアプローチが取られるようになった。</li>
                  <li>この<b>事前学習(Pre-Training)</b>で作られた<b>トランスフォーマー</b>型の深層学習モデルは、2018年にGoogle から発表された<b>BERT</b>以降、自然言語処理においてスタンダードになった。</li>
                  <li>言語モデルの規模を表すパラメータ数は、<b>BERT</b>の場合、3.4 億個であったが、2020年にOpenAIから発表された<b>GPT-3(Generative Pretrained Transformer)</b>では、事前学習に45TB のデータを用い、モデルのパラメータ数は1750億個となった。</li>
                  <li>これらは大規模なパラメータを持つことから<b>大規模言語モデル(Large Language Model: LLM)</b>と呼ばれるが、高い汎用性を示すことから<b>基盤モデル(Foundation Model)</b>とも呼ばれるようになった。</li>
                  <li>また、<b>GPT-3(Generative Pretrained Transformer)</b>においては、それまでのGPTアーキテクチャと同様に後続の系列を予測する自己回帰型の自己教師あり学習が用いられた。</li>
                  <li>タスクごとの<b>追加学習(Fine Tuning)</b>をせずとも、最初に入力する系列にタスクの記述や事例を含めることを意味する<b>プロンプト</b>により複数のタスクに対応することを<b>ゼロショット</b>と呼び、言語モデルの汎用的な活用が開拓された。</li>
                  <li>さらに、2022 年 11 月末に OpenAI から<b>ChatGPT</b>がWeb 公開された。</li>
                  <li>OpenAI が2020 年 6月に発表したGPT-3.5 に人間のフィードバックを用いた強化学習の一つである<b>RLHF(Reinforcement Learning from Human Feedback)</b>を加え、対話システムとして<b>追加学習(Fine Tuning)</b>されたものだということである。</li>
                  <li>入力された質問に対してまるで人間が書いたかのような自然な文章で説明を返し、用途に応じたテキスト（論文・電子メール・<b>プログラムコード</b>など）の作成にも活用できる。</li>
                  <li>しかし、その性能に驚く一方、誤った内容をもっともらしく回答するケースも見られ（特に<b>計算や演繹推論</b>で間違うケースが見られる）、誤って信じてしまうリスクやそれが悪用されるリスクが懸念される。</li>
            </ul>
            <h3>概説2</h3>
            <ul>
                  <li>ディープラーニングは2012年に開催されたILSVRC (ImageNet Large Scale Visual Recognition Challenge) と呼ばれる画像認識コンテストで顕著な成果を収めたことで一躍有名になった。</li>
                  <li>ディープラーニングは、<b>ニューラルネットワーク</b>の層を3層以上に多層に積み重ね、<b>オートエンコーダ</b>などの手法を用いて<b>勾配消失問題</b>を克服したもので、<b>ジェフリー・ヒントン</b>教授の研究チームが提唱した。</li>
                  <li>現在、課題別に応じて様々な構造を持つモデルが提案されている。</li>
                  <li>例えば、画像認識によく用いられるモデルとして、<b>ResNet</b>や<b>AlexNet</b>などの<b>畳み込み</b>ネットワークが有名である。</li>
                  <li>基本的な<b>畳み込み</b>ネットワークでは、<b>畳み込み</b>層、<b>プーリング</b>層、<b>全結合</b>層を経て入力画像の認識を行う。</li>
                  <li>その他、時系列データに対して長期・短期の時間依存性を学習することができる<b>LSTM (Long Short Term Memory)</b>というモデルもある。</li>
                  <li><b>LSTM</b>は内部にループ構造を持つ<b>リカレント</b>ネットワークの応用であり、<b>忘却ゲート</b>層などを導入することで飛躍的に性能が伸びた。</li>
                  <li>応用面では、囲碁の対決においてDeepMind 社が開発した<b>AlphaGo</b>が2017年に当時世界チャンピオンであった柯潔（カ・ケツ）を打ち負かしたことなどが記憶に新しい。</li>
                  <li>このモデルは<b>方策</b>ネットワーク、<b>価値</b>ネットワークからなり、過去の棋譜をベースに<b>教師あり学習</b>を行ったあと、自己対戦による<b>強化学習</b>を行うことによって学習が行われる。</li>
                  <li>ディープラーニングの実行には膨大な計算が必要であるが、計算時間を短縮するために<b>GPU (Graphics Processing Unit)</b>を利用した<b>TensorFlow</b>や<b>PyTorch</b>などのライブラリが公開されており、これらのライブラリを使うことで効率よくモデルの開発が可能となった。</li>
            </ul>
            <h3>Q学習</h3>
            <ul>
                <li><b style="color:brown">状態遷移が有限マルコフ決定過程であるとはどういうことか。</b></li>
                <ul>
                    <li>マルコフ性を満たす環境においてエージェントが意思決定をして状態が確率的に遷移し、状態や行動が有限であるということ。</li>
                    <li>環境がマルコフ性を持つとは、過去の環境の履歴すべてが、現在の環境情報に集約されていることを指す。</li>
                </ul>
                <li><b style="color:brown">式を用いて方策\(\pi(s, a)\)の例を示せ。</b></li>
                <ul>
                    <li>
                        グリーディ方策
                        <div class="scroll">
                        \begin{align}
                        \pi(s, a) = \arg\max\limits_a Q(s, a)
                        \end{align}
                        </div>
                    </li>
                    <li>
                        ε-グリーディ方策
                        <div class="scroll">
                        \begin{align}
                        \pi(s, a) = 
                        \begin{cases} 
                        \arg\max\limits_a Q(s, a) & \text{with probability } 1 - \epsilon \\
                        \text{random action} & \text{with probability } \epsilon
                        \end{cases}
                        \end{align}
                        </div>
                    </li>
                    <li>
                        ソフトマックス方策
                        <div class="scroll">
                        \begin{align}
                        \pi(s, a) = \frac{e^{Q(s, a) / \tau}}{\sum\limits_{b} e^{Q(s, b) / \tau}}
                        \end{align}
                        </div>
                    </li>
                </ul>
                <li><b style="color:brown">エージェントが状態\(s_t\)において取った行動を\(a_t\)とするとき、行動価値\(Q(s_t, a_t)\)の更新を式で表せ。</b></li>
                <ul>
                    <li>
                        <div class="scroll">
                        \begin{align}
                        Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( r_{t+1} + \gamma \max\limits_a Q(s_{t+1}, a) - Q(s_t, a_t) \right)
                        \end{align}
                        </div>
                    </li>
                </ul>
                <li><b style="color:brown">すべての状態と行動の組が無限に繰り返されて行動価値が更新されるとき、最終的に行動価値はどのような値に収束するか。</b></li>
                <ul>
                    <li>真の最適行動価値関数に収束する。</li>
                </ul>
            </ul>
            <h3>ニューラルネットワーク</h3>
            <ul>
                <li><b style="color:brown">ニューラルネットワークの教師あり学習における学習プロセスを説明せよ。</b></li>
                <ul>
                    <li>入力層に入力信号を与えて順方向に中間層の出力を計算し、出力層からの出力を計算する。</li>
                    <li>出力層からの出力と教師信号との誤差から<b>損失関数</b>の値を計算する。</li>
                    <li><b>誤差逆伝播法</b>を用いて出力層から入力層に向かう逆方向に各ユニット間の<b>重み</b>に対する<b>勾配</b>を計算する。</li>
                    <li><b>損失関数</b>の値を最小化するために、<b>最急降下法</b>を適用して各ユニット間の<b>重み</b>を更新する。</li>
                    <li>終了条件を満たすまで以上を繰り返す。</li>
                </ul>
                <li><b style="color:brown">上記の学習プロセスにおいて更新する重みを\(w_i\)、学習率を\(\alpha\)、損失関数を\(L(\cdot)\)とするとき、偏微分を用いて最急降下法による重みの更新式を示せ。</b></li>
                <ul>
                    <li>
                        <div class="scroll">
                        \begin{align}
                        w_{i+1} &= w_i - \alpha \frac{\partial L}{\partial w_i}
                        \end{align}
                        </div>
                    </li>
                </ul>
                <li><b style="color:brown">活性化関数にシグモイド関数がよく使われるのはなぜか。</b></li>
                <ul>
                    <li>シグモイド関数は非線形な関数であり、ニューラルネットワークに複雑なパターンや関係を学習する能力を持たせることができるから。</li>
                </ul>
                <li><b style="color:brown">最急降下法は使用するサンプルの数により、バッチ最急降下法と確率的最急降下法がある。その違い、およびそれぞれメリットとデメリットを簡単に説明せよ。</b></li>
                <ul>
                    <li>バッチ最急降下法では、すべてのトレーニングデータを使用して勾配を計算するため、各ステップの勾配は安定していて、収束は滑らかだが、計算コストが高く、メモリの使用量も多い。</li>
                    <li>一方、確率的最急降下法ではトレーニングデータからランダムに選んだ1つのサンプルを用いるため、計算コストが低いが、収束が不安定である。</li>
                </ul>
            </ul>
            <h3>サポートベクターマシン</h3>
            <ul>
                <li><b style="color:brown">サポートベクターマシン（SVM）は、パターン識別用の教師あり機械学習の方法であり、局所解収束の問題がないという長所がある。
                特に、<b>マージン最大化</b>という手法で汎化能力も高め、現在知られている方法としては、優秀なパターン識別能力を持つとされている。
                また、<b>カーネルトリック</b>という巧妙な方法を用いることにより、応用範囲が格段に広がっている。</b></li>
                <li><b style="color:brown">マージン最大化について説明せよ。</b></li>
                <ul>
                    <li>SVMは、データポイントを識別するために<b>識別境界面</b>を見つける。
                        この識別境界面は、データポイント間の<b>マージン</b>を最大化するように配置される。
                        <b>マージン</b>とは、識別境界面から最も近いデータポイントまでの「距離」を指し、この距離を最大化することで、分類の信頼性が高まる。</li>
                </ul>
                <li><b style="color:brown">カーネルトリックについて説明せよ。</b></li>
                <ul>
                    <li>SVMは、データが<b>低次元</b>空間で<b>線形分離</b>できない場合、カーネルトリックを使用してデータを<b>高次元</b>空間にマッピングする。
                        これにより、<b>高次元</b>空間ではデータが<b>線形分離</b>可能となり、SVMは効果的に分類を行うことができる。</li>
                </ul>
                <li><b style="color:brown">以下の式(a)と(b)は、SVMにおいて解を得る際の目的関数と制約条件を表している。
                <div class="scroll">
                        \begin{align}
                        \text{(a) } & \min_{\mathbf{w}, w_0} \frac{1}{2} \|\mathbf{w}\|^2 \\
                        \text{(b) } & y_i (\mathbf{w} \cdot \mathbf{x}_i + w_0) \geq 1 \quad i = 1, \ldots, N
                        \end{align}
                        </div>
                ここで、太字の記号はベクトルを表し、\(N\)は訓練データ数、\(x_i\)は\(i\)番目の訓練データを表すベクトル、\(y_i\)は\(i\)番目の訓練データに与えられた正解情報（正例では1、不例では-1）、\(w\)は重みパラメータのベクトル、\(w_0\)はバイアスパラメータとする。    
                </b></li>
                <li><b style="color:brown">式(b)で等号が成り立つのはどのような場合かを\(x_i\)を用いて説明せよ。</b></li>
                <ul>
                    <li>式(b)で等号が成り立つのは、訓練データ \(\mathbf{x}_i\) がマージンの境界上にある場合である。</li>
                    <li>つまり、\(\mathbf{x}_i\) がサポートベクターであるときに成り立つ。</li>
                </ul>
            </ul>
            <h3>K近傍法</h3>
            <ul>
                <li><b style="color:brown">２クラス分類におけるK近傍法は、分類したいデータ点\(\mathbf{x}\)が与えられたとき、訓練データ中で\(\mathbf{x}\)から最も近いK個のデータ点（K近傍と呼ぶ）を探し、それらの中の多数派が属するクラスを\(\mathbf{x}\)のクラスとする。
                ２次元平面乗の５つのデータ点からなる訓練データ\(\mathbf{x}_1 = (1, 0)\), \(\mathbf{x}_2 = (1, 2)\), \(\mathbf{x}_3 = (3, 3)\), \(\mathbf{x}_4 = (4, 3)\), \(\mathbf{x}_5 = (4, 5)\)において、\(\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3\)はクラス1に、\(\mathbf{x}_4, \mathbf{x}_5\)はクラス2に属しているとする。
                新しいデータ点\(\mathbf{x} = (2, 3)\)をどちらのクラスに分類するか、\(K=3\)としたK近傍法により決定せよ。
                ただし、データ点間の距離はユークリッド距離で測ることとする。</b></li>
                <ul>
                    <li>まず、ユークリッド距離を使用して、\(\mathbf{x} = (2, 3)\) から各訓練データ点までの距離を計算する。</li>
                    <ul>
                        <div class="scroll">
                        <li>\(\mathbf{x}_1 = (1, 0)\): \(\sqrt{(2-1)^2 + (3-0)^2} = \sqrt{1 + 9} = \sqrt{10}\)</li>
                        <li>\(\mathbf{x}_2 = (1, 2)\): \(\sqrt{(2-1)^2 + (3-2)^2} = \sqrt{1 + 1} = \sqrt{2}\)</li>
                        <li>\(\mathbf{x}_3 = (3, 3)\): \(\sqrt{(2-3)^2 + (3-3)^2} = \sqrt{1 + 0} = \sqrt{1}\)</li>
                        <li>\(\mathbf{x}_4 = (4, 3)\): \(\sqrt{(2-4)^2 + (3-3)^2} = \sqrt{4 + 0} = \sqrt{4}\)</li>
                        <li>\(\mathbf{x}_5 = (4, 5)\): \(\sqrt{(2-4)^2 + (3-5)^2} = \sqrt{4 + 4} = \sqrt{8}\)</li>
                        </div>
                    </ul>
                    <li>次に、最も近いK個 (K=3) のデータ点を見つける。</li>
                    <ul>
                        <li>\(\sqrt{1}\) (最も近い) → \(\mathbf{x}_3\) (クラス1)</li>
                        <li>\(\sqrt{2}\) (次に近い) → \(\mathbf{x}_2\) (クラス1)</li>
                        <li>\(\sqrt{4}\) (3番目に近い) → \(\mathbf{x}_4\) (クラス2)</li>
                    </ul>
                    <li>多数決によって、新しいデータ点 \(\mathbf{x} = (2, 3)\) はクラス1に分類される。</li>
                </ul>
                <li><b style="color:brown">近傍のサイズKを大きくして訓練データと同数にした場合、任意の点はどのように分類されるかを考察せよ。</b></li>
                <ul>
                    <li>近傍のサイズKを訓練データと同数の5にした場合、すべての訓練データ点が考慮される。
                        この場合、クラス1のデータ点が3つ、クラス2のデータ点が2つ存在する。</li>
                    <ul>
                        <li>クラス1: \(\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3\)</li>
                        <li>クラス2: \(\mathbf{x}_4, \mathbf{x}_5\)</li>
                    </ul>
                    <li>K=5では常にクラス1のデータ点が多数派となるため、任意の新しい点はクラス1に分類される。
                        したがって、この方法では訓練データのクラス比率が分類に強く影響を与えることになる。</li>
                </ul>
                <li><b style="color:brown">K近傍法に代表されるノンパラメトリック法を、線形識別モデルに代表されるパラメトリック法と比較した場合の問題点について、必要な計算量と記憶量の点から考察せよ。</b></li>
                <ul>
                    <li>ノンパラメトリック法は、訓練時の計算量は少ないが、予測時の計算量が大きくなる。</li>
                    <li>また、ノンパラメトリック法はデータセット全体を保存する必要があるため、記憶量が大きくなる。</li>
                </ul>
            </ul>
        </section>
    </main>
</body>
</html>
