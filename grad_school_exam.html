<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>走る作曲家のAIカフェ - 院試</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        ※数式はスクロールできます。
        <section>
          <h2>目次</h2>
          <ul>
            <li><a href="#overview">概要</a></li>
            <li><a href="#strategy">戦略</a></li>
            <li><a href="#basic-math">基礎数学</a></li>
            <li><a href="#info-math">情報数学</a></li>
            <li><a href="#probability-statistics">確率・統計</a></li>
            <li><a href="#algorithms-data-structures">アルゴリズムとデータ構造</a></li>
            <li><a href="#artificial-intelligence">人工知能</a></li>
          </ul>
        </section>
        <section id="overview">
          <h2>概要</h2>
          このページでは、「北海道大学大学院情報科学院修士課程入学試験」（令和6年8月実施）の対策を行います。過去問から傾向を掴み、必要な知識や要点をまとめていきます。
        </section>
        <section id="strategy">
          <h2>戦略</h2>
          院試は以下の内容で構成されます。
          <ul>
            <li>専門科目1: 基礎数学、情報数学、【選択】確率・統計、【選択】情報理論（4問のうち基礎数学と情報数学を含む3問を解答）</li>
            <li>専門科目2: アルゴリズムとデータ構造、人工知能、コンピュータシステム、応用数学（4問のうち2問を選択し解答）</li>
          </ul>
          俗に、専門科目1では「情報理論」が、専門科目2では「アルゴリズムとデータ構造」と「コンピュータシステム」が簡単だと言われています。
          しかし、今後の自分の研究に役立てたいので、専門科目1では「確率・統計」を、専門科目2では「アルゴリズムとデータ構造」と「人工知能」を選択しようと思います。
        <section id="basic-math">
          <h2>基礎数学</h2>
          <ul>
              <li></li>
              <li></li>
              <li></li>
              <li></li>
          </ul>
        </section>
        <section id="info-math">
          <h2>情報数学</h2>
          <ul>
              <li></li>
              <li></li>
              <li></li>
              <li></li>
          </ul>
        </section>
        <section id="probability-statistics">
          <h2>確率・統計</h2>
          <ul>
              <li></li>
              <li></li>
              <li></li>
              <li></li>
          </ul>
        </section>
        <section id="algorithms-data-structures">
          <h2>アルゴリズムとデータ構造</h2>
          <ul>
              <li></li>
              <li></li>
              <li></li>
              <li></li>
          </ul>
        </section>
        <section id="artificial-intelligence">
          <h2>人工知能</h2>
            <h3>概説1</h3>
            <ul>
                  <li>近年のAI（人工知能）技術の発展は目を見張るものがある。</li>
                  <li>ここでは、このAI技術の発展を特に、言語・知識系の研究分野と、その成果を応用した対話システムに注目して概観する。</li>
                  <li>ニューラルネットワーク中のどの部分（特定の単語など）に注目するかを動的に決定する<b>アテンション機構</b>が考案され、言語の出力系列生成の品質向上につながった。</li>
                  <li>ニューラルネットワーク中のどの部分（特定の単語など）に注目するかを動的に決定する<b>アテンション機構</b>が考案され、言語の出力系列生成の品質向上につながった。</li>
                  <li>この<b>アテンション機構</b>を最大限に生かした新しい深層学習モデルとして、2017 年に<b>トランスフォーマー</b>が Google から発表された。</li>
                  <li><b>トランスフォーマー</b>は、RNN(Recurrent Neural Network)やCNN(Convolutional Neural Network)を使わずに、<b>アテンション機構</b>で構成した深層学習モデルである。</li>
                  <li>RNNやCNNより計算量が抑えられ、訓練が容易で、並列処理もしやすく、複数の言語現象を効率良く扱えて、文章中の長距離の<b>依存関係</b>も考慮しやすいといった特長を持つ。</li>
                  <li>ニューラルネットワークを用いた自然言語処理で高い精度を達成するには、大量の訓練データが必要だが、さまざまなタスクのおのおのについて大量の訓練データを用意することは容易なことではない。</li>
                  <li>そこでまず、さまざまなタスクに共通的な汎用性の高いモデルを大量のラベルなしデータで<b>事前学習(Pre-Training)</b>しておき、それをベースに個別のタスクごとに少量のラベル付きデータでの<b>追加学習(Fine Tuning)</b>を行うというアプローチが取られるようになった。</li>
                  <li>この<b>事前学習(Pre-Training)</b>で作られた<b>トランスフォーマー</b>型の深層学習モデルは、2018年にGoogle から発表された<b>BERT</b>以降、自然言語処理においてスタンダードになった。</li>
                  <li>言語モデルの規模を表すパラメータ数は、<b>BERT</b>の場合、3.4 億個であったが、2020年にOpenAIから発表された<b>GPT-3(Generative Pretrained Transformer)</b>では、事前学習に45TB のデータを用い、モデルのパラメータ数は1750億個となった。</li>
                  <li>これらは大規模なパラメータを持つことから<b>大規模言語モデル(Large Language Model: LLM)</b>と呼ばれるが、高い汎用性を示すことから<b>基盤モデル(Foundation Model)</b>とも呼ばれるようになった。</li>
                  <li>また、<b>GPT-3(Generative Pretrained Transformer)</b>においては、それまでのGPTアーキテクチャと同様に後続の系列を予測する自己回帰型の自己教師あり学習が用いられた。</li>
                  <li>タスクごとの<b>追加学習(Fine Tuning)</b>をせずとも、最初に入力する系列にタスクの記述や事例を含めることを意味する<b>プロンプト</b>により複数のタスクに対応することを<b>ゼロショット</b>と呼び、言語モデルの汎用的な活用が開拓された。</li>
                  <li>さらに、2022 年 11 月末に OpenAI から<b>ChatGPT</b>がWeb 公開された。</li>
                  <li>OpenAI が2020 年 6月に発表したGPT-3.5 に人間のフィードバックを用いた強化学習の一つである<b>RLHF(Reinforcement Learning from Human Feedback)</b>を加え、対話システムとして<b>追加学習(Fine Tuning)</b>されたものだということである。</li>
                  <li>入力された質問に対してまるで人間が書いたかのような自然な文章で説明を返し、用途に応じたテキスト（論文・電子メール・<b>プログラムコード</b>など）の作成にも活用できる。</li>
                  <li>しかし、その性能に驚く一方、誤った内容をもっともらしく回答するケースも見られ（特に<b>計算や演繹推論</b>で間違うケースが見られる）、誤って信じてしまうリスクやそれが悪用されるリスクが懸念される。</li>
            </ul>
            <h3>概説2</h3>
            <ul>
                  <li>ディープラーニングは2012年に開催されたILSVRC (ImageNet Large Scale Visual Recognition Challenge) と呼ばれる画像認識コンテストで顕著な成果を収めたことで一躍有名になった。</li>
                  <li>ディープラーニングは、<b>ニューラルネットワーク</b>の層を3層以上に多層に積み重ね、<b>オートエンコーダ</b>などの手法を用いて<b>勾配消失問題</b>を克服したもので、<b>ジェフリー・ヒントン</b>教授の研究チームが提唱した。</li>
                  <li>現在、課題別に応じて様々な構造を持つモデルが提案されている。</li>
                  <li>例えば、画像認識によく用いられるモデルとして、<b>ResNet</b>や<b>AlexNet</b>などの<b>畳み込み</b>ネットワークが有名である。</li>
                  <li>基本的な<b>畳み込み</b>ネットワークでは、<b>畳み込み</b>層、<b>プーリング</b>層、<b>全結合</b>層を経て入力画像の認識を行う。</li>
                  <li>その他、時系列データに対して長期・短期の時間依存性を学習することができる<b>LSTM (Long Short Term Memory)</b>というモデルもある。</li>
                  <li><b>LSTM</b>は内部にループ構造を持つ<b>リカレント</b>ネットワークの応用であり、<b>忘却ゲート</b>層などを導入することで飛躍的に性能が伸びた。</li>
                  <li>応用面では、囲碁の対決においてDeepMind 社が開発した<b>AlphaGo</b>が2017年に当時世界チャンピオンであった柯潔（カ・ケツ）を打ち負かしたことなどが記憶に新しい。</li>
                  <li>このモデルは<b>方策</b>ネットワーク、<b>価値</b>ネットワークからなり、過去の棋譜をベースに<b>教師あり学習</b>を行ったあと、自己対戦による<b>強化学習</b>を行うことによって学習が行われる。</li>
                  <li>ディープラーニングの実行には膨大な計算が必要であるが、計算時間を短縮するために<b>GPU (Graphics Processing Unit)</b>を利用した<b>TensorFlow</b>や<b>PyTorch</b>などのライブラリが公開されており、これらのライブラリを使うことで効率よくモデルの開発が可能となった。</li>
            </ul>
            <h3>Q学習</h3>
            <ul>
                <li><b style="color:brown">状態遷移が有限マルコフ決定過程であるとはどういうことか。</b></li>
                <ul>
                    <li>マルコフ性を満たす環境においてエージェントが意思決定をして状態が確率的に遷移し、状態や行動が有限であるということ。</li>
                    <li>環境がマルコフ性を持つとは、過去の環境の履歴すべてが、現在の環境情報に集約されていることを指す。</li>
                </ul>
                <li><b style="color:brown">式を用いて方策\(\pi(s, a)\)の例を示せ。</b></li>
                <ul>
                    <li>
                        グリーディ方策
                        <div class="scroll">
                        \begin{align}
                        \pi(s, a) = \arg\max\limits_a Q(s, a)
                        \end{align}
                        </div>
                    </li>
                    <li>
                        ε-グリーディ方策
                        <div class="scroll">
                        \begin{align}
                        \pi(s, a) = 
                        \begin{cases} 
                        \arg\max\limits_a Q(s, a) & \text{with probability } 1 - \epsilon \\
                        \text{random action} & \text{with probability } \epsilon
                        \end{cases}
                        \end{align}
                        </div>
                    </li>
                    <li>
                        ソフトマックス方策
                        <div class="scroll">
                        \begin{align}
                        \pi(s, a) = \frac{e^{Q(s, a) / \tau}}{\sum\limits_{b} e^{Q(s, b) / \tau}}
                        \end{align}
                        </div>
                    </li>
                </ul>
                <li><b style="color:brown">エージェントが状態\(s_t\)において取った行動を\(a_t\)とするとき、行動価値\(Q(s_t, a_t)\)の更新を式で表せ。</b></li>
                <ul>
                    <li>
                        <div class="scroll">
                        \begin{align}
                        Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( r_{t+1} + \gamma \max\limits_a Q(s_{t+1}, a) - Q(s_t, a_t) \right)
                        \end{align}
                        </div>
                    </li>
                </ul>
                <li><b style="color:brown">すべての状態と行動の組が無限に繰り返されて行動価値が更新されるとき、最終的に行動価値はどのような値に収束するか。</b></li>
                <ul>
                    <li>真の最適行動価値関数に収束する。</li>
                </ul>
            </ul>
            <h3>ニューラルネットワーク</h3>
            <ul>
                <li><b style="color:brown">ニューラルネットワークの教師あり学習における学習プロセスを説明せよ。</b></li>
                <ul>
                    <li>入力層に入力信号を与えて順方向に中間層の出力を計算し、出力層からの出力を計算する。</li>
                    <li>出力層からの出力と教師信号との誤差から<b>損失関数</b>の値を計算する。</li>
                    <li><b>誤差逆伝播法</b>を用いて出力層から入力層に向かう逆方向に各ユニット間の<b>重み</b>に対する<b>勾配</b>を計算する。</li>
                    <li><b>損失関数</b>の値を最小化するために、<b>最急降下法</b>を適用して各ユニット間の<b>重み</b>を更新する。</li>
                    <li>終了条件を満たすまで以上を繰り返す。</li>
                </ul>
                <li><b style="color:brown">上記の学習プロセスにおいて更新する重みを\(w_i\)、学習率を\(\alpha\)、損失関数を\(L(\cdot)\)とするとき、偏微分を用いて最急降下法による重みの更新式を示せ。</b></li>
                <ul>
                    <li>
                        <div class="scroll">
                        \begin{align}
                        w_{i+1} &= w_i - \alpha \frac{\partial L}{\partial w_i}
                        \end{align}
                        </div>
                    </li>
                </ul>
                <li><b style="color:brown">活性化関数にシグモイド関数がよく使われるのはなぜか。</b></li>
                <ul>
                    <li>シグモイド関数は非線形な関数であり、ニューラルネットワークに複雑なパターンや関係を学習する能力を持たせることができるから。</li>
                </ul>
                <li><b style="color:brown">最急降下法は使用するサンプルの数により、バッチ最急降下法と確率的最急降下法がある。その違い、およびそれぞれメリットとデメリットを簡単に説明せよ。</b></li>
                <ul>
                    <li>バッチ最急降下法では、すべてのトレーニングデータを使用して勾配を計算するため、各ステップの勾配は安定していて、収束は滑らかだが、計算コストが高く、メモリの使用量も多い。</li>
                    <li>一方、確率的最急降下法ではトレーニングデータからランダムに選んだ1つのサンプルを用いるため、計算コストが低いが、収束が不安定である。</li>
                </ul>
            </ul>
            <h3>サポートベクターマシン</h3>
            <ul>
                <li><b style="color:brown">サポートベクターマシン（SVM）は、パターン識別用の教師あり機械学習の方法であり、局所解収束の問題がないという長所がある。
                特に、<b>マージン最大化</b>という手法で汎化能力も高め、現在知られている方法としては、優秀なパターン識別能力を持つとされている。
                また、<b>カーネルトリック</b>という巧妙な方法を用いることにより、応用範囲が格段に広がっている。</b></li>
                <li><b style="color:brown">マージン最大化について説明せよ。</b></li>
                <ul>
                    <li>SVMは、データポイントを識別するために<b>識別境界面</b>を見つける。
                        この識別境界面は、データポイント間の<b>マージン</b>を最大化するように配置される。
                        <b>マージン</b>とは、識別境界面から最も近いデータポイントまでの「距離」を指し、この距離を最大化することで、分類の信頼性が高まる。</li>
                </ul>
                <li><b style="color:brown">カーネルトリックについて説明せよ。</b></li>
                <ul>
                    <li>SVMは、データが<b>低次元</b>空間で<b>線形分離</b>できない場合、カーネルトリックを使用してデータを<b>高次元</b>空間にマッピングする。
                        これにより、<b>高次元</b>空間ではデータが<b>線形分離</b>可能となり、SVMは効果的に分類を行うことができる。</li>
                </ul>
                <li><b style="color:brown">以下の式(a)と(b)は、SVMにおいて解を得る際の目的関数と制約条件を表している。
                <div class="scroll">
                        \begin{align}
                        \text{(a) } & \min_{\mathbf{w}, w_0} \frac{1}{2} \|\mathbf{w}\|^2 \\
                        \text{(b) } & y_i (\mathbf{w} \cdot \mathbf{x}_i + w_0) \geq 1 \quad i = 1, \ldots, N
                        \end{align}
                        </div>
                ここで、太字の記号はベクトルを表し、\(N\)は訓練データ数、\(x_i\)は\(i\)番目の訓練データを表すベクトル、\(y_i\)は\(i\)番目の訓練データに与えられた正解情報（正例では1、不例では-1）、\(w\)は重みパラメータのベクトル、\(w_0\)はバイアスパラメータとする。    
                </b></li>
                <li><b style="color:brown">式(b)で等号が成り立つのはどのような場合かを\(x_i\)を用いて説明せよ。</b></li>
                <ul>
                    <li>式(b)で等号が成り立つのは、訓練データ \(\mathbf{x}_i\) がマージンの境界上にある場合である。</li>
                    <li>つまり、\(\mathbf{x}_i\) がサポートベクターであるときに成り立つ。</li>
                </ul>
            </ul>
            <h3>K近傍法</h3>
            <ul>
                <li>２クラス分類におけるK近傍法は、分類したいデータ点\(\mathbf{x}\)が与えられたとき、 </li>
                <li><b style="color:brown"></b></li>
                <ul>
                    <li></li>
                    <li></li>
                </ul>
            </ul>
        </section>
    </main>
</body>
</html>
