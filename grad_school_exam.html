<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【研究ノート】走る作曲家のAIカフェ - 院試</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
          <h2>目次</h2>
          <ul>
            <li><a href="#overview">概要</a></li>
            <li><a href="#strategy">戦略</a></li>
            <li><a href="#basic-math">基礎数学</a></li>
            <li><a href="#info-math">情報数学</a></li>
            <li><a href="#probability-statistics">確率・統計</a></li>
            <li><a href="#algorithms-data-structures">アルゴリズムとデータ構造</a></li>
            <li><a href="#artificial-intelligence">人工知能</a></li>
          </ul>
        </section>
        <section id="overview">
          <h2>概要</h2>
          このページでは、「北海道大学大学院情報科学院修士課程入学試験」（令和6年8月実施）の対策を行います。過去問から傾向を掴み、必要な知識や要点をまとめていきます。
        </section>
        <section id="strategy">
          <h2>戦略</h2>
          院試は以下の内容で構成されます。
          <ul>
            <li>専門科目1: 基礎数学、情報数学、【選択】確率・統計、【選択】情報理論（4問のうち基礎数学と情報数学を含む3問を解答）</li>
            <li>専門科目2: アルゴリズムとデータ構造、人工知能、コンピュータシステム、応用数学（4問のうち2問を選択し解答）</li>
          </ul>
          俗に、専門科目1では「情報理論」が、専門科目2では「アルゴリズムとデータ構造」と「コンピュータシステム」が簡単だと言われています。
          しかし、今後の自分の研究に役立てたいので、専門科目1では「確率・統計」を、専門科目2では「アルゴリズムとデータ構造」と「人工知能」を選択しようと思います。
        </section>
        <section id="basic-math">
          <h2>基礎数学</h2>
            <h3>線形写像</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://www.momoyama-usagi.com/entry/math-linear-algebra13#1-3">うさぎでもわかる線形代数　第13羽　線形写像（後編）　核空間・像空間　線形写像の全射・単射について _ 工業大学生ももやまのうさぎ塾.html</a></p>
            <ul>
                <li>集合\(V\)から集合\(W\)への写像があるとする。</li>
                <ul>
                    <li><b>全射</b>：\(W\)の要素すべてが\(V\)のどれかの要素と対応しているような写像。</li>
                    <li><b>単射</b>：\(W\)のある要素に写すための\(V\)の要素は１つしかない（２つ以上存在しない）写像。</li>
                    <li><b>全単射</b>：全射の条件、単射の条件の両方を満たすような写像。\(V\)の要素と\(W\)の要素は１対１の関係になる。</li>
                </ul>
                <li>\(f(x) = Ax\)を満たす行列\(A\)（表現行列）が正則であることと、\(f\)が全単射であることは同値である。</li>
            </ul>
            <h3>線形独立・線形従属</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://www.momoyama-usagi.com/entry/math-linear-algebra06#google_vignette">うさぎでもわかる線形代数　第06羽　1次独立・1次従属 _ 工業大学生ももやまのうさぎ塾.html</a></p>
            <ul>
                <li>線形独立</li>
                <ul>
                    <li>ベクトルの集合が線形独立であるとは、集合内のどのベクトルも他のベクトルの線形結合として表せないことを意味する。
                        具体的には、ベクトル\(v_{1}, v_{2}, ..., v_{n}\)が線形独立であるとは、次の方程式が成り立つときである：
                        <div class="scroll">
                        \begin{align}
                        c_{1} v_{1} + c_{2} v_{2} + ... + c_{n} v_{n} = 0
                        \end{align}
                        </div>
                    </li>
                    <li>ここで、すべてのスカラー\(c_{i}\) が0である場合に限り成り立つ。つまり、
                        <div class="scroll">
                        \begin{align}
                        c_{1} = c_{2} = ... = c_{n} = 0
                        \end{align}
                        </div>
                    </li>
                </ul>
                <li>線形従属</li>
                <ul>
                    <li>一方、ベクトルの集合が線形従属であるとは、少なくとも一つのベクトルが他のベクトルの線形結合として表せることを意味する。
                        つまり、ベクトル\(v_{1}, v_{2}, ..., v_{n}\)が線形従属であるとは、次の方程式が成り立つときである：
                        <div class="scroll">
                        \begin{align}
                        c_{1} v_{1} + c_{2} v_{2} + ... + c_{n} v_{n} = 0
                        \end{align}
                        </div>
                    </li>
                    <li>ここで、少なくとも一つの\(c_{i}\)が0ではない（つまり、非自明な解が存在する）場合である。</li>
                </ul>
                <li>具体例</li>
                <div class="example">
                    <b>線形独立の例</b>
                    <p>ベクトル\(v_{1} = (1, 0)\) と \(v_{2} = (0, 1)\) は線形独立である。なぜなら、</p>
                        <div class="scroll">
                        \begin{align}
                        c_{1} (1, 0) + c_{2} (0, 1) = (0, 0)
                        \end{align}
                        </div>                  
                    <p>が成り立つのは \(c_{1} = 0\)かつ \(c_{2} = 0\) のときだけだからである。</p>
                </div>
                <div class="example">
                    <b>線形従属の例</b>
                    <p>ベクトル\(v_{1} = (1, 2)\) と \(v_{2} = (2, 4)\)は線形従属である。なぜなら、</p>
                        <div class="scroll">
                        \begin{align}
                        2 (1, 2) - 1 (2, 4) = (0, 0)
                        \end{align}
                        </div>                      
                    <p>という関係があり、非自明な解 \(c_{1} = 2, c_{2} = -1\) が存在するからである。</p>
                </div>                
            </ul>
            <h3>二変数間の停留点</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://pictblog.com/twoparadiffer">【微積分】二変数関数の停留点(極大点・極小点・鞍点).html</a></p>
            <ul>
                <li>二変数関数\(f(x, y)\)の停留点\((a_x, a_y)\)を求めることを考える。</li>
                <li>関数の変化がなくなる点が停留点であるため、停留点では\(x\)方向、\(y\)方向ともにその変化量が0、すなわち
                    <div class="scroll">
                    \begin{align}
                    \frac{\partial f(x,y)}{\partial x}=\frac{\partial f(x,y)}{\partial y}=0
                    \end{align}
                    </div>
                を満たす点が停留点となる。</li>
                <li>停留点\((a_x, a_y)\)を求めたら、次はその種類をヘッセ行列により判定する。</li>
                <li>ヘッセ行列は次の式で表される。
                    <div class="scroll">
                    \begin{align}
                    \mathsf{H}=
                        \begin{pmatrix}\displaystyle{\frac{\partial^{2} f(x,y)}{\partial x^{2}}}&\displaystyle{\frac{\partial^{2} f(x,y)}{\partial x\partial y}} \\ \\ 
                        \displaystyle{\frac{\partial^{2} f(x,y)}{\partial x\partial y}}&\displaystyle{\frac{\partial^{2} f(x,y)}{\partial y^{2}}}\end{pmatrix}
                    \end{align}
                    </div>
                </li>
                <li>ヘッセ行列に停留点の座標\((a_x, a_y)\)を代入する。
                    <div class="scroll">
                    \begin{align}
                    \mathsf{H}=
                        \mathsf{H}_{(a_{x},a_{y})}
                        =\begin{pmatrix}\displaystyle{\left.\frac{\partial^{2} f(x,y)}{\partial x^{2}}\right|_{x=a_{x},y=a_{y}}}
                        &\displaystyle{\left.\frac{\partial^{2} f(x,y)}{\partial x\partial y}\right|_{x=a_{x},y=a_{y}}} \\ \\ 
                        \displaystyle{\left.\frac{\partial^{2} f(x,y)}{\partial x\partial y}\right|_{x=a_{x},y=a_{y}}}
                        &\displaystyle{\left.\frac{\partial^{2} f(x,y)}{\partial y^{2}}\right|_{x=a_{x},y=a_{y}}}\end{pmatrix}
                    \end{align}
                    </div>
                </li>
                <li>この行列の固有値を\(\lambda_{1},\lambda_{2}\)とすると、次のように停留点の種類を判別できる。
                    <div class="scroll">
                    \begin{align}
                    \begin{cases}
                        \lambda_{1}>0,\lambda_{2}>0&\to\,\text{極小点}\\ 
                        \lambda_{1}<0,\lambda_{2}<0&\to\,\text{極大点}\\ 
                        \lambda_{1}\lambda_{2}<0&\to\,\text{鞍点} 
                    \end{cases}
                    \end{align}
                    </div>
                </li>
                <li>具体例</li>
                <div class="example">
                    <b>\( f(x, y) = -3x^2 + 2xy - 4y^3 \)の停留点を調べる。</b>
                    <p>一階偏微分を求める。</p>
                        <div class="scroll">
                        \begin{align}
                        f_x = \frac{\partial f}{\partial x} = -6x + 2y \\
                        f_y = \frac{\partial f}{\partial y} = 2x - 12y^2
                        \end{align}
                        </div>
                    <p>これらの一階偏微分を0に設定して連立方程式を解く。</p>
                        <div class="scroll">
                        \begin{align}
                        -6x + 2y = 0 \quad \Rightarrow \quad y = 3x \\
                        2x - 12(3x)^2 = 0 \quad \Rightarrow \quad 2x - 108x^2 = 0 \quad \Rightarrow \quad x(2 - 108x) = 0
                        \end{align}
                        </div>
                    <p>よって、 \( x = 0 \) または \( x = \frac{1}{54} \) である。</p>
                    <p>それぞれの場合に対応する \( y \) を求める。</p>
                        <div class="scroll">
                        \begin{align}
                        x = 0 \quad \Rightarrow \quad y = 0 \\
                        x = \frac{1}{54} \quad \Rightarrow \quad y = \frac{1}{18}
                        \end{align}
                        </div>
                    <p>停留点は \( (0, 0) \) と \( \left( \frac{1}{54}, \frac{1}{18} \right) \) である。</p>
                    <p>次に、二階偏微分を求める。</p>
                        <div class="scroll">
                        \begin{align}
                        f_{xx} = \frac{\partial^2 f}{\partial x^2} = -6 \\
                        f_{xy} = \frac{\partial^2 f}{\partial x \partial y} = 2 \\
                        f_{yy} = \frac{\partial^2 f}{\partial y^2} = -24y
                        \end{align}
                        </div>
                    <p>ヘッセ行列 \( H \) は以下の通りである：</p>
                        <div class="scroll">
                        \begin{align}
                        H = \begin{bmatrix}
                            f_{xx} & f_{xy} \\
                            f_{xy} & f_{yy}
                        \end{bmatrix}
                        = \begin{bmatrix}
                            -6 & 2 \\
                            2 & -24y
                        \end{bmatrix}
                        \end{align}
                        </div>
                    <p>ヘッセ行列の判別式 \( D \) を求める。</p>
                        <div class="scroll">
                        \begin{align}
                        D = f_{xx} f_{yy} - (f_{xy})^2
                        \end{align}
                        </div>
                    <p>停留点 \( (0, 0) \) の場合、</p>
                        <div class="scroll">
                        \begin{align}
                        f_{xx}(0, 0) = -6, \quad f_{yy}(0, 0) = 0, \quad f_{xy}(0, 0) = 2 \\
                        D(0, 0) = (-6)(0) - (2)^2 = -4
                        \end{align}
                        </div>
                    <p>\( D < 0 \) なので、 \( (0, 0) \) は鞍点である。</p>
                    <p>停留点 \( \left( \frac{1}{54}, \frac{1}{18} \right) \) の場合</p>
                        <div class="scroll">
                        \begin{align}
                        f_{xx}\left( \frac{1}{54}, \frac{1}{18} \right) = -6, \quad f_{yy}\left( \frac{1}{54}, \frac{1}{18} \right) = -\frac{4}{3}, \quad f_{xy}\left( \frac{1}{54}, \frac{1}{18} \right) = 2 \\
                        D\left( \frac{1}{54}, \frac{1}{18} \right) = (-6) \left( -\frac{4}{3} \right) - (2)^2 = 8 - 4 = 4
                        \end{align}
                        </div>
                    <p>\( D > 0 \) かつ \( f_{xx} < 0 \) なので、 \( \left( \frac{1}{54}, \frac{1}{18} \right) \) は極大点である。</p>
                </div>
            </ul>
            <h3>極座標変換</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://www.momoyama-usagi.com/entry/math-analysis25">うさぎでもわかる解析　Part25　極座標変換を用いた2重積分の求め方 _ 工業大学生ももやまのうさぎ塾.html</a></p>
            <ul>
                <li>\(x\)と\(y\)を次のようにおく。
                    <div class="scroll">
                    \begin{align}
                    x = r \cos \theta , \ \ \ y = r \sin \theta \\ \left( r \geqq 0, \ \ 0 \leqq \theta \leqq 2 \pi \right)
                    \end{align}
                    </div>
                </li>
                <li>このとき、ヤコビアンを計算すると以下のようになる。
                    <div class="scroll">
                    \begin{align}
                    J = & \left| \begin{array}{ccc} \frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\ \frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta} \end{array} \right|
                        \\ = & \left| \begin{array}{ccc} \cos \theta & - r \sin \theta \\ \sin \theta & r \cos \theta \end{array} \right|
                        \\ = & \ r \left( \cos^2 \theta + \sin^2 \theta \right)
                        \\ = & \ r
                    \end{align}
                    </div>
                </li>
                <li>よって、\(dxdy = r \ dr d \theta\)となる。</li>
                <li>具体例</li>
                <div class="example">
                    <b>\(\int _{-1}^1\int _{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}e^{\sqrt{x^2+y^2}}dydx\)を計算する。</b>
                    <p>極座標変換を行う。</p>
                        <div class="scroll">
                        \begin{align}
                        \int _{-1}^1\int _{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}e^{\sqrt{x^2+y^2}}dydx = \int _{0}^{2\pi}\int _{0}^{1}e^{r}rdrd\theta
                        \end{align}
                        </div>                  
                    <p>内側の積分をまず計算する。</p>
                        <div class="scroll">
                        \begin{align}
                        \int _{0}^{1}re^{r}dr
                        \end{align}
                        </div>        
                    <p>この積分は部分積分を用いて解く。<br>部分積分の公式： \(\int u dv = uv - \int v du\)</p>
                    <ul>
                        <li>\(u = r \Rightarrow du = dr\)</li>
                        <li>\(dv = e^r dr \Rightarrow v = e^r\)</li>
                    </ul>
                        <div class="scroll">
                        \begin{align}
                        \int r e^r dr = r e^r - \int e^r dr = r e^r - e^r
                        \end{align}
                        </div> 
                    <p>これを\(0\)から\(1\)の範囲で評価する：</p>
                        <div class="scroll">
                        \begin{align}
                        \left[ r e^r - e^r \right]_{0}^{1} = (1 e^1 - e^1) - (0 e^0 - e^0) = (e - e) - (0 - 1) = 1
                        \end{align}
                        </div> 
                    <p>したがって、内側の積分の結果は \(1\) である。</p>
                    <p>次に外側の積分を計算する：</p>
                        <div class="scroll">
                        \begin{align}
                        \int _{0}^{2\pi} 1 d\theta = \int _{0}^{2\pi} d\theta = 2\pi
                        \end{align}
                        </div> 
                    <p>したがって、元の2重積分の結果は：</p>
                        <div class="scroll">
                        \begin{align}
                        \int _{-1}^1\int _{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}e^{\sqrt{x^2+y^2}}dydx = 2\pi
                        \end{align}
                        </div> 
                </div>
            </ul>
        </section>
        <section id="info-math">
          <h2>情報数学</h2>
            <h3>形式言語</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://xtech.nikkei.com/it/article/COLUMN/20070618/275020/">第2回　形式言語とオートマトン---「文」のルールを知り，機械に解釈させる _ 日経クロステック（xTECH）.html</a></p>
            <ul>
                <li><b>正規言語 (Regular Language)</b></li>
                <ul>
                    <li>正規言語は、有限オートマトン（有限状態機械）によって認識される言語である。</li>
                    <li>正規文法によって生成される。</li>
                    <li>決定性有限オートマトン（DFA）や非決定性有限オートマトン（NFA）で認識可能である。</li>
                    <li>\(a \rightarrow b\)において、\(b\)が終端記号、または終端記号の後に非終端記号を付ける。</li>
                    <li>例：\(L = { a^n | n ≥ 0 }\)(空文字列または任意の個数の \(a\) からなる言語。)</li>
                    <li>例：\(L = (ab)^*\)(\(ab\) の繰り返しからなる言語。)</li>
                </ul>
                <li><b>文脈自由言語 (Context-Free Language, CFL)</b></li>
                <ul>
                    <li>文脈自由言語は、文脈自由文法（CFG）によって生成される言語である。</li>
                    <li>文脈自由文法は、1つの非終端記号が文字列に置き換えられる生成規則を持つ。</li>
                    <li>プッシュダウンオートマトン（PDA）で認識可能である。</li>
                    <li>スタックを使用して構造を保持するため、より複雑な構造を認識できる。</li>
                    <li>\(a \rightarrow b\)において、\(a\)が非終端記号１つだけから成る。</li>
                    <li>例：\(L = { a^n b^n | n ≥ 0 }\)(同じ数の \(a\) と \(b\) からなる言語。)</li>
                    <li>例：\(ww^R | w ∈ {a, b}^*\)(任意の文字列 \(w\) とその逆順 \(w^R\) からなる言語。)</li>
                </ul>
                <li><b>文脈依存言語 (Context-Sensitive Language, CSL)</b></li>
                <ul>
                    <li>文脈依存言語は、文脈依存文法（CSG）によって生成される言語である。</li>
                    <li>文脈依存文法の生成規則は、形式 \(α A β → α γ β\) を持ち、ここで \(A\) は非終端記号、\(α\)、\(β\)、および \(γ\) は任意の文字列である。</li>
                    <li>線形有界オートマトン（LBA）で認識可能である。</li>
                    <li>メモリが有限で、入力の長さに比例して制約される。</li>
                    <li>\(a \rightarrow b\)において、\(b\)の長さが\(a\)の長さ以上である。</li>
                    <li>例：\(L = { a^n b^n c^n | n ≥ 1 }\)(同じ数の \(a, b, c\) からなる言語。)</li>
                </ul>
                <li><b>クリーネ閉包</b></li>
                <ul>
                    <li>アルファベット \(Σ = {a, b}\) に対して、そのクリーネ閉包（Kleene closure）、またはクリーネスター（Kleene star）とは、\(Σ\) から生成されるすべての有限長の文字列の集合を指す。</li>
                    <li>記号としては \(Σ*\) と表現される。</li>
                    <li>具体的には、\(Σ*\) は以下のようなすべての文字列を含む集合である：
                        <ul>
                            <li>空文字列\(ε\)</li>
                            <li>\(a\) と \(b\) の任意の組み合わせ</li>
                            <li>文字列の長さはゼロ以上</li>
                        </ul>
                    </li>
                    <li>つまり、\(Σ*\) は次のような集合である：
                        <ul>
                            <li>\(ε\)（空文字列）</li>
                            <li>長さ1の文字列： \(a, b\)</li>
                            <li>長さ2の文字列： \(aa, ab, ba, bb\)</li>
                            <li>長さ3の文字列： 
                                <div class="scroll">
                                \begin{align}
                                aaa, aab, aba, abb, baa, bab, bba, bbb
                                \end{align}
                                </div> 
                            </li>
                            <li>など、任意の有限長のすべての組み合わせ</li>
                        </ul>
                    </li>
                </ul>
                <li><b>ポンピング定理</b></li>
                <ul>
                    <li>任意の正規言語 \(L\) について、あるポンピング長 \(p\)（自然数）が存在して、\(L\) のすべての文字列 \(s\) が以下の条件を満たす場合に限り、\(s \in L\) である：
                        <ul>
                            <li>\(s\) の長さは \(p\) 以上である。</li>
                            <li>\(s\) は \(s = xyz\) という形に分割できる。</li>
                            <li>\(|xy| \leq p\)。</li>
                            <li>\(y \neq \epsilon\)。</li>
                            <li>任意の非負整数 \(i\) に対して、文字列 \(xy^iz \in L\) である。</li>
                        </ul>
                    </li>
                    <li>具体例</li>
                    <div class="example">
                        <b>言語 \(L = \{0^n 1^n \mid n \geq 0\}\) が正規言語でないことをポンピング定理を使って証明する。</b>
                        <p>仮に、言語 \(L = \{0^n 1^n \mid n \geq 0\}\) が正規言語であると仮定する。すると、ポンピング長 \(p\) が存在するはずである。</p>
                        <ol>
                            <li>\(L\) から \(s\) を選ぶ。ここで、\(s = 0^p 1^p\) とする。この文字列 \(s\) の長さは \(2p\) で、ポンピング補題の条件を満たす。</li>
                            <li>ポンピング補題に従い、\(s\) を \(s = xyz\) の形に分割する。ここで、\(|xy| \leq p\) であり、\(y \neq \epsilon\) である。</li>
                        </ol>
                        <p>\(|xy| \leq p\) なので、\(x\) と \(y\) はすべて \(0\) から構成される。具体的には、\(x = 0^a\)、\(y = 0^b\)、\(z = 0^{p-a-b} 1^p\) という形になる。ここで、\(a\) と \(b\) は非負整数であり、\(a + b \leq p\)、かつ \(b > 0\) である。</p>
                    
                        <ol start="3">
                            <li>ポンピング補題により、任意の非負整数 \(i\) に対して、文字列 \(xy^iz\) が \(L\) に属するはずである。</li>
                        </ol>
                        <p>しかし、\(i \neq 1\) の場合を考える。</p>
                        <ul>
                            <li>\(i = 0\) のとき、\(xy^0z = xz = 0^a 0^{p-a-b} 1^p = 0^{p-b} 1^p\) である。ここで、\(b > 0\) なので、この文字列は \(0\) の数と \(1\) の数が一致せず、\(L\) に属さない。</li>
                            <li>\(i = 2\) のとき、\(xy^2z = xyyz = 0^a 0^{2b} 0^{p-a-b} 1^p = 0^{p+b} 1^p\) である。ここでも、\(b > 0\) なので、この文字列も \(0\) の数と \(1\) の数が一致せず、\(L\) に属さない。</li>
                        </ul>
                        <p>これらの例から、\(xy^iz\) が \(L\) に属さない \(i\) が存在することが示される。したがって、ポンピング補題の条件を満たさないため、仮定に矛盾する。</p>
                        <p>したがって、言語 \(L = \{0^n 1^n \mid n \geq 0\}\) は正規言語ではないことが証明された。</p>
                    </div>
                </ul>
            </ul>
        </section>
        <section id="probability-statistics">
          <h2>確率・統計</h2>
            <p>参考にした本は以下のとおり。</p>
            <p><a href="https://www.amazon.co.jp/%E6%94%B9%E8%A8%82%E7%89%88-%E6%97%A5%E6%9C%AC%E7%B5%B1%E8%A8%88%E5%AD%A6%E4%BC%9A%E5%85%AC%E5%BC%8F%E8%AA%8D%E5%AE%9A-%E7%B5%B1%E8%A8%88%E6%A4%9C%E5%AE%9A2%E7%B4%9A%E5%AF%BE%E5%BF%9C-%E7%B5%B1%E8%A8%88%E5%AD%A6%E5%9F%BA%E7%A4%8E-%E7%94%B0%E4%B8%AD%E8%B1%8A/dp/4489022271/ref=sr_1_3?adgrpid=118645167349&dib=eyJ2IjoiMSJ9.tqheIVwzNsAlWRtDMXpk4NQuS-Y3KADO2zuJVrYBspygQ-wuTir-4Zh8ZccMsf6hif_83PZ9kySPkkoITDdWUa6ipD1MaPRi0Pvb8Fsxyoen0-IcBaseJIM5lZbG_2f-Bmy9i4NWn4GJlyD2nWgtvjYngUr26gaISrg2sSB7CBeIDGkgl9iHQpriMgPZCYevDn16ax_7dg_j2YfDOTEsEUUnGkaOOcuYJtUGSL1REs-viAjaz9CfdZaetJeMn8edfB5RF27GSp4DK5Im-q-9yKoe3FTymhvb6z2cCOblnC0.AYS8Y7i56awkZav04EPeM51WBXaA4QbTF7kHfUD_MQE&dib_tag=se&hvadid=666001842902&hvdev=c&hvqmt=b&hvtargid=kwd-536488505477&hydadcr=27299_14701689&jp-ad-ap=0&keywords=%E7%B5%B1%E8%A8%88%E6%A4%9C%E5%AE%9A2%E7%B4%9A&qid=1722681801&sr=8-3">改訂版　日本統計学会公式認定　統計検定２級対応　統計学基礎</a></p>
            <h3>チェビシェフの不等式</h3>
            <ul>
                <li>期待値\(\mu\)、分散\(\sigma\)をもつ確率分布に従う確率変数\(X\)があるとする。</li>
                <li>このとき、任意の\(k\)に対して次の不等式が成り立つ。
                    <div class="scroll">
                    \begin{align}
                    P(|X-\mu| \geq k\sigma) \leq 1/k^2
                    \end{align}
                    </div>
                </li>
                <li>\(\epsilon = k\sigma\)とおくと、次の式に変形できる。
                    <div class="scroll">
                    \begin{align}
                    P(|X-\mu|\geq\epsilon)\leq\frac{\sigma^2}{\epsilon^2} \\
                    P(|X-\mu|<\epsilon)\geq\frac{1-\sigma^2}{\epsilon^2}
                    \end{align}
                    </div>
                </li>
            </ul>
            <h3>大数の法則</h3>
            <ul>
                <li>大数の法則は多数回の試行の結果として得られたデータの平均（標本平均）や相対度数が、試行回数\(n\)を大きくするとき、確率分布の平均（母平均）や生起確率に近づくことを保証する理論的根拠となっている定理である。</li>
                <li>\(X_1, X_2, ..., X_n\)が互いに独立に平均\(\mu\)、分散\(\sigma^2\)の同一の確率分布に従うとする。</li>
                <li>このとき、平均\(\overline{X}=(X_1+X_2+...+X_n)/n\)の期待値と分散は\(E[\overline{X}]=\mu, V[\overline{X}]=\frac{\sigma^2}{n}\)となる。</li>
                <li>\(\overline{X}\)の場合についてチェビシェフの不等式を求めると
                    <div class="scroll">
                    \begin{align}
                    P(|\overline{X}-\mu|<\epsilon)\geq\frac{1-\sigma^2}{n\epsilon^2}
                    \end{align}
                    </div>
                となり、\(n\)を大きくするとき次の性質を持つことがいえる。</li>
                <li><b>定理（大数の弱法則）</b></li>
                <ul>
                    <li>\(X_1, X_2, ..., X_n\)が互いに独立に平均\(\mu\)、分散\(\sigma^2\)の同一の確率分布に従うとき、任意の\(\epsilon\)に対して
                        <div class="scroll">
                        \begin{align}
                        \lim_{n \to \infty} P(|\overline{X}-\mu|<\epsilon)=1
                        \end{align}
                        </div>
                    が成り立つ。</li>
                    <li>これを標本平均\overline{X}は母平均\(\mu\)に確率収束するといい、\(\overline{X} \xrightarrow{P} \mu\)と表す。</li>
                    <li>これより、標本平均\overline{X}は\(n\)を無限に大きくするとき期待値（母平均）\(\mu\)に近づくことがわかる。</li>
                </ul>
            </ul>
        </section>
        <section id="algorithms-data-structures">
          <h2>アルゴリズムとデータ構造</h2>
            <h3>計算量、オーダー表記</h3>
            <p>参考にしたサイトは以下のとおり。</p>
            <p><a href="https://www.momoyama-usagi.com/entry/calc-order">プログラムの計算量、オーダー表記 O( ) の求め方のまとめ _ 工業大学生ももやまのうさぎ塾.html</a></p>
            <ul>
                <li>項の強さを弱い順に上から並べると、以下のようになる。</li>
                <ul>
                    <li>\(logn\)</li>
                    <li>\(\sqrt{n}\)</li>
                    <li>\(n\)</li>
                    <li>\(nlogn\)</li>
                    <li>\(n^2\)</li>
                    <li>\(n^3\)</li>
                    <li>\(2^n\)</li>
                    <li>\(n!\)</li>
                </ul>
            </ul>
            
        </section>
        <section id="artificial-intelligence">
          <h2>人工知能</h2>
            <h3>概説1</h3>
            <ul>
                  <li>近年のAI（人工知能）技術の発展は目を見張るものがある。</li>
                  <li>ここでは、このAI技術の発展を特に、言語・知識系の研究分野と、その成果を応用した対話システムに注目して概観する。</li>
                  <li>ニューラルネットワーク中のどの部分（特定の単語など）に注目するかを動的に決定する<b>アテンション機構</b>が考案され、言語の出力系列生成の品質向上につながった。</li>
                  <li>ニューラルネットワーク中のどの部分（特定の単語など）に注目するかを動的に決定する<b>アテンション機構</b>が考案され、言語の出力系列生成の品質向上につながった。</li>
                  <li>この<b>アテンション機構</b>を最大限に生かした新しい深層学習モデルとして、2017 年に<b>トランスフォーマー</b>が Google から発表された。</li>
                  <li><b>トランスフォーマー</b>は、RNN(Recurrent Neural Network)やCNN(Convolutional Neural Network)を使わずに、<b>アテンション機構</b>で構成した深層学習モデルである。</li>
                  <li>RNNやCNNより計算量が抑えられ、訓練が容易で、並列処理もしやすく、複数の言語現象を効率良く扱えて、文章中の長距離の<b>依存関係</b>も考慮しやすいといった特長を持つ。</li>
                  <li>ニューラルネットワークを用いた自然言語処理で高い精度を達成するには、大量の訓練データが必要だが、さまざまなタスクのおのおのについて大量の訓練データを用意することは容易なことではない。</li>
                  <li>そこでまず、さまざまなタスクに共通的な汎用性の高いモデルを大量のラベルなしデータで<b>事前学習(Pre-Training)</b>しておき、それをベースに個別のタスクごとに少量のラベル付きデータでの<b>追加学習(Fine Tuning)</b>を行うというアプローチが取られるようになった。</li>
                  <li>この<b>事前学習(Pre-Training)</b>で作られた<b>トランスフォーマー</b>型の深層学習モデルは、2018年にGoogle から発表された<b>BERT</b>以降、自然言語処理においてスタンダードになった。</li>
                  <li>言語モデルの規模を表すパラメータ数は、<b>BERT</b>の場合、3.4 億個であったが、2020年にOpenAIから発表された<b>GPT-3(Generative Pretrained Transformer)</b>では、事前学習に45TB のデータを用い、モデルのパラメータ数は1750億個となった。</li>
                  <li>これらは大規模なパラメータを持つことから<b>大規模言語モデル(Large Language Model: LLM)</b>と呼ばれるが、高い汎用性を示すことから<b>基盤モデル(Foundation Model)</b>とも呼ばれるようになった。</li>
                  <li>また、<b>GPT-3(Generative Pretrained Transformer)</b>においては、それまでのGPTアーキテクチャと同様に後続の系列を予測する自己回帰型の自己教師あり学習が用いられた。</li>
                  <li>タスクごとの<b>追加学習(Fine Tuning)</b>をせずとも、最初に入力する系列にタスクの記述や事例を含めることを意味する<b>プロンプト</b>により複数のタスクに対応することを<b>ゼロショット</b>と呼び、言語モデルの汎用的な活用が開拓された。</li>
                  <li>さらに、2022 年 11 月末に OpenAI から<b>ChatGPT</b>がWeb 公開された。</li>
                  <li>OpenAI が2020 年 6月に発表したGPT-3.5 に人間のフィードバックを用いた強化学習の一つである<b>RLHF(Reinforcement Learning from Human Feedback)</b>を加え、対話システムとして<b>追加学習(Fine Tuning)</b>されたものだということである。</li>
                  <li>入力された質問に対してまるで人間が書いたかのような自然な文章で説明を返し、用途に応じたテキスト（論文・電子メール・<b>プログラムコード</b>など）の作成にも活用できる。</li>
                  <li>しかし、その性能に驚く一方、誤った内容をもっともらしく回答するケースも見られ（特に<b>計算や演繹推論</b>で間違うケースが見られる）、誤って信じてしまうリスクやそれが悪用されるリスクが懸念される。</li>
            </ul>
            <h3>概説2</h3>
            <ul>
                  <li>ディープラーニングは2012年に開催されたILSVRC (ImageNet Large Scale Visual Recognition Challenge) と呼ばれる画像認識コンテストで顕著な成果を収めたことで一躍有名になった。</li>
                  <li>ディープラーニングは、<b>ニューラルネットワーク</b>の層を3層以上に多層に積み重ね、<b>オートエンコーダ</b>などの手法を用いて<b>勾配消失問題</b>を克服したもので、<b>ジェフリー・ヒントン</b>教授の研究チームが提唱した。</li>
                  <li>現在、課題別に応じて様々な構造を持つモデルが提案されている。</li>
                  <li>例えば、画像認識によく用いられるモデルとして、<b>ResNet</b>や<b>AlexNet</b>などの<b>畳み込み</b>ネットワークが有名である。</li>
                  <li>基本的な<b>畳み込み</b>ネットワークでは、<b>畳み込み</b>層、<b>プーリング</b>層、<b>全結合</b>層を経て入力画像の認識を行う。</li>
                  <li>その他、時系列データに対して長期・短期の時間依存性を学習することができる<b>LSTM (Long Short Term Memory)</b>というモデルもある。</li>
                  <li><b>LSTM</b>は内部にループ構造を持つ<b>リカレント</b>ネットワークの応用であり、<b>忘却ゲート</b>層などを導入することで飛躍的に性能が伸びた。</li>
                  <li>応用面では、囲碁の対決においてDeepMind 社が開発した<b>AlphaGo</b>が2017年に当時世界チャンピオンであった柯潔（カ・ケツ）を打ち負かしたことなどが記憶に新しい。</li>
                  <li>このモデルは<b>方策</b>ネットワーク、<b>価値</b>ネットワークからなり、過去の棋譜をベースに<b>教師あり学習</b>を行ったあと、自己対戦による<b>強化学習</b>を行うことによって学習が行われる。</li>
                  <li>ディープラーニングの実行には膨大な計算が必要であるが、計算時間を短縮するために<b>GPU (Graphics Processing Unit)</b>を利用した<b>TensorFlow</b>や<b>PyTorch</b>などのライブラリが公開されており、これらのライブラリを使うことで効率よくモデルの開発が可能となった。</li>
            </ul>
            <h3>Q学習</h3>
            <ul>
                <li><b style="color:brown">状態遷移が有限マルコフ決定過程であるとはどういうことか。</b></li>
                <ul>
                    <li>マルコフ性を満たす環境においてエージェントが意思決定をして状態が確率的に遷移し、状態や行動が有限であるということ。</li>
                    <li>環境がマルコフ性を持つとは、過去の環境の履歴すべてが、現在の環境情報に集約されていることを指す。</li>
                </ul>
                <li><b style="color:brown">式を用いて方策\(\pi(s, a)\)の例を示せ。</b></li>
                <ul>
                    <li>
                        グリーディ方策
                        <div class="scroll">
                        \begin{align}
                        \pi(s, a) = \arg\max\limits_a Q(s, a)
                        \end{align}
                        </div>
                    </li>
                    <li>
                        ε-グリーディ方策
                        <div class="scroll">
                        \begin{align}
                        \pi(s, a) = 
                        \begin{cases} 
                        \arg\max\limits_a Q(s, a) & \text{with probability } 1 - \epsilon \\
                        \text{random action} & \text{with probability } \epsilon
                        \end{cases}
                        \end{align}
                        </div>
                    </li>
                    <li>
                        ソフトマックス方策
                        <div class="scroll">
                        \begin{align}
                        \pi(s, a) = \frac{e^{Q(s, a) / \tau}}{\sum\limits_{b} e^{Q(s, b) / \tau}}
                        \end{align}
                        </div>
                    </li>
                </ul>
                <li><b style="color:brown">エージェントが状態\(s_t\)において取った行動を\(a_t\)とするとき、行動価値\(Q(s_t, a_t)\)の更新を式で表せ。</b></li>
                <ul>
                    <li>
                        <div class="scroll">
                        \begin{align}
                        Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( r_{t+1} + \gamma \max\limits_a Q(s_{t+1}, a) - Q(s_t, a_t) \right)
                        \end{align}
                        </div>
                    </li>
                </ul>
                <li><b style="color:brown">すべての状態と行動の組が無限に繰り返されて行動価値が更新されるとき、最終的に行動価値はどのような値に収束するか。</b></li>
                <ul>
                    <li>真の最適行動価値関数に収束する。</li>
                </ul>
            </ul>
            <h3>ニューラルネットワーク</h3>
            <ul>
                <li><b style="color:brown">ニューラルネットワークの教師あり学習における学習プロセスを説明せよ。</b></li>
                <ul>
                    <li>入力層に入力信号を与えて順方向に中間層の出力を計算し、出力層からの出力を計算する。</li>
                    <li>出力層からの出力と教師信号との誤差から<b>損失関数</b>の値を計算する。</li>
                    <li><b>誤差逆伝播法</b>を用いて出力層から入力層に向かう逆方向に各ユニット間の<b>重み</b>に対する<b>勾配</b>を計算する。</li>
                    <li><b>損失関数</b>の値を最小化するために、<b>最急降下法</b>を適用して各ユニット間の<b>重み</b>を更新する。</li>
                    <li>終了条件を満たすまで以上を繰り返す。</li>
                </ul>
                <li><b style="color:brown">上記の学習プロセスにおいて更新する重みを\(w_i\)、学習率を\(\alpha\)、損失関数を\(L(\cdot)\)とするとき、偏微分を用いて最急降下法による重みの更新式を示せ。</b></li>
                <ul>
                    <li>
                        <div class="scroll">
                        \begin{align}
                        w_{i+1} &= w_i - \alpha \frac{\partial L}{\partial w_i}
                        \end{align}
                        </div>
                    </li>
                </ul>
                <li><b style="color:brown">活性化関数にシグモイド関数がよく使われるのはなぜか。</b></li>
                <ul>
                    <li>シグモイド関数は非線形な関数であり、ニューラルネットワークに複雑なパターンや関係を学習する能力を持たせることができるから。</li>
                </ul>
                <li><b style="color:brown">最急降下法は使用するサンプルの数により、バッチ最急降下法と確率的最急降下法がある。その違い、およびそれぞれメリットとデメリットを簡単に説明せよ。</b></li>
                <ul>
                    <li>バッチ最急降下法では、すべてのトレーニングデータを使用して勾配を計算するため、各ステップの勾配は安定していて、収束は滑らかだが、計算コストが高く、メモリの使用量も多い。</li>
                    <li>一方、確率的最急降下法ではトレーニングデータからランダムに選んだ1つのサンプルを用いるため、計算コストが低いが、収束が不安定である。</li>
                </ul>
            </ul>
            <h3>サポートベクターマシン</h3>
            <ul>
                <li><b style="color:brown">サポートベクターマシン（SVM）は、パターン識別用の教師あり機械学習の方法であり、局所解収束の問題がないという長所がある。
                特に、<b>マージン最大化</b>という手法で汎化能力も高め、現在知られている方法としては、優秀なパターン識別能力を持つとされている。
                また、<b>カーネルトリック</b>という巧妙な方法を用いることにより、応用範囲が格段に広がっている。</b></li>
                <li><b style="color:brown">マージン最大化について説明せよ。</b></li>
                <ul>
                    <li>SVMは、データポイントを識別するために<b>識別境界面</b>を見つける。
                        この識別境界面は、データポイント間の<b>マージン</b>を最大化するように配置される。
                        <b>マージン</b>とは、識別境界面から最も近いデータポイントまでの「距離」を指し、この距離を最大化することで、分類の信頼性が高まる。</li>
                </ul>
                <li><b style="color:brown">カーネルトリックについて説明せよ。</b></li>
                <ul>
                    <li>SVMは、データが<b>低次元</b>空間で<b>線形分離</b>できない場合、カーネルトリックを使用してデータを<b>高次元</b>空間にマッピングする。
                        これにより、<b>高次元</b>空間ではデータが<b>線形分離</b>可能となり、SVMは効果的に分類を行うことができる。</li>
                </ul>
                <li><b style="color:brown">以下の式(a)と(b)は、SVMにおいて解を得る際の目的関数と制約条件を表している。
                <div class="scroll">
                        \begin{align}
                        \text{(a) } & \min_{\mathbf{w}, w_0} \frac{1}{2} \|\mathbf{w}\|^2 \\
                        \text{(b) } & y_i (\mathbf{w} \cdot \mathbf{x}_i + w_0) \geq 1 \quad i = 1, \ldots, N
                        \end{align}
                        </div>
                ここで、太字の記号はベクトルを表し、\(N\)は訓練データ数、\(x_i\)は\(i\)番目の訓練データを表すベクトル、\(y_i\)は\(i\)番目の訓練データに与えられた正解情報（正例では1、不例では-1）、\(w\)は重みパラメータのベクトル、\(w_0\)はバイアスパラメータとする。    
                </b></li>
                <li><b style="color:brown">式(b)で等号が成り立つのはどのような場合かを\(x_i\)を用いて説明せよ。</b></li>
                <ul>
                    <li>式(b)で等号が成り立つのは、訓練データ \(\mathbf{x}_i\) がマージンの境界上にある場合である。</li>
                    <li>つまり、\(\mathbf{x}_i\) がサポートベクターであるときに成り立つ。</li>
                </ul>
            </ul>
            <h3>K近傍法</h3>
            <ul>
                <li><b style="color:brown">２クラス分類におけるK近傍法は、分類したいデータ点\(\mathbf{x}\)が与えられたとき、訓練データ中で\(\mathbf{x}\)から最も近いK個のデータ点（K近傍と呼ぶ）を探し、それらの中の多数派が属するクラスを\(\mathbf{x}\)のクラスとする。
                ２次元平面乗の５つのデータ点からなる訓練データ\(\mathbf{x}_1 = (1, 0)\), \(\mathbf{x}_2 = (1, 2)\), \(\mathbf{x}_3 = (3, 3)\), \(\mathbf{x}_4 = (4, 3)\), \(\mathbf{x}_5 = (4, 5)\)において、\(\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3\)はクラス1に、\(\mathbf{x}_4, \mathbf{x}_5\)はクラス2に属しているとする。
                新しいデータ点\(\mathbf{x} = (2, 3)\)をどちらのクラスに分類するか、\(K=3\)としたK近傍法により決定せよ。
                ただし、データ点間の距離はユークリッド距離で測ることとする。</b></li>
                <ul>
                    <li>まず、ユークリッド距離を使用して、\(\mathbf{x} = (2, 3)\) から各訓練データ点までの距離を計算する。</li>
                    <ul>
                        <div class="scroll">
                        <li>\(\mathbf{x}_1 = (1, 0)\): \(\sqrt{(2-1)^2 + (3-0)^2} = \sqrt{1 + 9} = \sqrt{10}\)</li>
                        <li>\(\mathbf{x}_2 = (1, 2)\): \(\sqrt{(2-1)^2 + (3-2)^2} = \sqrt{1 + 1} = \sqrt{2}\)</li>
                        <li>\(\mathbf{x}_3 = (3, 3)\): \(\sqrt{(2-3)^2 + (3-3)^2} = \sqrt{1 + 0} = \sqrt{1}\)</li>
                        <li>\(\mathbf{x}_4 = (4, 3)\): \(\sqrt{(2-4)^2 + (3-3)^2} = \sqrt{4 + 0} = \sqrt{4}\)</li>
                        <li>\(\mathbf{x}_5 = (4, 5)\): \(\sqrt{(2-4)^2 + (3-5)^2} = \sqrt{4 + 4} = \sqrt{8}\)</li>
                        </div>
                    </ul>
                    <li>次に、最も近いK個 (K=3) のデータ点を見つける。</li>
                    <ul>
                        <li>\(\sqrt{1}\) (最も近い) → \(\mathbf{x}_3\) (クラス1)</li>
                        <li>\(\sqrt{2}\) (次に近い) → \(\mathbf{x}_2\) (クラス1)</li>
                        <li>\(\sqrt{4}\) (3番目に近い) → \(\mathbf{x}_4\) (クラス2)</li>
                    </ul>
                    <li>多数決によって、新しいデータ点 \(\mathbf{x} = (2, 3)\) はクラス1に分類される。</li>
                </ul>
                <li><b style="color:brown">近傍のサイズKを大きくして訓練データと同数にした場合、任意の点はどのように分類されるかを考察せよ。</b></li>
                <ul>
                    <li>近傍のサイズKを訓練データと同数の5にした場合、すべての訓練データ点が考慮される。
                        この場合、クラス1のデータ点が3つ、クラス2のデータ点が2つ存在する。</li>
                    <ul>
                        <li>クラス1: \(\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3\)</li>
                        <li>クラス2: \(\mathbf{x}_4, \mathbf{x}_5\)</li>
                    </ul>
                    <li>K=5では常にクラス1のデータ点が多数派となるため、任意の新しい点はクラス1に分類される。
                        したがって、この方法では訓練データのクラス比率が分類に強く影響を与えることになる。</li>
                </ul>
                <li><b style="color:brown">K近傍法に代表されるノンパラメトリック法を、線形識別モデルに代表されるパラメトリック法と比較した場合の問題点について、必要な計算量と記憶量の点から考察せよ。</b></li>
                <ul>
                    <li>ノンパラメトリック法は、訓練時の計算量は少ないが、予測時の計算量が大きくなる。</li>
                    <li>また、ノンパラメトリック法はデータセット全体を保存する必要があるため、記憶量が大きくなる。</li>
                </ul>
            </ul>
        </section>
    </main>
</body>
</html>
