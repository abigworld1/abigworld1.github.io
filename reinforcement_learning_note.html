<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>走る作曲家のAIカフェ - 強化学習（清書版）</title>
    <link rel="stylesheet" href="style.css"> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1050827580219099"
     crossorigin="anonymous"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
            <h2>目次</h2>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#source">Source</a></li>
                <li><a href="#bandit-problem">Bandit Problem</a></li>
                <li><a href="#mdp">Markov Decision Process</a></li>
                <li><a href="#bellman-equation">Bellman Equation</a></li>
                <li><a href="#dynamic-programming">Dynamic Programming</a></li>
                <li><a href="#mc">Monte Carlo Method</a></li>
                <li><a href="#td">Temporal Difference</a></li>
            </ul>
        </section>
        <section id="overview">
            <h2>Overview</h2>
            強化学習とは
            <ul>
                <li>エージェントが環境と相互作用しながら集めたデータを使って高い報酬を得る方法を学習する</li>
                <li>エージェントが試行錯誤的に行動し、環境から与えられる報酬をもとに、期待報酬を最大化するような行動を選択するように行動を修正していく</li>
            </ul>
            という手法です。このページでは、全くの初学者である私が、強化学習についてまとめていきます。
        </section>
        <section id="source">
            <h2>Source</h2>
            以下の講義・書籍を参考にしました。
            <ul>
                <li><a href="https://www.amazon.co.jp/dp/0262039249">Reinforcement Learning, second edition: An Introduction (Adaptive Computation and Machine Learning series)</a></li>
                <ul>
                    <li><a href="http://incompleteideas.net/book/RLbook2020.pdf">Full PDF</a></li>
                </ul>
                <li><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u">Stanford CS234: Reinforcement Learning | Winter 2019</a></li>
                <li><a href="https://www.youtube.com/playlist?list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps">UC Berkeley CS 285: Deep Reinforcement Learning course | Fall 2023</a></li>
                <li><a href="https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNBPJdt8WamRAt4XKc639wF">UC Berkeley CS 287: Advanced Robotics | Fall 2019</a></li>
                <li><a href="https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ">DeepMind x UCL | Introduction to Reinforcement Learning 2015</a></li>
                <li><a href="https://weblab.t.u-tokyo.ac.jp/education/deep-reinforcement-learning/">深層強化学習 Deep Learning 応用講座 2024 Summer</a></li>
                <li><a href="https://www.amazon.co.jp/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%EF%BC%88%E7%AC%AC2%E7%89%88%EF%BC%89-R-Sutton/dp/4627826621/ref=pd_lpo_sccl_2/357-3139727-5663608?pd_rd_w=NkO82&content-id=amzn1.sym.855d8f70-df76-4181-80b0-56e48ae3bb9b&pf_rd_p=855d8f70-df76-4181-80b0-56e48ae3bb9b&pf_rd_r=PMD3GFN7FFG0MYVRX9KR&pd_rd_wg=tHSVD&pd_rd_r=57b87974-355e-4c18-ae1f-bcdf55adb96d&pd_rd_i=4627826621&psc=1#customerReviews">強化学習（第2版）</a></li>
                <li><a href="https://www.amazon.co.jp/%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8BDeep-Learning-%E2%80%95%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E7%B7%A8-%E6%96%8E%E8%97%A4-%E5%BA%B7%E6%AF%85/dp/4873119758/ref=sr_1_3?adgrpid=120952843404&dib=eyJ2IjoiMSJ9.hH2wBl7SFx37GV25E2wHXN5tkG88yeA4w0_uhCDQeAGXHennOFOA9HtC0IZoe1CJ12MDkBW19CKDw3V8W2epn6z8IWch7fF_7Pwyi6zEDUlrbfxmsgNs9jOCWG4xqL5Ukct-erC8712p4NJMeWAkLnC9f4WVhcuWRdquRpwzfwj4z7us0YWOIBZY56XeEUZOZo4nhQB5e-BXeh85Ks22-elRZBG8YkYU53x3nirZCAH7Yd4T3n8nN0MGoXI2EKoztLgR5El8GbWZOVz6wVt01ZaJENrUy8OFCaJItSusR_E.zYa2-YOpfMzxqHg0sL06bGmED9UNuzELMrLgzQ2FWCQ&dib_tag=se&hvadid=665912024283&hvdev=c&hvqmt=b&hvtargid=kwd-851214456388&hydadcr=27494_14701818&jp-ad-ap=0&keywords=%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8Bdeep-learning&qid=1721996873&sr=8-3">ゼロから作るDeep Learning ❹ ―強化学習編</a></li>
                <li><a href="https://www.amazon.co.jp/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E6%A3%AE%E6%9D%91-%E5%93%B2%E9%83%8E/dp/4065155916">強化学習 (機械学習プロフェッショナルシリーズ) </a></li>
                <li><a href="https://www.amazon.co.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-Python%E3%81%A7%E5%AD%A6%E3%81%B6%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E6%94%B9%E8%A8%82%E7%AC%AC2%E7%89%88-%E5%85%A5%E9%96%80%E3%81%8B%E3%82%89%E5%AE%9F%E8%B7%B5%E3%81%BE%E3%81%A7-%E4%B9%85%E4%BF%9D/dp/4065172519">機械学習スタートアップシリーズ Pythonで学ぶ強化学習 [改訂第2版] 入門から実践まで</a></li>
                <li><a href="https://www.amazon.co.jp/%E3%81%A4%E3%81%8F%E3%82%8A%E3%81%AA%E3%81%8C%E3%82%89%E5%AD%A6%E3%81%B6-%E6%B7%B1%E5%B1%A4%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-PyTorch%E3%81%AB%E3%82%88%E3%82%8B%E5%AE%9F%E8%B7%B5%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-%E6%A0%AA%E5%BC%8F%E4%BC%9A%E7%A4%BE%E9%9B%BB%E9%80%9A%E5%9B%BD%E9%9A%9B%E6%83%85%E5%A0%B1%E3%82%B5%E3%83%BC%E3%83%93%E3%82%B9-%E5%B0%8F%E5%B7%9D%E9%9B%84%E5%A4%AA%E9%83%8E/dp/4839965625">つくりながら学ぶ! 深層強化学習 ~PyTorchによる実践プログラミング~</a></li>
            </ul>
        </section>
        <section id="bandit-problem">
            <h2>Bandit Problem</h2>
            <ul>
                <li>強化学習の中で最もシンプルな問題の１つ。</li>
                <li>「バンディット」とは、「スロットマシン」のこと。</li>
                <li>「<b>多腕バンディット問題</b>」は、１本レバーのスロットマシンが複数台ある状況。</li>
                <li>各マシンの当たりやすさは異なるが、どれが当たりやすいかは分からない。</li>
                <li>決められた回数の中でプレイして、なるべく多く当てたい。</li>
                <li>スロットマシンは「<b>環境</b>」、プレイヤーは「<b>エージェント</b>」と呼ばれる。</li>
                <li>プレイは「<b>行動</b>」、当たったときにもらえるコインは「<b>報酬</b>」と呼ばれる。</li>
                <li>スロットマシンである「環境」の状態は変化しない。</li>
                <li>プレイしたときにもらえるコインの枚数の期待値（<b>行動価値</b>）が大きいスロットマシンを選んでプレイすればよい。</li>
                <li>行動 \(A\) に対する価値を \( q(A) = \mathbb{E}[R \mid A] \) で表す。</li>
                <li>真の行動価値を\(q(A)\)、行動価値の推定値を\(Q(A)\)とする。</li>
                <li>プレイヤーはスロットマシンの価値を事前に知ることはできないが、価値が最大のスロットマシンを選びたい。</li>
                <li>実際にプレイして、その結果から選択の善し悪しを推定する必要がある。</li>
                <li>「実際に得られた報酬の平均値」をスロットマシンの価値の推定値として考える。</li>
                <li>ある１台のスロットマシンを\(n\)回プレイして、実際に報酬\(R_1,R_2,...,R_n\)が得られたとき、行動価値の推定値は\(Q_n = \frac{R_1 + R_2 + \ldots + R_n}{n}\)となる。</li>
                <li>\(n\)回目の行動価値の推定値\(Q_n\)は、\(n\)個ある報酬の標本平均として求められ、\(Q_n = Q_{n-1} + \frac{1}{n}(R_n - Q_{n-1})\)と書き直せる。</li>
                <li>プレイヤーの戦略として、これまでにプレイした結果をもとに一番良いスロットマシンを選ぶ、という方法が考えられる。</li>
                <li>各スロットマシンの価値の推定値の中で最も大きい値のスロットマシンを選ぶというのは、greedyな方法である。</li>
                <li>greedyな方法では、様々なスロットマシンを試すことなく、同一のスロットマシンが選ばれ続ける。</li>
                <li>これまでの経験を<b>活用</b>するだけでなく、<b>探索</b>も行うことでより良いスロットマシンを見つけようとする方法の代表例が、<b>ε-greedy法</b>である。</li>
                <li>ε-greedy法では、εの確率で探索（ランダムな行動）を行い、1-εの確率で活用（greedyな行動）を行う。</li>
                <li>非定常問題（スロットマシンの勝率がプレイする度に変動する場合）では、新しく得た報酬の重みほど大きくする必要がある。</li>
                <li>標本平均を用いる場合の、行動価値の推定値の更新式は\(Q_n = Q_{n-1} + \frac{1}{n}(R_n - Q_{n-1})\)であり、すべての報酬の重みは同じである。</li>
                <li>新しく得た報酬の重みほど大きくするには、ステップサイズである\(1/n\)の代わりに、\(α\)という固定値を使えばよい。</li>
                <li>新しく得た報酬を重視する更新式は、\(Q_n = Q_{n-1} + α(R_n - Q_{n-1})\)となる。</li>
                <li>新しく得た報酬を重視する更新式を用いる場合、各報酬に対する重みが過去になるにつれて指数関数的に減少する<b>指数移動平均</b>となる。</li>
            </ul>
        </section>
        <section id="mdp">
            <h3>Markov Decision Process</h3>
            <ul>
                <li>エージェントの行動によって状況が変わる問題の一部は<b>マルコフ決定過程（MDP）</b>として定式化される。</li>
                <li>決定過程とは、エージェントが（環境と相互作用しながら）行動を決定する過程である。</li>
                <li>エージェントの行動によって変化する、エージェントが置かれる状況のことを「<b>状態</b>」と呼ぶ。</li>
                <li>エージェントは目先の報酬ではなく、将来を通して得られる報酬の総和を考える必要がある。</li>
                <li>状態\(S_t\)に基づいてエージェントが行動\(A_t\)を行い、報酬\(R_t\)を得て、次の状態である\(S_{t+1}\)へと遷移する場合を考える。</li>
                <li>決定論的な状態遷移の場合、次の状態\(s'\)は、今の状態\(s\)と行動\(a\)によって一意に決まるため、\(s' = f(s, a)\)（<b>状態遷移関数</b>）という関数の形で表せる。<</li>
                <li>確率的な状態遷移の場合、状態\(s\)にいて行動\(a\)を行い、次の状態\(s'\)に移動する確率は\(p(s'|s, a)\)（<b>状態遷移確率</b>）と表せる。</li>
                <li>これまでにどのような状態にあって、どのような行動を行ってきたかという情報を必要としない性質を<b>マルコフ性</b>という。</li>
                <li>エージェントが状態\(s\)にいて行動\(a\)を行い、次の状態が\(s'\)になったときに得られる報酬を\(r(s, a, s')\)という関数（<b>報酬関数</b>）で定義する。</li>
                <li><b>方策</b>は、エージェントがどのように行動を決定するかを表す。</li>
                <li>決定論的な方策は、関数として\(a = \mu(s)\)のように定義できる。</li>
                <li>確率的な方策は、\(\pi(a|s)\)と表せる。</li>
                <li>MDPの目標は、<b>最適方策</b>を見つけることである。</li>
                <li>MDPには、「終わり」のあるエピソードタスクと、「終わり」がない連続タスクがある。</li>
                <li>エージェントの目標は<b>収益</b>を最大にすることであり、収益は\(G_t=R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+...\)として定義される。</li>
                <li>収益はエージェントが得る報酬の和であり、\(\gamma\)は「<b>割引率</b>」と呼ばれる。</li>
                <li>収益の期待値は\(v_{\pi}(s)=\mathbb{E}[G_t|S_t=s,\pi]\)（<b>状態価値関数</b>）で表せる。</li>
                <li>方策\(\pi\)が変われば、エージェントが得る報酬も変わり、その総和である収益も変わる。</li>
                <li>状態価値関数は、\(v_{\pi}(s)=\mathbb{E_{\pi}}[G_t|S_t=s]\)として書くこともできる。</li>
                <li>\(v_{\pi}\)は真の状態価値関数であり、\(V_{\pi}\)は推定値としての状態価値関数を意味する。</li>
                <li>最適方策\(\pi_*\)は、他のどの方策と比較しても、すべての状態において状態価値関数\(v_{\pi_*}(s)\)の値が大きい方策である。</li>
                <li>MDPでは、決定論的な最適方策が少なくとも一つ存在することが保証されている。</li></li>
                <li>最適方策における状態価値関数は、<b>最適状態価値関数</b>と呼ばれ、\(v_*\)で表す。</li>
            </ul>
        </section>
        <section id="bellman-equation">
            <h3>Bellman Equation</h3>
            <ul>
                <li>収益は\(G_t=R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+...\)として定義され、以下のように式変形できる。
                    <div class="scroll">
                    \begin{align}
                    G_t &= R_t+\gamma\{R_{t+1}(\gamma R_{t+2}+...)\} \\
                    &= R_t+\gamma G_{t+1}
                    \end{align}
                    </div>
                </li>
                <li>状態価値関数は\(v_{\pi}(s)=\mathbb{E_{\pi}}[G_t|S_t=s]\)と定義されるため、以下のように書ける。
                    <div class="scroll">
                    \begin{align}
                    v_{\pi}(s) &= \mathbb{E_{\pi}}[R_t+\gamma G_{t+1}|S_t=s] \\
                    &= \mathbb{E_{\pi}}[R_t|S_t=s]+\gamma\mathbb{E_{\pi}}[G_{t+1}|S_t=s]
                    \end{align}
                    </div>
                </li>
                <li>状態が\(s\)で、エージェントが方策\(\pi(a|s)\)に従って行動し、状態遷移確率\(p(s'|s, a)\)に従って新しい状態\(s'\)に移行する場合を考える。</li>
                <li>報酬関数が\(r(s, a, s')\)のとき、期待報酬は以下のように書ける。
                    <div class="scroll">
                    \begin{align}
                    \mathbb{E_{\pi}}[R_t|S_t=s] &= \sum_{a}\sum_{s'}\pi(a|s)(p(s'|s, a)r(s, a, s')
                    \end{align}
                    </div>
                </li>
                <li>また、以下の式が成り立つ。
                    <div class="scroll">
                    \begin{align}
                    \mathbb{E}_{\pi}[G_{t+1}|S_t=s] 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s'] \\ 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)v_{\pi}(s')
                    \end{align}
                    </div>
                </li>
                <li>以上より、以下の式（<b>ベルマン方程式</b>）を導くことができる。
                    <div class="scroll">
                    \begin{align}
                    v_{\pi}(s) 
                    &= \mathbb{E}_{\pi}[R_t|S_t=s] + \gamma\mathbb{E}_{\pi}[G_{t+1}|S_t=s] \\ 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)r(s,a,s') + \gamma\sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)v_{\pi}(s') \\ 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\{r(s,a,s') + \gamma v_{\pi}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>ベルマン方程式は、「状態\(s\)の価値関数」と「その次に取り得る状態\(s'\)の価値関数」との関係性を表した式である。</li>
                <li>ベルマン方程式は、すべての状態\(s\)とすべての方策\(\pi\)について成り立つ。</li>
                <li>ベルマン方程式を使えば状態価値関数を求めることができる。</li>
                <li>状態価値関数の２条件「状態」と「方策」に、「行動」を追加すると<b>行動価値関数（Q関数）</b>になる。</li>
                <li>Q関数を数式で表すと、\(q_{\pi}(s, a) = \mathbb{E_{\pi}}[G_t|S_t=s, A_t=a]\)となる。
                <li>Q関数は、時刻\(t\)の時に状態\(s\)で行動\(a\)を取り、時刻\(t+1\)以降では方策\(\pi\)に従った行動を取る。</li>
                <li>その時に得られる収益の期待値が\(q_{\pi}(s, a)\)である。</li>
                <li>\(q_{\pi}(s, a)\)の行動\(a\)は方策\(\pi\)とは関係なく、自由に決めることができる。</li>
                <li>仮にQ関数の行動\(a\)を方策\(\pi\)に従って選ぶとすると、Q関数と状態価値関数は同じになる。</li>
                <li>収益の期待値に関して、\(v_{\pi}(s)=\sum_{a}\pi(a|s)q_{\pi}(s, a)\)が成り立つ。</li>
                <li>Q関数は以下のように展開でき、「Q関数を用いたベルマン方程式」を得る。
                    <div class="scroll">
                    \begin{align}
                    q_{\pi}(s, a) &= \mathbb{E_{\pi}}[G_t|S_t=s, A_t=a] \\
                    &= \mathbb{E_{\pi}}[R_t+\gamma G_{t+1}|S_t=s, A_t=a] \\
                    &= \sum_{s'}p(s'|s, a)r(s, a, s')+\gamma\sum_{s'}p(s'|s, a)\mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s'] \\
                    &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma v_{\pi}(s')\} \\
                    &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma\sum_{a'}\pi(a'|s')q_{\pi}(s', a')\}
                    \end{align}
                    </div>
                    </li>
                <li>ベルマン方程式は、ある方策\(\pi\)について成り立つ。</li>
                <li>最適方策に関して成り立つ方程式を<b>ベルマン最適方程式</b>と呼ぶ。</li>
                <li>状態価値関数のベルマン方程式は次の式で表される。
                    <div class="scroll">
                    \begin{align}
                        v_{\pi}(s) 
                        &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\{r(s,a,s') + \gamma v_{\pi}(s')\} \\
                        &= \sum_{a}\pi(a|s)\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma v_{\pi}(s')\}
                    \end{align}
                    </div>
                </li>    
                <li>最適方策を\(\pi_{*}(a|s)\)とした場合、次のようにベルマン方程式が成り立つ。
                    <div class="scroll">
                    \begin{align}
                        v_{*}(s) 
                        &= \sum_{a}\pi_{*}(a|s)\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma v_{*}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>最適方策は\(\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma v_{*}(s')\}\)の値が最大の行動を選び、その最大値がそのまま\(v_{*}\)になる。</li>
                <li>これを数式で表すと、次のようになる。
                    <div class="scroll">
                    \begin{align}
                        v_{*}(s) 
                        &= \max_{a}\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma v_{*}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>この式がベルマン最適方程式である。</li>
                <li>行動価値関数（Q関数）についても同様にベルマン最適方程式を求めることができる。</li>
                <li>最適方策における行動価値関数は<b>最適行動価値関数</b>と呼ばれる。</li>
                <li>Q関数のベルマン方程式は次のとおり。
                    <div class="scroll">
                    \begin{align}
                        q_{\pi}(s, a) 
                        &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma\sum_{a'}\pi(a'|s')q_{\pi}(s', a')\}
                    \end{align}
                    </div>
                </li>
                <li>この式に最適方策\(\pi_{*}\)を代入すると、以下のようになる。
                    <div class="scroll">
                    \begin{align}
                        q_{*}(s, a) 
                        &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma\sum_{a'}\pi_{*}(a'|s')q_{*}(s', a')\}
                    \end{align}
                    </div>
                </li>
                <li>状態価値関数のベルマン最適方程式と同様に展開すると、以下の式が得られる。
                    <div class="scroll">
                    \begin{align}
                        q_{*}(s, a) 
                        &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma\max_{a'}q_{*}(s', a')\}
                    \end{align}
                    </div>
                </li>
                <li>この式がQ関数に関するベルマン最適方程式である。</li>
                <li>\(\pi\)を用いずに表せるのは、MDPでは決定論的な最適方策が少なくとも一つ存在するからである。</li>
                <li>ここで、最適行動価値関数\(q_{*}(s, a)\)が分かっていると仮定すると、状態\(s\)における最適な行動は次のように求まる。
                    <div class="scroll">
                    \begin{align}
                        \mu_{*}(s) 
                        &= arg\max_{a}q_{*}(s, a)
                    \end{align}
                    </div>
                </li>
                <li>行動価値関数におけるベルマン方程式をこの式に代入すると、以下の式が得られる。
                    <div class="scroll">
                    \begin{align}
                        \mu_{*}(s) 
                        &= arg\max_{a}\sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma v_{*}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>このように、最適状態価値関数\(v_{*}(s)\)を使って、最適方策\(\mu_{*}(s)\)を得ることができる。</li>
                <li>最適な行動価値関数を最大とするような行動であるため、「greedyな方策」といえる。</li>
            </ul>
        </section>
        <section id="dynamic-programming">
            <h2>Dynamic Programming</h2>
            <ul>
                <li>動的計画法を使えば、状態と行動の数がある程度大きくなっても価値関数を評価することができる。</li>
                <li>強化学習の問題は、多くの場合、<b>方策評価</b>と<b>方策制御</b>の２つのタスクに取り組むことになる。</li>
                <li>方策評価とは、ある方策\(\pi\)が与えられたときに、その方策の価値関数\(v_{\pi}(s)\)や\(q_{\pi}(s, a)\)を求めることである。</li>
                <li>方策制御とは、方策を制御して最適方策へと調整することである。</li>
                <li>動的計画法では、方策評価を行う。</li>
                <li>価値関数の定義は次のとおり。
                    <div class="scroll">
                    \begin{align}
                    v_{\pi}(s) &= \mathbb{E_{\pi}}[R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+...|S_t=s]
                    \end{align}
                    </div>
                </li>
                <li>無限を含む期待値の計算は不可能なので、次のベルマン方程式によって解決する。
                    <div class="scroll">
                    \begin{align}
                    v_{\pi}(s) 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\{r(s,a,s') + \gamma v_{\pi}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>ベルマン方程式は、上式のように「現在の状態\(s\)の価値関数\(v_{\pi}(s)\)」と「次の状態\(s'\)の価値関数\(v_{\pi}(s')\)」との関係性を表す。</li>
                <li>DPでは、ベルマン方程式を以下のように「更新式」へと変形する。
                    <div class="scroll">
                    \begin{align}
                    V_{k+1}(s) 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\{r(s,a,s') + \gamma V_{k}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>\(V_{k}(s)\)は\(k\)回目に更新された価値関数、\(V_{k+1}(s)\)は\(k+1\)回目に更新された価値関数であり、共に「推定値」である。</li>
                <li>この式の特徴は、「次に取り得る状態の価値関数\(V_{k}(s')\)」を使って、「今いる状態の価値関数\(V_{k+1}(s)\)」を更新することである。</li>
                <li>ここで行っていることは「推定値\(V_{k}(s')\)」を使って「別の推定値\(V_{k+1}(s)\)」を改善することである。</li>
                <li>推定値を使って推定値を改善するプロセスのことを、<b>ブートストラッピング</b>という。</li>
                <li>DPでは、まず、\(V_{0}(s)\)の初期値を設定し、先に示した更新式によって\(V_{0}(s)\)から\(V_{1}(s)\)へと更新する。</li>
                <li>この更新を繰り返し行うことで、最終的なゴールである\(V_{\pi}(s)\)へと近づけるアルゴリズムを、<b>反復方策評価</b>と呼ぶ。</li>
                <li>反復方策評価では、\(V_{\pi}(s)\)への収束が証明されている。</li>
                <li>DPのエッセンスは「同じ計算を二度しない」ことであり、その実現方法には、「トップダウン方式（メモ化）」と「ボトムアップ方式」がある。</li>
                <li>今回の\(V_{0}(s), V_{1}(s), ...\)と1つずつ繰り上げながら価値関数を更新する手法は「ボトムアップ方式」である。</li>
                <li>ゴールにおける価値関数の値は常に0である。</li>
                <li>DPを使うことで、効率的に方策の評価を行うことができる。</li>
                <li>目標の最適方策は、ベルマン最適方程式を満たす連立方程式を解くことで求められるが、計算量的に問題がある。</li>
                <li>具体的には、状態のサイズを\(S\)、行動のサイズを\(A\)としたとき、解を求めるために\(A^S\)のオーダーの計算量が必要になる。</li>
                <li>DPの反復方策評価によって方策を評価できたので、あとは方策の「改善」ができたらよい。</li>
                <li>最適方策は以下の式で表される。
                    <div class="scroll">
                    \begin{align}
                        \mu_{*}(s)
                        &= arg\max_{a}q_{*}(s, a) \\
                        &= arg\max_{a}\sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma v_{*}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>最適方策は最大値をとる行動\(a\)によって決まるため、この式によって得られる方策は「greedyな方策」と呼ばれる。</li>
                <li>最適価値関数\(v_{*}\)がわかれば、最適方策\(\mu_{*}\)が求まるが、\(v_{*}\)を知るには\(\mu_{*}\)が必要である。</li>
                <li>「何らかの決定論的方策\(\mu\)」に対して上式を適用すると、以下のようになる。
                    <div class="scroll">
                    \begin{align}
                        \mu'(s)
                        &= arg\max_{a}q_{\mu}(s, a) \\
                        &= arg\max_{a}\sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma v_{\mu}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>ここでは、現状の方策を\(\mu(s)\)、方策\(\mu(s)\)における状態価値関数を\(v_{\mu}(s)\)、新たな方策を\(\mu'(s)\)とする。</li>
                <li>もしすべての状態\(s\)において\(\mu(s)\)と\(\mu'(s)\)（greedy化された方策）が同じであれば、以下の式が成り立ち、方策\(\mu(s)\)は既に最適方策であるといえる。
                    <div class="scroll">
                    \begin{align}
                        \mu(s)
                        &= arg\max_{a}q_{\mu}(s, a)
                    \end{align}
                    </div>
                </li>
                <li>逆に、greedy化によって方策が更新される場合、すべての状態\(s\)において\(v_{\mu'}(s) \geqq v_{\mu}(s)\)が成り立ち、方策は常に改善される。</li>
                <li>したがって、最適方策を見つけるには、以下のようなフローを続ければよい。
                    <ul>
                        <li>まず、\(\pi_{0}\)という方策からスタートする。</li>
                        <li>次に、方策\(\pi_{0}\)における価値関数を評価して\(V_0\)を得る。（反復方策評価）</li>
                        <li>そして、価値関数\(V_0\)を使ってgreedy化を行う。greedy化された方策は常に一つの行動が選ばれるので、決定論的な方策として\(\mu_1\)が得られる。</li>
                    </ul>
                </li>
                <li>greedy化によって方策が変更されない地点に到達したとき、その方策が最適方策であり、最適価値関数である。</li>
                <li>評価と改善を繰り返すアルゴリズムを<b>方策反復法</b>と呼ぶ。</li>
                <li>環境は状態遷移確率\(p(s'|s, a)\)と報酬関数\(r(s, a, s')\)によって表される。</li>
                <li>強化学習の分野では、その2つを指して「環境のモデル」や単に「モデル」と呼ぶ。</li>
                <li>環境のモデルが既知であれば、エージェントは何も行動することなく、価値関数を評価することができる。</li>
                <li>方策反復法を使えば最適方策も見つけることができる。</li>
                <li>エージェントが実際の行動を行わずに最適方策を見つける問題は<b>プランニング問題</b>と呼ばれる。</li>
                <li>方策反復法のアイデアは、「評価」と「改善」という２つの過程を交互に繰り返すことである。
                    <ul>
                        <li>「評価」のフェーズでは、方策を評価して価値関数を得る。</li>
                        <li>「改善」のフェーズでは、価値関数をgreedy化することで改善された方策を得る。</li>
                    </ul>
                </li>
                <li>これを繰り返すことで、最適方策と最適価値関数に徐々に近づいていく。</li>
                <li>方策反復法では「評価」と「改善」をそれぞれ"最大限"に行って、交互にフェーズを切り替える。</li>
                <li><b>価値反復法</b>では、「評価」と「改善」をそれぞれ"最小限"に行う。</li>
                <li>改善フェーズで行うgreedy化は数式では次のように書ける。
                    <div class="scroll">
                    \begin{align}
                        \mu(s)
                        &= arg\max_{a}\sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma V(s')\}
                    \end{align}
                    </div>
                </li>
                <li>argmaxによって行動は１つに決まるため、\(\mu(s)\)のように決定論的方策として表すことができる。</li>
                <li>評価フェーズにおけるDPによる更新式は以下のように表される。
                    <div class="scroll">
                    \begin{align}
                    V'(s) 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\{r(s,a,s') + \gamma V(s')\}
                    \end{align}
                    </div>
                </li>
                <li>この式では、方策\(\pi(a|s)\)が確率的な方策として表記されているが、「改善」のフェーズを一度経由すれば、方策はgreedyな方策として表すことができる。</li>
                <li>greedyな方策は、最大値を取る行動がただ１つ選ばれるため、決定論的な方策である。</li>
                <li>そのため、上式における方策は、決定論的な方策\(\mu(s)\)として扱うことができ、\(a=\mu(s)\)として、以下のように簡略化できる。
                    <div class="scroll">
                    \begin{align}
                    V'(s) 
                    &= \sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma V(s')\}
                    \end{align}
                    </div>
                </li>
                <li>これが「評価」フェーズにおける価値関数の更新式である。</li>
                <li>「改善」フェーズの式と「評価」フェーズの式を見比べると、計算に重複が生じることが分かる。</li>
                <li>よって、以下のように１つの式にまとめることができる。
                    <div class="scroll">
                    \begin{align}
                    V'(s) 
                    &= \max_{a}\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma V(s')\}
                    \end{align}
                    </div>
                </li>
                <li>この式のとおり、最大値をとるmax演算子によって、価値関数を直接更新する。</li>
                <li>方策を使わずに価値関数を更新していることから、この式によって最適価値関数を得るアルゴリズムは「価値反復法」と呼ばれる。</li>
                <li>価値反復法はこの１つの式によって、「評価」と「改善」を同時に行う。</li>
                <li>ベルマン最適方程式は次の式で表される。
                    <div class="scroll">
                    \begin{align}
                        v_{*}(s) 
                        &= \max_{a}\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma v_{*}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>したがって、価値反復法における式はベルマン最適方程式を「更新式」にしたものといえる。</li>
                <li>また、価値反復法における式は次のようにも書ける。
                    <div class="scroll">
                    \begin{align}
                    V_{k+1}(s) 
                    &= \max_{a}\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma V_{k}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>この式によって\(V_{*}(s)\)が得られれば、最適方策\(\mu_{*}(s)\)は次のように得られる。
                    <div class="scroll">
                    \begin{align}
                    \mu_{*}(s) 
                    &= arg\max_{a}\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma V_{*}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>つまり、greedyな方策を求めれば、それが最適方策となる。</li>
            </ul>
        </section>
        <section id="mc">
            <h2>Monte Carlo Method</h2>
            <ul>
                <li>動的計画法では、「環境のモデル（状態遷移確率と報酬関数）」が既知である必要がある。</li>
                <li><b>モンテカルロ法</b>とは、データのサンプリングを繰り返し行って、その結果から推定する手法の総称である。</li>
                <li>強化学習では、モンテカルロ法を使うことで、経験から価値関数を推定することができる。</li>
                <li>ここで言う「経験」とは、環境とエージェントが実際にやりとりを行って得られたデータである。</li>
                <li>具体的には、「状態、行動、報酬」の一連のデータが経験である。</li>
                <li>確率分布として表されたモデルを<b>分布モデル</b>と呼ぶ。</li>
                <li>サンプリングさえできれば良いというモデルを<b>サンプルモデル</b>と呼ぶ。</li>
                <li>分布モデルが確率分布を明示的に保持できることが条件であるのに対して、サンプルモデルはサンプリングできることが条件である。</li>
                <li>モンテカルロ法では、実際にサンプリングを行い、そのサンプルデータから期待値を計算する。</li>
                <li>具体的には、エージェントが実際に行動して得た経験から価値関数を推定する。</li>
                <li>価値関数は以下の式で表される。
                    <div class="scroll">
                    \begin{align}
                    v_{\pi}(s)=\mathbb{E}_{\pi}[G|s]
                    \end{align}
                    </div>
                </li>
                <li>ここでは、状態\(s\)からスタートして得られる収益を\(G\)で表す。</li>
                <li>価値関数\(v_{\pi}(s)\)は上式のとおり、方策\(\pi\)に従って行動した時に得られる収益の期待値として定義される。</li>
                <li>ここではエピソードタスクを想定し、ある時刻にはゴールにたどり着く場合を考える。</li>
                <li>この価値関数\(v_{\pi}(s)\)をモンテカルロ法によって計算する。</li>
                <li>それには、方策\(\pi\)に従ってエージェントに実際に行動させる。</li>
                <li>そうして得られた実際の収益がサンプルデータになる。</li>
                <li>そのようなサンプルデータをたくさん集めて、その平均を求めるのがモンテカルロ法である。</li>
                <li>これを数式で表すと、次のようになる。
                    <div class="scroll">
                    \begin{align}
                    v_{\pi}(s) = \frac{G^{(1)} + G^{(2)} + \cdots + G^{(n)}}{n}
                    \end{align}
                    </div>
                </li>
                <li>ここでは、状態\(s\)からスタートして得られる収益を\(G\)で表し、\(i\)回目のエピソードで得られた収益を\(G^{(i)}\)のように表記する。</li>
                <li>モンテカルロ法で計算するには、上式のように\(n\)回のエピソードを行い、そこで得られたサンプルデータの平均を求める。</li>
                <li>モンテカルロ法は、エピソードタスクでしか使うことができない。</li>
                <li>エージェントの開始位置が固定されていても、エピソードを繰り返すうちにすべての状態を経由することができれば、すべての状態についての収益のサンプルデータを集めることができる。</li>
                <li>最適方策は、「評価」と「改善」を交互に繰り返すことで得られる。</li>
                <li>「評価」のフェーズでは、方策を評価して価値関数を得る。</li>
                <li>「改善」のフェーズでは、価値関数をgreedy化することで方策を改善する。</li>
                <li>この２つを交互に繰り返すことで、最適方策・最適価値関数に近づける。</li>
                <li>\(\pi\)という方策があれば、モンテカルロ法によって\(V_{\pi}(s)\)を得ることができる。</li>
                <li>改善におけるgreedy化は以下の数式で行われる。
                    <div class="scroll">
                    \begin{align}
                    \mu(s)
                    &= arg\max_{a}Q(s, a) \\
                    &= arg\max_{a}\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma V(s')\}
                    \end{align}
                    </div>
                </li>
                <li>改善フェーズでは、価値関数が最大となる行動を選ぶ。</li>
                <li>Q関数（行動価値関数）の場合は、Q関数が最大値をとる行動を選ぶ。</li>
                <li>このとき、行動が1つに決まるため、関数\(\mu(s)\)として表すことができる。</li>
                <li>一般的な強化学習の問題では、\(p(s'|s,a)\)と\(r(s,a,s')\)を知ることができない。</li>
                <li>そのため、上式の一行目のQ関数による表現での改善を行う。</li>
                <li>Q関数を対象に改善する場合、Q関数に関して「評価」を行う必要がある。</li>
                <li>Q関数を対象とするモンテカルロ法の更新式は以下のとおり。
                    <div class="scroll">
                    \begin{align}
                    Q_{n}(s, a) &= \frac{G^{(1)} + G^{(2)} + \cdots + G^{(n)}}{n} \\
                    &= Q_{n-1}(s, a) + 1/n \{G^{(n)} - Q_{n-1}(s, a)\}
                    \end{align}
                    </div>
                </li>
                <li>ここで、\(Q_{n}(s, a)\)は\(n\)回目のエピソードが終わった時点でのQ関数の推定値である。</li>
                <li>エージェントは改善フェーズで方策をgreedy化する。</li>
                <li>greedy化によって、ある状態で行う行動は１つに固定される。</li>
                <li>これでは、すべての状態と行動の組み合わせに対して収益のサンプルデータを集めることができない。</li>
                <li>この問題を解決するには、エージェントに「探索」させる必要がある。</li>
                <li>その方法の１つが、ε-greedy法である。</li>
                <li>エージェントの行動にランダム性を少し加えることで、基本的にはQ関数が最大の行動を選び、低い確率でランダムな行動を選ぶようにする。</li>
                <li>そうすることで、決まった状態や行動だけしか選ばれないという問題を防ぐ。</li>
                <li>また、先の更新式では各データに対する重みがすべて\(1/n\)であるが、これを固定値\(\alpha\)にする。</li>
                <li>そうすることで、新しいデータほど重みが大きくなる（指数移動平均）。</li>
                <li>モンテカルロ法による方策制御では、標本平均ではなく、指数移動平均が適している。</li>
                <li>その理由は、収益（というサンプルデータ）が生成される確率分布が時間と共に変動するからである。</li>
                <li>自分とは別の場所で得られた経験から自分の方策を改善するアプローチを、<b>方策オフ型</b>という。</li>
                <li>一方、自分で得た経験から自分の方策を改善する場合は<b>方策オン型</b>という。</li>
                <li>エージェントの方策は、役割的に見ると、２つの方策がある、</li>
                <li>１つは、評価と改善の対象となる方策「<b>ターゲット方策</b>」である。</li>
                <li>もう１つは、エージェントが実際に行動を起こす際に使う方策「<b>挙動方策</b>」である。</li>
                <li>つまり、方策オフ型は、「ターゲット方策と挙動方策が離れている」ということ。</li>
                <li>方策オフ型では、挙動方策には「探索」を行わせ、ターゲット方策には「活用」だけを行わせることができる。</li>
                <li>ただし、挙動方策から得られたサンプルデータを使って、ターゲット方策に関連する期待値を求めるには計算に工夫が必要である。</li>
                <li>ここで登場するのが<b>重点サンプリング</b>というテクニックである。</li>
                <li>重点サンプリングとは、ある確率分布の期待値を、別の確率分布からサンプリングしたデータを使って計算する手法である。</li>
                <li>ここで、次の期待値を考える。
                    <div class="scroll">
                    \begin{align}
                    \mathbb{E}_{\pi}[x] &= \sum x \pi(x)
                    \end{align}
                    </div>
                </li>
                <li>この期待値をモンテカルロ法によって近似するには、\(x\)を確率分布\(\pi\)からサンプリングして、その平均を取る。</li>
                <li>数式で書くと以下のようになる。
                    <div class="scroll">
                    \begin{align}
                    &\text{sampling: } x^{(i)} \sim \pi \quad (i=1, 2, \ldots, n) \\
                    &\mathbb{E}_{\pi}[x] \simeq \frac{x^{(1)} + x^{(2)} + \cdots + x^{(n)}}{n}
                    \end{align}
                    </div>
                </li>
                <li>ここで、\(x\)が\(\pi\)ではなく、\(b\)という確率分布からサンプリングされたとする。</li>
                <li>この場合、期待値\(\mathbb{E}_{\pi}[x]\)はどのように近似できるか。</li>
                <li>期待値\(\mathbb{E}_{\pi}[x]\)は以下のように式変形できる。
                    <div class="scroll">
                    \begin{align}
                    \mathbb{E}_{\pi}[x] &= \sum x \pi(x) \\
                    &= \sum x \frac{b(x)}{b(x)} \pi(x) \\
                    &= \sum x \frac{\pi(x)}{b(x)} b(x) \\
                    &= \mathbb{E}_{b}[x \frac{\pi(x)}{b(x)}]
                    \end{align}
                    </div>
                </li>
                <li>ここで\(\rho(x)=\frac{\pi(x)}{b(x)}\)とすれば、各\(x\)には「重み」として\(\rho(x)\)が掛けられていると見なすことができる。</li>
                <li>以上から、上式に基づいたモンテカルロ法は以下のようになる。
                    <div class="scroll">
                    \begin{align}
                    &\text{sampling: } x^{(i)} \sim b \quad (i=1, 2, \ldots, n) \\
                    &\mathbb{E}_{\pi}[x] \simeq \frac{\rho(x^{(1)})x^{(1)} + \rho(x^{(2)})x^{(2)} + \cdots + \rho(x^{(n)})x^{(n)}}{n}
                    \end{align}
                    </div>
                </li>
                <li>これで、\(\pi\)とは異なる確率分布\(b\)からサンプリングしたデータを使って、\(\mathbb{E}_{\pi}[x]\)を計算することができる。</li>
                <li>重点サンプリングを行うときには、２つの確率分布を近づけることで分散を小さくすることができる。</li>
                <li>重点サンプリングを使えば、方策オフ型を実現できる。</li>
                <li>具体的には、挙動方策という確率分布からサンプリングされたデータを使って、ターゲット方策に関する期待値が計算できる。</li>
            </ul>
        </section>
        <section id="td">
            <h2>Temporal Difference</h2>
                <li>モンテカルロ法は、エピソードの「終わり」にたどり着いてからでないと、価値関数の更新ができない。</li>
                <li>なぜなら、エピソードの終わりになって初めて「収益」が確定するから。</li>
                <li>連続タスクの場合、モンテカルロ法は使うことができない。</li>
                <li>エピソードタスクであっても、エピソードを終えるのに時間がかかる場合は、モンテカルロ法だと価値関数の更新に多くの時間を要する。</li>
                <li>特にエピソードの最初の段階では、エージェントの方策はランダムなことが多いため、さらに多くの時間を必要とする。</li>
                <li>TD法では、環境のモデルを使わずに、行動を１つ行うたびに価値関数を更新する手法である。</li>
                <li>エピソードの終わりを待つのではなく、一定の時間が進むごとに方策の評価と改善を行う。</li>
                <li>TD法は、「モンテカルロ法」と「動的計画法」を合わせたような手法である。</li>
                <li>収益は以下のように定義される。
                    <div class="scroll">
                    \begin{align}
                    G_t &= R_t+\gamma\{R_{t+1}(\gamma R_{t+2}+...)\} \\
                    &= R_t+\gamma G_{t+1}
                    \end{align}
                    </div>
                </li>
                <li>この収益を用いて、価値関数は以下のように定義される。
                    <div class="scroll">
                    \begin{align}
                    v_{\pi}(s) &= \mathbb{E_{\pi}}[G_{t}|S_t=s] & \text{(a)}\\
                    &= \mathbb{E_{\pi}}[R_t+\gamma G_{t+1}|S_t=s] & \text{(b)}
                    \end{align}
                    </div>
                </li>
                <li>MC法を使った手法は式(a)から、DP法を使った手法は式(b)から導くことができる。</li>
                <li>MC法では、期待値を計算する代わりに、実際に得られた収益のサンプルデータを平均することで、式(a)の期待値を近似する。</li>
                <li>平均には、標本平均と指数移動平均の２つがある。</li>
                <li>指数移動平均を実現するには、新しい収益が得られるたびに固定値\(\alpha\)で更新する。</li>
                <li>これを数式で表すと次のようになる。
                    <div class="scroll">
                    \begin{align}
                    V'_{\pi}(S_t)=V_{\pi}(S_t)+\alpha\{G_t-V_{\pi}(S_t)\}
                    \end{align}
                    </div>
                </li>
                <li>ここで、現状の価値関数を\(V_{\pi}\)、更新後の価値関数を\(V'_{\pi}\)とする。</li>
                <li>この式では、現状の価値関数\(V_{\pi}\)を\(G_t\)の方へと更新している。</li>
                <li>どのくらい\(G_t\)の方へ更新するかを\(\alpha\)で調整する。</li>
                <li>DP法は式(b)に基づいて期待値を計算する。</li>
                <li>MC法とは異なり、DP法では数式どおり期待値を計算する。</li>
                <li>数式で表すと次のようになる。
                    <div class="scroll">
                    \begin{align}
                    v_{\pi}(s) &= \mathbb{E_{\pi}}[G_{t}|S_t=s] & \text{(a)}\\
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\{r(s,a,s') + \gamma v_{\pi}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>上のように、状態遷移確率\(p(s'|s, a)\)と報酬関数\(r(s, a, s')\)を使って期待値を計算する。</li>
                <li>この式はベルマン方程式である。</li>
                <li>つまり、DP法はベルマン方程式に基づいて以下のように価値価値関数を逐次更新する。
                    <div class="scroll">
                    \begin{align}
                    V'_{\pi}(s)
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\{r(s,a,s') + \gamma V_{\pi}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>この式で重要な点は、「今の状態における価値関数」を「次の状態における価値関数」を使って更新することである。</li>
                <li>このとき、すべての遷移を考慮している点が特徴である。</li>
                <li>DP法は、今の価値関数の推定値を、次の価値関数の推定値を使って更新する。</li>
                <li>この原理は「ブートストラップ」と呼ばれる。</li>
                <li>一方、MC法は実際に得られた経験によって、今の価値関数を更新する。</li>
                <li>この２つの手法を融合させたのが、TD法である。</li>
                <li>TD法は、次の行動と価値関数だけを使って現在の価値関数を更新する。</li>
                <li>重要な点は、次の２つである。
                    <ul>
                        <li>DP法のように、ブートストラップにより価値関数を逐次更新できる。</li>
                        <li>MC法のように、環境に関する知識を必要とせずにサンプリングされたデータを使って価値関数を更新できる。</li>
                    </ul>
                </li>
                <li>TD法を数式から導く。
                    <div class="scroll">
                    \begin{align}
                    v_{\pi}(s) &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\{r(s,a,s') + \gamma v_{\pi}(s')\} & \text{(c)} \\ 
                    &= \mathbb{E_{\pi}}[R_t+\gamma v_{\pi}(S_{t+1})|S_t=s] & \text{(d)}
                    \end{align}
                    </div>
                </li>
                <li>式(c)はすべての候補に関して、\(r(s, a, s')+\gamma v_{\pi}(s)\)というように、報酬と次の価値関数を計算する。</li>
                <li>これを期待値の形で書き直すと式(d)になる。</li>
                <li>TD法では式(d)を用いて価値関数を更新するにあたり、\(R_t+\gamma v_{\pi}(S_{t+1})\)の部分をサンプルデータから近似する。</li>
                <li>数式で表すと、TD法の更新式は以下のようになる。
                    <div class="scroll">
                    \begin{align}
                    V'_{\pi}(S_t) &= V_{\pi}(S_t)+\alpha\{R_t+\gamma V_{\pi}(S_{t+1}) - V_{\pi}(S_t)\}
                    \end{align}
                    </div>
                </li>
                <li>ここで、\(V_{\pi}\)は価値関数の推定値であり、目的地が\(R_t+\gamma V_{\pi}(S_{t+1})\)になる。</li>
                <li>この目的地である\(R_t+\gamma V_{\pi}(S_{t+1})\)は<b>TDターゲット</b>と呼ばれる。</li>
                <li>TD法は、TDターゲットの方向へと\(V_{\pi}(S_t)\)を更新する。</li>
                <li>ここではTDターゲットとして１ステップ先の情報を使ったが、\(n\)ステップ先の情報を使うことも考えられる。</li>
                <li>これは「\(n\)ステップのTD法」と呼ばれる。</li>
                <li>環境のモデルが未知の場合に使える道具として、MC法とTD法の２つがある。</li>
                <li>ここでの疑問は、MC法とTD法のどちらを使うべきか、もしくはどちらが優れているかということである。</li>
                <li>連続タスクではMC法を使えないため、TD法を使うことになる。</li>
                <li>しかし、エピソードタスクに関してはどちらが良いかわからない。</li>
                <li>実際、どちらが優れているかは理論的に証明されていない。</li>
                <li>しかし現実の多くの問題では、TD法の方が速く学習できる（価値関数の更新が速く進む）。</li>
                <li>その理由は、それぞれが何をターゲットとしているかを見るとわかる。</li>
                <li>MC法は\(G_t\)をターゲットとし、その方向へと\(V_{\pi}\)を更新する。</li>
                <li>\(G_t\)はゴールにたどり着いてから得られる収益のサンプルデータである。</li>
                <li>一方、TD法のターゲットは、１ステップ先の情報をもとに計算する。</li>
                <li>TD法の場合、時間が１ステップ進むごとに価値関数を更新することができるため、効率の良い学習が期待できる。</li>
                <li>また、MC法のターゲットは、多くの時間を積み重ねて得られた結果であるため、その値は分散が大きくなる。</li>
                <li>一方、TD法は１ステップ先のデータに基づくため、その変動は小さくなる。</li>
                <li>TDターゲットの中には推定値\(V_{\pi}\)が使われている。</li>
                <li>TD法は「推定値で推定値を更新している」ブートストラッピングである。</li>
                <li>TDターゲットは推定値を含んでいるため、正確な値ではなく、「偏り（バイアス）がある」と呼ばれる。</li></li>
                <li>ただし、そのバイアスは更新を繰り返すごとに小さくなり、最終的には0になる。</li>
                <li>一方、MC法のターゲットには推定値が含まれない。</li>
                <li>つまり、MC法のターゲットには「偏り（バイアス）がない」ということになる。</li>
                <li>「方策オン型」の方策制御に、<b>SARSA</b>という手法がある。</li>
                <li>ここでも、評価と改善のプロセスを繰り返し行うことで最適方策へと近づけていく。</li>
                <li>改善フェーズでは方策をgreedy化する必要があり、\(V_{\pi}(s)\)の場合は環境のモデルが必要になる。</li>
                <li>一方、\(Q_{\pi}(s, a)\)であれば以下のように計算でき、環境のモデルを必要としない。
                    <div class="scroll">
                    \begin{align}
                    \mu(s) &= arg\max_{a}Q_{\pi}(s, a)
                    \end{align}
                    </div>
                </li>
                <li>TD法の更新式は以下のとおりである。
                    <div class="scroll">
                    \begin{align}
                    V'_{\pi}(S_t) &= V_{\pi}(S_t)+\alpha\{R_t+\gamma V_{\pi}(S_{t+1}) - V_{\pi}(S_t)\}
                    \end{align}
                    </div>
                </li>
                <li>この式における状態価値関数\(V_{\pi}(S_t)\)を行動価値関数\(Q_{\pi}(S_t, A_t)\)に変更すると、以下の式が得られる。
                    <div class="scroll">
                    \begin{align}
                    Q'_{\pi}(S_t, A_t) &= Q_{\pi}(S_t, A_t)+\alpha\{R_t+\gamma Q_{\pi}(S_{t+1}, A_{t+1}) - Q_{\pi}(S_t, A_t)\}
                    \end{align}
                    </div>
                </li>
                <li>これがQ関数を対象にしたTD法の更新式である。</li>
                <li>方策オン型では、実際に行動を起こす方策（挙動方策）と評価・改善を行う方策（ターゲット方策）が一致する。</li>
                <li>方策オン型の手法の場合、挙動方策とターゲット方策が同じであるため、改善フェーズでは完全にgreedy化することはできない。</li>
                <li>もしそうしてしまうと、「探索」が行えなくなるため、ε-greedy法を使う。</li>
                <li>エージェントが方策\(\pi\)に従って行動し、\((S_{t}, A_{t}, R_{t}, S_{t+1}, A_{t+1})\)というデータが得られた場合、先の更新式に従って、\(Q_{\pi}(S_t, A_t)\)を即座に更新することができる。</li>
                <li>そして、この更新が終わればすぐに「改善」フェーズへと進むことができる。</li>
                <li>この例では\(Q_{\pi}(S_t, A_t)\)が更新されるため、状態\(S_t\)における方策が変更される可能性がある。</li>
                <li>具体的には、状態\(S_t\)における方策は以下のように更新できる。
                    <div class="scroll">
                        \begin{align}
                        \pi'(a|S_t) &= 
                        \begin{cases} 
                        \arg\max_{a} Q_{\pi}(S_t, a) & \text{with probability } 1 - \epsilon \\
                        \text{random action} & \text{with probability } \epsilon
                        \end{cases}
                        \end{align}
                    </div>
                </li>
                <li>このε-greedy法によって、状態\(S_t\)における行動の選び方が更新される。</li>
                <li>方策オフ型の場合、エージェントは挙動方策とターゲット方策の２つの方策を持つ。</li>
                <li>挙動方策では、多様な行動を行って広くサンプルデータを集める。</li>
                <li>そしてサンプルデータを使って、ターゲット方策をgreedyに更新する。</li>
                <li>このとき注意すべき点は次の２つである。
                    <ul>
                        <li>挙動方策とターゲット方策は似たような確率分布である方が結果は安定する。その点を考慮して、現状のQ関数に対して挙動方策はε-greedyに更新し、ターゲット方策はgreedyに更新する。</li>
                        <li>２つの方策が異なるため、重点サンプリングを使って重み\(\rho\)による補正を行う。</li>
                    </ul>
                </li>
                <li>SARSAの更新式は次のようになる。
                    <div class="scroll">
                    \begin{align}
                    Q'_{\pi}(S_t, A_t) &= Q_{\pi}(S_t, A_t)+\alpha\{R_t+\gamma Q_{\pi}(S_{t+1}, A_{t+1}) - Q_{\pi}(S_t, A_t)\}
                    \end{align}
                    </div>
                </li>
                <li>ペアデータ\((S_t, A_t)\)が更新される対象である。</li>
                <li>この\((S_t, A_t)\)という更新対象は任意に選ぶことができる。</li>
                <li>その選ばれたペアデータに対して次の時刻\(t+1\)の遷移を考える。</li>
                <li>このとき、次の状態\(S_{t+1}\)は、環境の状態遷移確率\(p'(s'|s, a)\)によってサンプリングされる。</li>
                <li>そして、状態\(S_{t+1}\)で選ばれる行動は、ターゲット方策\(\pi\)（もしくは挙動方策\(b\)）によってサンプリングされる。</li>
                <li>そうして得られたサンプルデータを使い、先の更新式に従って\(Q_{\pi}(S_t, A_t)\)を更新する。</li>
                <li>このとき、方策\(\pi\)によって行動が選ばれることを明示すると、SARSAの更新式は次のように書ける。
                    <div class="scroll">
                    \begin{align}
                    &\text{sampling: } A_{t+1} \sim \pi \\
                    &\ Q'_{\pi}(S_t, A_t) = Q_{\pi}(S_t, A_t)+\alpha\{R_t+\gamma Q_{\pi}(S_{t+1}, A_{t+1}) - Q_{\pi}(S_t, A_t)\}
                    \end{align}
                    </div>
                </li>
                <li>この\(R_t+\gamma Q_{\pi}(S_{t+1}, A_{t+1})\)は「TDターゲット」と呼ばれる。</li>
                <li>行動\(A_{t+1}\)が方策\(b\)によってサンプリングされた場合を考える。</li>
                <li>その場合、重み\(\rho\)によってTDターゲットを補正する（重点サンプリング）。</li>
                <li>重み\(\rho\)は、「方策が\(\pi\)だったときにTDターゲットが得られる確率」と「方策が\(b\)だったときにTDターゲットが得られる確率」の比率である。</li>
                <li>これを数式で表すと、次のようになる。
                    <div class="scroll">
                    \begin{align}
                    \rho &= \frac{\pi(A_{t+1}|S_{t+1})}{b(A_{t+1}|S_{t+1})}
                    \end{align}
                    </div>
                </li>
                <li>よって、方策オフ型のSARSAの更新式は次のように表される。
                    <div class="scroll">
                    \begin{align}
                    &\text{sampling: } A_{t+1} \sim \pi \\
                    &\ Q'_{\pi}(S_t, A_t) = Q_{\pi}(S_t, A_t)+\alpha\{\rho(R_t+\gamma Q_{\pi}(S_{t+1}, A_{t+1})) - Q_{\pi}(S_t, A_t)\}
                    \end{align}
                    </div>
                </li>
                <li>上式のとおり、行動は方策\(b\)によってサンプリングされ、重み\(\rho\)によってTDターゲットが補正される。</li>
                <li>方策オフ型のSARSAでは重点サンプリングを使う必要がある。</li>
                <li>重点サンプリングは、結果が不安定になりやすいという問題があるため、使うのは避けたい。</li>
                <li>特に、２つの方策の確率分布が異なれば異なるほど、重点サンプリングで用いる重み\(\rho\)も大きく変動する。</li>
                <li>それによってSARSAの更新式にあるターゲットも変動するため、Q関数の更新が不安定になる。</li>
                <li>これを解決するのが、<b>Q学習</b>である。</li>
                <li>Q学習は次の３つの特徴を持つ。
                    <ul>
                        <li>TD法</li>
                        <li>方策オフ型</li>
                        <li>重点サンプリングを使わない</li>
                    </ul>
                </li>
                <li>ベルマン方程式からSARSAが導出され、ベルマン最適方程式からQ学習が導出される。</li>
                <li>まずはベルマン方程式とSARSAの関係を見る。</li>
                <li>方策\(\pi\)におけるQ関数を\(q_{\pi}(s, a)\)としたとき、ベルマン方程式は次の式で表される。
                    <div class="scroll">
                    \begin{align}
                        q_{\pi}(s, a) 
                        &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma\sum_{a'}\pi(a'|s')q_{\pi}(s', a')\}
                    \end{align}
                    </div>
                </li>
                <li>ベルマン方程式で重要な点は次の２つである。
                    <ul>
                        <li>環境の状態遷移確率\(p(s'|s, a)\)によって「次のすべての状態遷移」を考慮していること</li>
                        <li>エージェントの方策\(\pi\)によって)「次のすべての行動」を考慮していること</li>
                    </ul>
                </li>
                <li>つまり、ベルマン方程式は次の状態と次の行動のすべての候補を考慮する。</li>
                <li>これを踏まえた上でSARSAを見ると、SARSAはベルマン方程式の「サンプリング版」とみなすことができる。</li>
                <li>「サンプリング版」というのは、すべての遷移ではなく、ある１つのサンプリングされたデータを使うということである。</li>
                <li>SARSAでは、次の状態\(S_{t+1}\)は\(p(s'|s, a)\)に基づいてサンプリングする。</li>
                <li>そして、次の行動\(A_{t+1}\)は方策\(\pi(a|s)\)に基づいてサンプリングする。</li>
                <li>このときSARSAのTDターゲットは\(R_t+\gamma Q_{\pi}(S_{t+1}, A_{t+1})\)になる。</li>
                <li>このターゲットの方向へ少しだけQ関数を更新する。</li>
                <li>このようにベルマン方程式とSARSAは対応していて、ベルマン最適方程式に対応するのがQ学習である。</li>
                <li>価値反復法は、最適方策を得るための「評価」と「改善」という２つのプロセスを１つにまとめた手法である。</li>
                <li>価値反復法の重要な点は、ベルマン最適方程式に基づくただ１つの更新式を繰り返すことで最適方策が得られることである。</li>
                <li>ここでは、ベルマン最適方程式による更新で、なおかつ、それを「サンプリング版」にした手法を考える。</li>
                <li>Q関数のベルマン最適方程式は次のとおりである。
                    <div class="scroll">
                    \begin{align}
                        q_{*}(s, a) 
                        &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma\max_{a'}q_{*}(s', a')\}
                    \end{align}
                    </div>
                </li>
                <li>ここでは最適方策\(\pi_{*}\)におけるQ関数を\(q_{*}(s, a)\)で表す。</li>
                <li>ベルマン最適方程式は、ベルマン方程式とは異なり、max演算子が使われる。</li>
                <li>Q学習では、推定値\(Q(S_t, A_t)\)のターゲットは、\(R_t+\gamma \max_{a}Q(S_{t+1}, a)\)になる。</li>
                <li>このターゲットの方向へとQ関数を更新する。</li>
                <li>数式で表すと、以下のようになる。
                    <div class="scroll">
                    \begin{align}
                    Q'(S_t, A_t) &= Q(S_t, A_t)+\alpha\{R_t+\gamma \max_{a}Q(S_{t+1}, a) - Q(S_t, A_t)\}
                    \end{align}
                    </div>
                </li>
                <li>この式に基づいてQ関数を繰り返し更新することで、最適方策におけるQ関数へと近づいていく。</li>
                <li>重要な点は、行動\(A_{t+1}\)がQ関数の最大値によってえらばれていることである。</li>
                <li>何らかの方策によってサンプリングされるのではなく、行動\(A_{t+1}\)はmax演算子によって選ばれる。</li>
                <li>そのため、方策オフ型の手法でありながら、重点サンプリングによる補正を行う必要がない。</li>
                <li>Q学習はターゲット方策と挙動方策の２つを持ち、挙動方策\(b\)には「探索」を行わせる。</li>
                <li>よく用いられる挙動方策は、現在の推定値であるQ関数をε-greedy化した方策である。</li>
                <li>挙動方策が決まれば、それに従って行動を選びサンプルデータを集める。</li>
                <li>そして、エージェントが行動するたびに上式に従てQ関数を更新する。</li>
                <li>エージェントの実装方法には、「分布モデル」と「サンプルモデル」がある。</li>
                <li>サンプルモデルの方がよりシンプルに実装できる。</li>
                <li>分布モデルとは、確率分布を明示的に保持するモデルである。</li>
                <li>サンプルモデルとは、サンプリングできることだけが条件のモデルである。</li>
            </ul>
        </section>
    </main>
</body>
</html>
