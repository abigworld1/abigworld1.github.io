<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>a big world - 強化学習</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>a big world</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        ※数式はスクロールできます。
        <section>
            <h2>Overview</h2>
            強化学習とは
            <ul>
                <li>エージェントが環境と相互作用しながら集めたデータを使って高い報酬を得る方法を学習する</li>
                <li>エージェントが試行錯誤的に行動し、環境から与えられる報酬をもとに、期待報酬を最大化するような行動を選択するように行動を修正していく</li>
            </ul>
            という手法です。このページでは、全くの初学者である私が、強化学習についてまとめていきます。
        </section>
        <section>
            <h2>Source</h2>
            以下の書籍・講座を参考にしました。
            <ul>
                <li><a href="https://www.amazon.co.jp/%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8BDeep-Learning-%E2%80%95%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E7%B7%A8-%E6%96%8E%E8%97%A4-%E5%BA%B7%E6%AF%85/dp/4873119758/ref=sr_1_3?adgrpid=120952843404&dib=eyJ2IjoiMSJ9.hH2wBl7SFx37GV25E2wHXN5tkG88yeA4w0_uhCDQeAGXHennOFOA9HtC0IZoe1CJ12MDkBW19CKDw3V8W2epn6z8IWch7fF_7Pwyi6zEDUlrbfxmsgNs9jOCWG4xqL5Ukct-erC8712p4NJMeWAkLnC9f4WVhcuWRdquRpwzfwj4z7us0YWOIBZY56XeEUZOZo4nhQB5e-BXeh85Ks22-elRZBG8YkYU53x3nirZCAH7Yd4T3n8nN0MGoXI2EKoztLgR5El8GbWZOVz6wVt01ZaJENrUy8OFCaJItSusR_E.zYa2-YOpfMzxqHg0sL06bGmED9UNuzELMrLgzQ2FWCQ&dib_tag=se&hvadid=665912024283&hvdev=c&hvqmt=b&hvtargid=kwd-851214456388&hydadcr=27494_14701818&jp-ad-ap=0&keywords=%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8Bdeep-learning&qid=1721996873&sr=8-3">ゼロから作るDeep Learning ❹ ―強化学習編</a></li>
                <li><a href="https://www.amazon.co.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-Python%E3%81%A7%E5%AD%A6%E3%81%B6%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E6%94%B9%E8%A8%82%E7%AC%AC2%E7%89%88-%E5%85%A5%E9%96%80%E3%81%8B%E3%82%89%E5%AE%9F%E8%B7%B5%E3%81%BE%E3%81%A7-%E4%B9%85%E4%BF%9D/dp/4065172519">機械学習スタートアップシリーズ Pythonで学ぶ強化学習 [改訂第2版] 入門から実践まで</a></li>
                <li><a href="https://www.amazon.co.jp/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E6%A3%AE%E6%9D%91-%E5%93%B2%E9%83%8E/dp/4065155916">強化学習 (機械学習プロフェッショナルシリーズ) </a></li>
                <li><a href="https://www.amazon.co.jp/%E3%81%A4%E3%81%8F%E3%82%8A%E3%81%AA%E3%81%8C%E3%82%89%E5%AD%A6%E3%81%B6-%E6%B7%B1%E5%B1%A4%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-PyTorch%E3%81%AB%E3%82%88%E3%82%8B%E5%AE%9F%E8%B7%B5%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-%E6%A0%AA%E5%BC%8F%E4%BC%9A%E7%A4%BE%E9%9B%BB%E9%80%9A%E5%9B%BD%E9%9A%9B%E6%83%85%E5%A0%B1%E3%82%B5%E3%83%BC%E3%83%93%E3%82%B9-%E5%B0%8F%E5%B7%9D%E9%9B%84%E5%A4%AA%E9%83%8E/dp/4839965625">つくりながら学ぶ! 深層強化学習 ~PyTorchによる実践プログラミング~</a></li>
                <li><a href="https://www.amazon.co.jp/%E3%83%9E%E3%83%AB%E3%83%81%E3%82%A8%E3%83%BC%E3%82%B8%E3%82%A7%E3%83%B3%E3%83%88%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AE%E5%9F%BA%E7%A4%8E%E3%81%A8%E5%BF%9C%E7%94%A8%E2%80%95%E8%A4%87%E9%9B%91%E7%B3%BB%E5%B7%A5%E5%AD%A6%E3%81%AE%E8%A8%88%E7%AE%97%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%83%A0-%E5%A4%A7%E5%86%85-%E6%9D%B1/dp/4339023884">マルチエ-ジェントシステムの基礎と応用: 複雑系工学の計算パラダイム</a></li>
                <li><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u">Stanford CS234: Reinforcement Learning | Winter 2019</a></li>
                <li><a href="https://weblab.t.u-tokyo.ac.jp/education/deep-reinforcement-learning/">深層強化学習 Deep Learning 応用講座 2024 Summer</a></li>
            </ul>
        </section>
        <section>
            <h2>Preparation</h2>
            <h3>Bandit Problem</h3>
            <ul>
                <li>強化学習の中で最もシンプルな問題の１つ。</li>
                <li>「バンディット」とは、「スロットマシン」のこと。</li>
                <li>「<b>多腕バンディット問題</b>」は、１本レバーのスロットマシンが複数台ある状況。</li>
                <li>各マシンの当たりやすさは異なるが、どれが当たりやすいかは分からない。</li>
                <li>決められた回数の中でプレイして、なるべく多く当てたい。</li>
                <li>スロットマシンは「<b>環境</b>」、プレイヤーは「<b>エージェント</b>」と呼ばれる。</li>
                <li>また、プレイは「<b>行動</b>」、当たったときにもらえるコインは「<b>報酬</b>」と呼ばれる。</li>
                <li>バンディット問題では、スロットマシンである「環境」の状態は変化しない。</li>
                <li>プレイしたときにもらえるコインの枚数の期待値（<b>行動価値</b>）が大きいスロットマシンを選んでプレイすればよい。</li>
                <li>行動 \(A\) に対する価値を \( q(A) = \mathbb{E}[R \mid A] \) で表す。</li>
                <li>真の行動価値を\(q(A)\)、行動価値の推定値を\(Q(A)\)とする。</li>
                <li>バンディット問題において、プレイヤーはスロットマシンの価値を知ることはできない。</li>
                <li>しかし、価値が最大のスロットマシンを選びたい。</li>
                <li>よって、実際にプレイして、その結果から選択の善し悪しを推定する必要がある。</li>
                <li>「実際に得られた報酬の平均値」をスロットマシンの価値の推定値として考える。</li>
                <li>ある１台のスロットマシンを\(n\)回プレイして、実際に報酬\(R_1,R_2,...,R_n\)が得られたとき、\(Q_n = \frac{R_1 + R_2 + \ldots + R_n}{n}\)となる。</li>
                <li>つまり、\(n\)回目の行動価値の推定値\(Q_n\)は、\(n\)個ある報酬の標本平均として求められる。</li>
                <li>先の式は、\(Q_n = Q_{n-1} + \frac{1}{n}(R_n - Q_{n-1})\)と書き直せる。</li>
                <li>プレイヤーの戦略として、これまでにプレイした結果をもとに一番良いスロットマシンを選ぶ、というのがまず考えられる。</li>
                <li>つまり、各スロットマシンの価値の推定値の中で最も大きい値のスロットマシンを選ぶという、greedyな方法である。</li>
                <li>しかし、この方法では、様々なスロットマシンを試すことなく、同一のスロットマシンが選ばれ続ける。</li>
                <li>これまでの経験を<b>活用</b>するだけでなく、<b>探索</b>も行うことでより良いスロットマシンを見つけようとする方法の代表例が、<b>ε-greedy法</b>である。</li>
                <li>この方法では、εの確率で探索（ランダムな行動）を行い、1-εの確率で活用（greedyな行動）を行う。</li>
                <li>非定常問題（スロットマシンの勝率がプレイする度に変動する場合）では、新しく得た報酬の重みほど大きくする必要がある。</li>
                <li>標本平均を用いる場合の更新式は\(Q_n = Q_{n-1} + \frac{1}{n}(R_n - Q_{n-1})\)であり、すべての報酬の重みは同じだった。</li>
                <li>ステップサイズである\(1/n\)の代わりに、\(α\)という固定値を使えばよい。</li>
                <li>したがって、更新式は、\(Q_n = Q_{n-1} + α(R_n - Q_{n-1})\)となる。</li>
                <li>この式では、各報酬に対する重みが、過去になるにつれて指数関数的に減少することから、この計算は<b>指数移動平均</b>と呼ばれる。</li>
            </ul>
            <h3>Markov Decision Process</h3>
            <ul>
                <li>エージェントの行動によって状況が変わる問題の一部は<b>マルコフ決定過程（MDP）</b>として定式化される。</li>
                <li>決定性とは、エージェントが（環境と相互作用しながら）行動を決定する過程。</li>
                <li>エージェントの行動によって変化する、エージェントが置かれる状況のことを「<b>状態</b>」と呼ぶ。</li>
                <li>エージェントは目先の報酬ではなく、将来を通して得られる報酬の総和を考える必要がある。</li>
                <li>状態\(s_t\)に基づいてエージェントが行動\(A_t\)を行い、報酬\(R_t\)を得て、次の状態である\(S_{t+1}\)へと遷移する。</li>
                <li>決定論的な状態遷移の場合、次の状態\(s'\)は、今の状態\(s\)と行動\(a\)によって一意に決まる。</li>
                <li>そのため、関数として\(s' = f(s, a)\)として表せる。</li>
                <li>\(f(s, a)\)は<b>状態遷移関数</b>と呼ばれる。</li>
                <li>確率的な状態遷移の場合、状態\(s\)にいて行動\(a\)を行い、次の状態\(s'\)に移動する確率は\(p(s'|s, a)\)と表せる。</li>
                <li>\(p(s'|s, a)\)は<b>状態遷移確率</b>と呼ばれる。</li>
                <li>これまでにどのような状態にあって、どのような行動を行ってきたかという情報は必要ない。</li>
                <li>この性質を<b>マルコフ性</b>という。</li>
                <li>エージェントが状態\(s\)にいて行動\(a\)を行い、次の状態が\(s'\)になったときに得られる報酬を\(r(s, a, s')\)という関数で定義する。</li>
                <li>\(r(s, a, s')\)は<b>報酬関数</b>と呼ばれる。</li>
                <li><b>方策</b>は、エージェントがどのように行動を決定するかを表す。</li>
                <li>決定論的な方策は、関数として\(a = \mu(s)\)のように定義できる。</li>
                <li>確率的な方策は、\(\pi(a|s)\)と表せる。</li>
                <li>MDPの目標は、<b>最適方策</b>を見つけること。</li>
                <li>MDPには、「終わり」のあるエピソードタスクと、「終わり」がない連続タスクがある。</li>
                <li>エージェントの目標は<b>収益</b>を最大にすることであり、収益は\(G_t=R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+...\)として定義される。</li>
                <li>収益はエージェントが得る報酬の和であり、\(\gamma\)は「<b>割引率</b>」と呼ばれる。</li>
                <li>収益の期待値は\(v_{\pi}(s)=\mathbb{E}[G_t|S_t=s,\pi]\)で表せる。</li>
                <li>\(v_{\pi}(s)\)は<b>状態価値関数</b>と呼ばれる。</li>
                <li>方策\(\pi\)が変われば、エージェントが得る報酬も変わり、その総和である収益も変わる。</li>
                <li>先の式は、\(v_{\pi}(s)=\mathbb{E_{\pi}}[G_t|S_t=s]\)として書くこともできる。</li>
                <li>\(v_{\pi}\)は真の状態価値関数であり、\(V_{\pi}\)は推定値としての状態価値関数を意味する。</li>
                <li>最適方策\(\pi_*\)は、他のどの方策と比較しても、すべての状態において状態価値関数\(v_{\pi_*}(s)\)の値が大きい方策である。</li>
                <li>MDPでは、最適方策が少なくとも一つは存在する。</li></li>
                <li>また、その最適方策は「決定論的方策」である。</li>
                <li>最適方策における状態価値関数は、<b>最適状態価値関数</b>と呼ばれ、\(v_*\)で表す。</li>
            </ul>
            <h3>Bellman Equation</h3>
            <ul>
                <li>収益は\(G_t=R_t+(\gamma)R_{t+1}+\gamma^2R_{t+2}+...\)として定義される。</li>
                <li>この式を変形すると、以下のようになる。
                    <div class="scroll">
                    \begin{align}
                    G_t &= R_t+\gamma(R_{t+1}(\gamma R_{t+2}+...) \\
                    &= R_t+\gamma G_{t+1}
                    \end{align}
                    </div>
                    </li>
                <li>状態価値関数は\(v_{\pi}(s)=\mathbb{E_{\pi}}[G_t|S_t=s]\)と定義される。</li>
                <li>したがって、以下のように書ける。
                    <div class="scroll">
                    \begin{align}
                    v_{\pi}(s) &= \mathbb{E_{\pi}}[R_t+\gamma G_{t+1}|S_t=s] \\
                    &= \mathbb{E_{\pi}}[R_t|S_t=s]+\gamma\mathbb{E_{\pi}}[G_{t+1}|S_t=s]
                    \end{align}
                    </div>
                    </li>
                <li>今、状態が\(s\)であり、エージェントが方策\(\pi(a|s)\)に従って行動し、状態遷移確率\(p(s'|s, a)\)に従って新しい状態\(s'\)に移行する場合を考える。</li>
                <li>報酬関数が\(r(s, a, s')\)のとき、期待報酬は以下のように書ける。
                    <div class="scroll">
                    \begin{align}
                    \mathbb{E_{\pi}}[R_t|S_t=s] &= \sum_{a}\sum_{s'}\pi(a|s)(p(s'|s, a)r(s, a, s')
                    \end{align}
                    </div>
                    </li>
                <li>また、以下の式が成り立つ。
                    <div class="scroll">
                    \begin{align}
                    \mathbb{E}_{\pi}[G_{t+1}|S_t=s] 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s'] \\ 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)v_{\pi}(s')
                    \end{align}
                    </div>
                    </li>
                <li>以上より、以下の式を導くことができる。
                    <div class="scroll">
                    \begin{align}
                    v_{\pi}(s) 
                    &= \mathbb{E}_{\pi}[R_t|S_t=s] + \gamma\mathbb{E}_{\pi}[G_{t+1}|S_t=s] \\ 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)r(s,a,s') + \gamma\sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)v_{\pi}(s') \\ 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\{r(s,a,s') + \gamma v_{\pi}(s')\}
                    \end{align}
                    </div>
                    </li>
                <li>この式が<b>ベルマン方程式</b>である。</li>
                <li>ベルマン方程式は、「状態\(s\)の価値関数」と「その次に取り得る状態\(s'\)の価値関数」との関係性を表した式である。</li>
                <li>このベルマン方程式は、すべての状態\(s\)とすべての方策\(\pi\)について成り立つ。</li>
                <li>ベルマン方程式を使えば状態価値関数を求めることができる。</li>
                <li>状態価値関数の２条件「状態」と「方策」に、「行動」を追加すると<b>行動価値関数（Q関数）</b>になる。</li>
                <li>数式で表すと、\(q_{\pi}(s, a) = \mathbb{E_{\pi}}[G_t|S_t=s, A_t=a]\)
                <li>Q関数は、時刻\(t\)の時に状態\(s\)で行動\(a\)を取り、時刻\(t+1\)以降では方策\(\pi\)に従った行動を取る。</li>
                <li>その時に得られる収益の期待値が\(q_{\pi}(s, a)\)である。</li>
                <li>\(q_{\pi}(s, a)\)の行動\(a\)は方策\(\pi\)とは関係なく、自由に決めることができる。</li>
                <li>仮にQ関数の行動\(a\)を方策\(\pi\)に従って選ぶとすると、Q関数と状態価値関数は同じになる。</li>
                <li>収益の期待値に関して、\(v_{\pi}(s)=\sum_{a}\pi(a|s)q_{\pi}(s, a)\)が成り立つ。</li>
                <li>Q関数は以下のように展開できる。
                    <div class="scroll">
                    \begin{align}
                    q_{\pi}(s, a) &= \mathbb{E_{\pi}}[G_t|S_t=s, A_t=a] \\
                    &= \mathbb{E_{\pi}}[R_t+\gamma G_{t+1}|S_t=s, A_t=a] \\
                    &= \sum_{s'}p(s'|s, a)r(s, a, s')+\gamma\sum_{s'}p(s'|s, a)\mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s'] \\
                    &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma v_{\pi}(s')\} \\
                    &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma\sum_{a'}\pi(a'|s')q_{\pi}(s', a')\}
                    \end{align}
                    </div>
                    </li>
                <li>この式がQ関数を用いたベルマン方程式である。</li>
                <li>ベルマン方程式は、ある方策\(\pi\)について成り立つ。</li>
                <li>最適方策に関して成り立つ方程式を<b>ベルマン最適方程式</b>と呼ぶ。</li>
                <li>ベルマン方程式は次の式で表される。
                    <div class="scroll">
                    \begin{align}
                        v_{\pi}(s) 
                        &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\{r(s,a,s') + \gamma v_{\pi}(s')\} \\
                        &= \sum_{a}\pi(a|s)\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma v_{\pi}(s')\}
                    \end{align}
                    </div>
                </li>    
                <li>最適方策を\(\pi_{*}(a|s)\)とした場合、次のようにベルマン方程式が成り立つ。
                    <div class="scroll">
                    \begin{align}
                        v_{*}(s) 
                        &= \sum_{a}\pi_{*}(a|s)\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma v_{*}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>最適方策は\(\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma v_{*}(s')\}\)の値が最大の行動を選び、その最大値がそのまま\(v_{*}\)になる。</li>
                <li>これを数式で表すと、次のようになる。
                    <div class="scroll">
                    \begin{align}
                        v_{*}(s) 
                        &= \max_{a}\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma v_{*}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>この式がベルマン最適方程式である。</li>
                <li>行動価値関数（Q関数）についても同様にベルマン最適方程式を求めることができる。</li>
                <li>最適方策における行動価値関数は<b>最適行動価値関数</b>と呼ばれる。</li>
                <li>Q関数のベルマン方程式は次のとおり。
                    <div class="scroll">
                    \begin{align}
                        q_{\pi}(s, a) 
                        &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma\sum_{a'}\pi(a'|s')q_{\pi}(s', a')\}
                    \end{align}
                    </div>
                </li>
                <li>この式に最適方策\(\pi_{*}\)を代入すると、以下のようになる。
                    <div class="scroll">
                    \begin{align}
                        q_{*}(s, a) 
                        &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma\sum_{a'}\pi_{*}(a'|s')q_{*}(s', a')\}
                    \end{align}
                    </div>
                </li>
                <li>状態価値関数のベルマン最適方程式と同様に展開すると、以下の式が得られる。
                    <div class="scroll">
                    \begin{align}
                        q_{*}(s, a) 
                        &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma\max_{a'}q_{*}(s', a')\}
                    \end{align}
                    </div>
                </li>
                <li>この式がQ関数に関するベルマン最適方程式である。</li>
                <li>\(\pi\)を用いずに表せるのは、MDPでは決定論的な最適方策が少なくとも一つ存在するからである。</li>
                <li>ここで、最適行動価値関数\(q_{*}(s, a)\)が分かっていると仮定する。</li>
                <li>この場合、状態\(s\)における最適な行動は次のように求まる。
                    <div class="scroll">
                    \begin{align}
                        \mu_{*}(s) 
                        &= \argmax_{a}q_{*}(s, a)
                    \end{align}
                    </div>
                </li>
                <li>また、行動価値関数におけるベルマン方程式は以下のとおりであった。
                    <div class="scroll">
                    \begin{align}
                        q_{\pi}(s, a) 
                        &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma\sum_{a'}\pi(a'|s')q_{\pi}(s', a')\}
                    \end{align}
                    </div>
                </li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
        </section>
    </main>
</body>
</html>
