<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>a big world - 強化学習</title>
    <link rel="stylesheet" href="style.css"> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>a big world</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        ※数式はスクロールできます。
        <section>
            <h2>目次</h2>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#source">Source</a></li>
                <li><a href="#preparation">Preparation</a>
                    <ul>
                        <li><a href="#bandit-problem">Bandit Problem</a></li>
                        <li><a href="#mdp">Markov Decision Process</a></li>
                        <li><a href="#bellman-equation">Bellman Equation</a></li>
                    </ul>
                </li>
                <li><a href="#planning">Planning</a>
                    <ul>
                        <li><a href="#dynamic-programming">Dynamic Programming</a></li>
                    </ul>
                </li>
            </ul>
        </section>
        <section id="overview">
            <h2>Overview</h2>
            強化学習とは
            <ul>
                <li>エージェントが環境と相互作用しながら集めたデータを使って高い報酬を得る方法を学習する</li>
                <li>エージェントが試行錯誤的に行動し、環境から与えられる報酬をもとに、期待報酬を最大化するような行動を選択するように行動を修正していく</li>
            </ul>
            という手法です。このページでは、全くの初学者である私が、強化学習についてまとめていきます。
        </section>
        <section>
            <h2>Source</h2>
            以下の書籍・講座を参考にしました。
            <ul>
                <li><a href="https://www.amazon.co.jp/%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8BDeep-Learning-%E2%80%95%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E7%B7%A8-%E6%96%8E%E8%97%A4-%E5%BA%B7%E6%AF%85/dp/4873119758/ref=sr_1_3?adgrpid=120952843404&dib=eyJ2IjoiMSJ9.hH2wBl7SFx37GV25E2wHXN5tkG88yeA4w0_uhCDQeAGXHennOFOA9HtC0IZoe1CJ12MDkBW19CKDw3V8W2epn6z8IWch7fF_7Pwyi6zEDUlrbfxmsgNs9jOCWG4xqL5Ukct-erC8712p4NJMeWAkLnC9f4WVhcuWRdquRpwzfwj4z7us0YWOIBZY56XeEUZOZo4nhQB5e-BXeh85Ks22-elRZBG8YkYU53x3nirZCAH7Yd4T3n8nN0MGoXI2EKoztLgR5El8GbWZOVz6wVt01ZaJENrUy8OFCaJItSusR_E.zYa2-YOpfMzxqHg0sL06bGmED9UNuzELMrLgzQ2FWCQ&dib_tag=se&hvadid=665912024283&hvdev=c&hvqmt=b&hvtargid=kwd-851214456388&hydadcr=27494_14701818&jp-ad-ap=0&keywords=%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8Bdeep-learning&qid=1721996873&sr=8-3">ゼロから作るDeep Learning ❹ ―強化学習編</a></li>
                <li><a href="https://www.amazon.co.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-Python%E3%81%A7%E5%AD%A6%E3%81%B6%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E6%94%B9%E8%A8%82%E7%AC%AC2%E7%89%88-%E5%85%A5%E9%96%80%E3%81%8B%E3%82%89%E5%AE%9F%E8%B7%B5%E3%81%BE%E3%81%A7-%E4%B9%85%E4%BF%9D/dp/4065172519">機械学習スタートアップシリーズ Pythonで学ぶ強化学習 [改訂第2版] 入門から実践まで</a></li>
                <li><a href="https://www.amazon.co.jp/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E6%A3%AE%E6%9D%91-%E5%93%B2%E9%83%8E/dp/4065155916">強化学習 (機械学習プロフェッショナルシリーズ) </a></li>
                <li><a href="https://www.amazon.co.jp/%E3%81%A4%E3%81%8F%E3%82%8A%E3%81%AA%E3%81%8C%E3%82%89%E5%AD%A6%E3%81%B6-%E6%B7%B1%E5%B1%A4%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-PyTorch%E3%81%AB%E3%82%88%E3%82%8B%E5%AE%9F%E8%B7%B5%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-%E6%A0%AA%E5%BC%8F%E4%BC%9A%E7%A4%BE%E9%9B%BB%E9%80%9A%E5%9B%BD%E9%9A%9B%E6%83%85%E5%A0%B1%E3%82%B5%E3%83%BC%E3%83%93%E3%82%B9-%E5%B0%8F%E5%B7%9D%E9%9B%84%E5%A4%AA%E9%83%8E/dp/4839965625">つくりながら学ぶ! 深層強化学習 ~PyTorchによる実践プログラミング~</a></li>
                <li><a href="https://www.amazon.co.jp/%E3%83%9E%E3%83%AB%E3%83%81%E3%82%A8%E3%83%BC%E3%82%B8%E3%82%A7%E3%83%B3%E3%83%88%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AE%E5%9F%BA%E7%A4%8E%E3%81%A8%E5%BF%9C%E7%94%A8%E2%80%95%E8%A4%87%E9%9B%91%E7%B3%BB%E5%B7%A5%E5%AD%A6%E3%81%AE%E8%A8%88%E7%AE%97%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%83%A0-%E5%A4%A7%E5%86%85-%E6%9D%B1/dp/4339023884">マルチエ-ジェントシステムの基礎と応用: 複雑系工学の計算パラダイム</a></li>
                <li><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u">Stanford CS234: Reinforcement Learning | Winter 2019</a></li>
                <li><a href="https://weblab.t.u-tokyo.ac.jp/education/deep-reinforcement-learning/">深層強化学習 Deep Learning 応用講座 2024 Summer</a></li>
            </ul>
        </section>
        <section id="source">
            <h2>Preparation</h2>
            <h3 id="bandit-problem">Bandit Problem</h3>
            <ul>
                <li>強化学習の中で最もシンプルな問題の１つ。</li>
                <li>「バンディット」とは、「スロットマシン」のこと。</li>
                <li>「<b>多腕バンディット問題</b>」は、１本レバーのスロットマシンが複数台ある状況。</li>
                <li>各マシンの当たりやすさは異なるが、どれが当たりやすいかは分からない。</li>
                <li>決められた回数の中でプレイして、なるべく多く当てたい。</li>
                <li>スロットマシンは「<b>環境</b>」、プレイヤーは「<b>エージェント</b>」と呼ばれる。</li>
                <li>また、プレイは「<b>行動</b>」、当たったときにもらえるコインは「<b>報酬</b>」と呼ばれる。</li>
                <li>バンディット問題では、スロットマシンである「環境」の状態は変化しない。</li>
                <li>プレイしたときにもらえるコインの枚数の期待値（<b>行動価値</b>）が大きいスロットマシンを選んでプレイすればよい。</li>
                <li>行動 \(A\) に対する価値を \( q(A) = \mathbb{E}[R \mid A] \) で表す。</li>
                <li>真の行動価値を\(q(A)\)、行動価値の推定値を\(Q(A)\)とする。</li>
                <li>バンディット問題において、プレイヤーはスロットマシンの価値を知ることはできない。</li>
                <li>しかし、価値が最大のスロットマシンを選びたい。</li>
                <li>よって、実際にプレイして、その結果から選択の善し悪しを推定する必要がある。</li>
                <li>「実際に得られた報酬の平均値」をスロットマシンの価値の推定値として考える。</li>
                <li>ある１台のスロットマシンを\(n\)回プレイして、実際に報酬\(R_1,R_2,...,R_n\)が得られたとき、\(Q_n = \frac{R_1 + R_2 + \ldots + R_n}{n}\)となる。</li>
                <li>つまり、\(n\)回目の行動価値の推定値\(Q_n\)は、\(n\)個ある報酬の標本平均として求められる。</li>
                <li>先の式は、\(Q_n = Q_{n-1} + \frac{1}{n}(R_n - Q_{n-1})\)と書き直せる。</li>
                <li>プレイヤーの戦略として、これまでにプレイした結果をもとに一番良いスロットマシンを選ぶ、というのがまず考えられる。</li>
                <li>つまり、各スロットマシンの価値の推定値の中で最も大きい値のスロットマシンを選ぶという、greedyな方法である。</li>
                <li>しかし、この方法では、様々なスロットマシンを試すことなく、同一のスロットマシンが選ばれ続ける。</li>
                <li>これまでの経験を<b>活用</b>するだけでなく、<b>探索</b>も行うことでより良いスロットマシンを見つけようとする方法の代表例が、<b>ε-greedy法</b>である。</li>
                <li>この方法では、εの確率で探索（ランダムな行動）を行い、1-εの確率で活用（greedyな行動）を行う。</li>
                <li>非定常問題（スロットマシンの勝率がプレイする度に変動する場合）では、新しく得た報酬の重みほど大きくする必要がある。</li>
                <li>標本平均を用いる場合の更新式は\(Q_n = Q_{n-1} + \frac{1}{n}(R_n - Q_{n-1})\)であり、すべての報酬の重みは同じだった。</li>
                <li>ステップサイズである\(1/n\)の代わりに、\(α\)という固定値を使えばよい。</li>
                <li>したがって、更新式は、\(Q_n = Q_{n-1} + α(R_n - Q_{n-1})\)となる。</li>
                <li>この式では、各報酬に対する重みが、過去になるにつれて指数関数的に減少することから、この計算は<b>指数移動平均</b>と呼ばれる。</li>
            </ul>
            <h3 id="mdp">Markov Decision Process</h3>
            <ul>
                <li>エージェントの行動によって状況が変わる問題の一部は<b>マルコフ決定過程（MDP）</b>として定式化される。</li>
                <li>決定性とは、エージェントが（環境と相互作用しながら）行動を決定する過程。</li>
                <li>エージェントの行動によって変化する、エージェントが置かれる状況のことを「<b>状態</b>」と呼ぶ。</li>
                <li>エージェントは目先の報酬ではなく、将来を通して得られる報酬の総和を考える必要がある。</li>
                <li>状態\(s_t\)に基づいてエージェントが行動\(A_t\)を行い、報酬\(R_t\)を得て、次の状態である\(S_{t+1}\)へと遷移する。</li>
                <li>決定論的な状態遷移の場合、次の状態\(s'\)は、今の状態\(s\)と行動\(a\)によって一意に決まる。</li>
                <li>そのため、関数として\(s' = f(s, a)\)として表せる。</li>
                <li>\(f(s, a)\)は<b>状態遷移関数</b>と呼ばれる。</li>
                <li>確率的な状態遷移の場合、状態\(s\)にいて行動\(a\)を行い、次の状態\(s'\)に移動する確率は\(p(s'|s, a)\)と表せる。</li>
                <li>\(p(s'|s, a)\)は<b>状態遷移確率</b>と呼ばれる。</li>
                <li>これまでにどのような状態にあって、どのような行動を行ってきたかという情報は必要ない。</li>
                <li>この性質を<b>マルコフ性</b>という。</li>
                <li>エージェントが状態\(s\)にいて行動\(a\)を行い、次の状態が\(s'\)になったときに得られる報酬を\(r(s, a, s')\)という関数で定義する。</li>
                <li>\(r(s, a, s')\)は<b>報酬関数</b>と呼ばれる。</li>
                <li><b>方策</b>は、エージェントがどのように行動を決定するかを表す。</li>
                <li>決定論的な方策は、関数として\(a = \mu(s)\)のように定義できる。</li>
                <li>確率的な方策は、\(\pi(a|s)\)と表せる。</li>
                <li>MDPの目標は、<b>最適方策</b>を見つけること。</li>
                <li>MDPには、「終わり」のあるエピソードタスクと、「終わり」がない連続タスクがある。</li>
                <li>エージェントの目標は<b>収益</b>を最大にすることであり、収益は\(G_t=R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+...\)として定義される。</li>
                <li>収益はエージェントが得る報酬の和であり、\(\gamma\)は「<b>割引率</b>」と呼ばれる。</li>
                <li>収益の期待値は\(v_{\pi}(s)=\mathbb{E}[G_t|S_t=s,\pi]\)で表せる。</li>
                <li>\(v_{\pi}(s)\)は<b>状態価値関数</b>と呼ばれる。</li>
                <li>方策\(\pi\)が変われば、エージェントが得る報酬も変わり、その総和である収益も変わる。</li>
                <li>先の式は、\(v_{\pi}(s)=\mathbb{E_{\pi}}[G_t|S_t=s]\)として書くこともできる。</li>
                <li>\(v_{\pi}\)は真の状態価値関数であり、\(V_{\pi}\)は推定値としての状態価値関数を意味する。</li>
                <li>最適方策\(\pi_*\)は、他のどの方策と比較しても、すべての状態において状態価値関数\(v_{\pi_*}(s)\)の値が大きい方策である。</li>
                <li>MDPでは、最適方策が少なくとも一つは存在する。</li></li>
                <li>また、その最適方策は「決定論的方策」である。</li>
                <li>最適方策における状態価値関数は、<b>最適状態価値関数</b>と呼ばれ、\(v_*\)で表す。</li>
            </ul>
            <h3 id="bellman-equation">Bellman Equation</h3>
            <ul>
                <li>収益は\(G_t=R_t+\gamma R_{t+1}+\gamma^2R_{t+2}+...\)として定義される。</li>
                <li>この式を変形すると、以下のようになる。
                    <div class="scroll">
                    \begin{align}
                    G_t &= R_t+\gamma(R_{t+1}(\gamma R_{t+2}+...) \\
                    &= R_t+\gamma G_{t+1}
                    \end{align}
                    </div>
                    </li>
                <li>状態価値関数は\(v_{\pi}(s)=\mathbb{E_{\pi}}[G_t|S_t=s]\)と定義される。</li>
                <li>したがって、以下のように書ける。
                    <div class="scroll">
                    \begin{align}
                    v_{\pi}(s) &= \mathbb{E_{\pi}}[R_t+\gamma G_{t+1}|S_t=s] \\
                    &= \mathbb{E_{\pi}}[R_t|S_t=s]+\gamma\mathbb{E_{\pi}}[G_{t+1}|S_t=s]
                    \end{align}
                    </div>
                    </li>
                <li>今、状態が\(s\)であり、エージェントが方策\(\pi(a|s)\)に従って行動し、状態遷移確率\(p(s'|s, a)\)に従って新しい状態\(s'\)に移行する場合を考える。</li>
                <li>報酬関数が\(r(s, a, s')\)のとき、期待報酬は以下のように書ける。
                    <div class="scroll">
                    \begin{align}
                    \mathbb{E_{\pi}}[R_t|S_t=s] &= \sum_{a}\sum_{s'}\pi(a|s)(p(s'|s, a)r(s, a, s')
                    \end{align}
                    </div>
                    </li>
                <li>また、以下の式が成り立つ。
                    <div class="scroll">
                    \begin{align}
                    \mathbb{E}_{\pi}[G_{t+1}|S_t=s] 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s'] \\ 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)v_{\pi}(s')
                    \end{align}
                    </div>
                    </li>
                <li>以上より、以下の式を導くことができる。
                    <div class="scroll">
                    \begin{align}
                    v_{\pi}(s) 
                    &= \mathbb{E}_{\pi}[R_t|S_t=s] + \gamma\mathbb{E}_{\pi}[G_{t+1}|S_t=s] \\ 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)r(s,a,s') + \gamma\sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)v_{\pi}(s') \\ 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\{r(s,a,s') + \gamma v_{\pi}(s')\}
                    \end{align}
                    </div>
                    </li>
                <li>この式が<b>ベルマン方程式</b>である。</li>
                <li>ベルマン方程式は、「状態\(s\)の価値関数」と「その次に取り得る状態\(s'\)の価値関数」との関係性を表した式である。</li>
                <li>このベルマン方程式は、すべての状態\(s\)とすべての方策\(\pi\)について成り立つ。</li>
                <li>ベルマン方程式を使えば状態価値関数を求めることができる。</li>
                <li>状態価値関数の２条件「状態」と「方策」に、「行動」を追加すると<b>行動価値関数（Q関数）</b>になる。</li>
                <li>数式で表すと、\(q_{\pi}(s, a) = \mathbb{E_{\pi}}[G_t|S_t=s, A_t=a]\)
                <li>Q関数は、時刻\(t\)の時に状態\(s\)で行動\(a\)を取り、時刻\(t+1\)以降では方策\(\pi\)に従った行動を取る。</li>
                <li>その時に得られる収益の期待値が\(q_{\pi}(s, a)\)である。</li>
                <li>\(q_{\pi}(s, a)\)の行動\(a\)は方策\(\pi\)とは関係なく、自由に決めることができる。</li>
                <li>仮にQ関数の行動\(a\)を方策\(\pi\)に従って選ぶとすると、Q関数と状態価値関数は同じになる。</li>
                <li>収益の期待値に関して、\(v_{\pi}(s)=\sum_{a}\pi(a|s)q_{\pi}(s, a)\)が成り立つ。</li>
                <li>Q関数は以下のように展開できる。
                    <div class="scroll">
                    \begin{align}
                    q_{\pi}(s, a) &= \mathbb{E_{\pi}}[G_t|S_t=s, A_t=a] \\
                    &= \mathbb{E_{\pi}}[R_t+\gamma G_{t+1}|S_t=s, A_t=a] \\
                    &= \sum_{s'}p(s'|s, a)r(s, a, s')+\gamma\sum_{s'}p(s'|s, a)\mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s'] \\
                    &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma v_{\pi}(s')\} \\
                    &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma\sum_{a'}\pi(a'|s')q_{\pi}(s', a')\}
                    \end{align}
                    </div>
                    </li>
                <li>この式がQ関数を用いたベルマン方程式である。</li>
                <li>ベルマン方程式は、ある方策\(\pi\)について成り立つ。</li>
                <li>最適方策に関して成り立つ方程式を<b>ベルマン最適方程式</b>と呼ぶ。</li>
                <li>ベルマン方程式は次の式で表される。
                    <div class="scroll">
                    \begin{align}
                        v_{\pi}(s) 
                        &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\{r(s,a,s') + \gamma v_{\pi}(s')\} \\
                        &= \sum_{a}\pi(a|s)\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma v_{\pi}(s')\}
                    \end{align}
                    </div>
                </li>    
                <li>最適方策を\(\pi_{*}(a|s)\)とした場合、次のようにベルマン方程式が成り立つ。
                    <div class="scroll">
                    \begin{align}
                        v_{*}(s) 
                        &= \sum_{a}\pi_{*}(a|s)\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma v_{*}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>最適方策は\(\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma v_{*}(s')\}\)の値が最大の行動を選び、その最大値がそのまま\(v_{*}\)になる。</li>
                <li>これを数式で表すと、次のようになる。
                    <div class="scroll">
                    \begin{align}
                        v_{*}(s) 
                        &= \max_{a}\sum_{s'}p(s'|s,a)\{r(s,a,s') + \gamma v_{*}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>この式がベルマン最適方程式である。</li>
                <li>行動価値関数（Q関数）についても同様にベルマン最適方程式を求めることができる。</li>
                <li>最適方策における行動価値関数は<b>最適行動価値関数</b>と呼ばれる。</li>
                <li>Q関数のベルマン方程式は次のとおり。
                    <div class="scroll">
                    \begin{align}
                        q_{\pi}(s, a) 
                        &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma\sum_{a'}\pi(a'|s')q_{\pi}(s', a')\}
                    \end{align}
                    </div>
                </li>
                <li>この式に最適方策\(\pi_{*}\)を代入すると、以下のようになる。
                    <div class="scroll">
                    \begin{align}
                        q_{*}(s, a) 
                        &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma\sum_{a'}\pi_{*}(a'|s')q_{*}(s', a')\}
                    \end{align}
                    </div>
                </li>
                <li>状態価値関数のベルマン最適方程式と同様に展開すると、以下の式が得られる。
                    <div class="scroll">
                    \begin{align}
                        q_{*}(s, a) 
                        &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma\max_{a'}q_{*}(s', a')\}
                    \end{align}
                    </div>
                </li>
                <li>この式がQ関数に関するベルマン最適方程式である。</li>
                <li>\(\pi\)を用いずに表せるのは、MDPでは決定論的な最適方策が少なくとも一つ存在するからである。</li>
                <li>ここで、最適行動価値関数\(q_{*}(s, a)\)が分かっていると仮定する。</li>
                <li>この場合、状態\(s\)における最適な行動は次のように求まる。
                    <div class="scroll">
                    \begin{align}
                        \mu_{*}(s) 
                        &= arg\max_{a}q_{*}(s, a)
                    \end{align}
                    </div>
                </li>
                <li>また、行動価値関数におけるベルマン方程式は以下のとおりであった。
                    <div class="scroll">
                    \begin{align}
                        q_{\pi}(s, a) 
                        &= \sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma v_{\pi}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>この式の最適方策版を先の最適な行動を表す式に代入すると、以下の式が得られる。
                    <div class="scroll">
                    \begin{align}
                        \mu_{*}(s) 
                        &= arg\max_{a}\sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma v_{*}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>このように、最適状態価値関数\(v_{*}(s)\)を使って、最適方策\(\mu_{*}(s)\)を得ることができる。</li>
                <li>最適な行動価値関数を最大とするような行動であるため、「greedyな方策」といえる。</li>
            </ul>
        </section>
        <section>
            <h2 id="planning">Planning</h2>
            <h3 id="dynamic-programming">Dynamic Programming</h3>
            <ul>
                <li>動的計画法を使えば、状態と行動の数がある程度大きくなっても価値関数を評価することができる。</li>
                <li>強化学習の問題は、多くの場合、<b>方策評価</b>と<b>方策制御</b>の２つのタスクに取り組むことになる。</li>
                <li>方策評価とは、ある方策\(\pi\)が与えられたときに、その方策の価値関数\(v_{\pi}(s)\)や\(q_{\pi}(s, a)\)を求めること。</li>
                <li>方策制御とは、方策を制御して最適方策へと調整すること。</li>
                <li>動的計画法では、方策評価を行う。</li>
                <li>価値関数を次のように定義した。
                    <div class="scroll">
                    \begin{align}
                    v_{\pi}(s) &= \mathbb{E_{\pi}}[R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+...|S_t=s]
                    \end{align}
                    </div>
                </li>
                <li>この無限を含む期待値の計算は不可能なので、次のベルマン方程式によって解決する。
                    <div class="scroll">
                    \begin{align}
                    v_{\pi}(s) 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\{r(s,a,s') + \gamma v_{\pi}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>ベルマン方程式は、上式のように「現在の状態\(s\)の価値関数\(v_{\pi}(s)\)」と「次の状態\(s'\)の価値関数\(v_{\pi}(s')\)」との関係性を表す。</li>
                <li>DPでは、ベルマン方程式を以下のように「更新式」へと変形する。
                    <div class="scroll">
                    \begin{align}
                    V_{k+1}(s) 
                    &= \sum_{a}\sum_{s'}\pi(a|s)p(s'|s,a)\{r(s,a,s') + \gamma V_{k}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>ここで、\(V_{k}(s)\)は\(k\)回目に更新された価値関数、\(V_{k+1}(s)\)は\(k+1\)回目に更新された価値関数であり、共に「推定値」である。</li>
                <li>この式の特徴は、「次に取り得る状態の価値関数\(V_{k}(s')\)」を使って、「今いる状態の価値関数\(V_{k+1}(s)\)」を更新することである。</li>
                <li>ここで行っていることは「推定値\(V_{k}(s')\)」を使って「別の推定値\(V_{k+1}(s)\)」を改善することである。</li>
                <li>このように、推定値を使って推定値を改善するプロセスのことを、<b>ブートストラッピング</b>という。</li>
                <li>DPでは、まず、\(V_{0}(s)\)の初期値を設定する。</li>
                <li>そして、先に示した更新式によって\(V_{0}(s)\)から\(V_{1}(s)\)へと更新する。</li>
                <li>これを繰り返し行うことで、最終的なゴールである\(V_{\pi}(s)\)へと近づいていく。</li>
                <li>このアルゴリズムは<b>反復方策評価</b>と呼ばれる。</li>
                <li>実際の問題で反復方策評価アルゴリズムを使う場合、繰り返し行う更新をどこかでストップする必要がある。</li>
                <li>何回更新するかを決めるには、更新された量から判断できる。</li>
                <li>先の更新式による更新を繰り返せば、\(V_{\pi}(s)\)に収束することが証明されている。</li>
                <li>ただし、収束するにはいくつかの条件が必要。</li>
                <li>DPのエッセンスは「同じ計算を二度しない」こと。</li>
                <li>その実現方法には、「トップダウン方式（メモ化）」と「ボトムアップ方式」がある。</li>
                <li>今回の\(V_{0}(s), V_{1}(s), ...\)と1つずつ繰り上げながら価値関数を更新する手法は「ボトムアップ方式」である。</li>
                <li>ゴールにおける価値関数の値は常に0である。</li>
                <li>DPを使うことで、効率的に方策の評価を行うことができる。</li>
                <li>目標は、最適方策を得ること。</li>
                <li>ベルマン最適方程式を満たす連立方程式を解くことで求められるが、計算量的に問題がある。</li>
                <li>具体的には、状態のサイズを\(S\)、行動のサイズを\(A\)としたとき、解を求めるために\(A^S\)のオーダーの計算量が必要になる。</li>
                <li>DPの反復方策評価によって方策を評価できたので、あとは方策の「改善」ができたらよい。</li>
                <li>最適方策は以下の式で表される。
                    <div class="scroll">
                    \begin{align}
                        \mu_{*}(s)
                        &= arg\max_{a}q_{*}(s, a) \\
                        &= arg\max_{a}\sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma v_{*}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>最適方策は最大値をとる行動\(a\)によって決まるため、この式によって得られる方策は「greedyな方策」と呼ばれる。</li>
                <li>また、この式より、最適価値関数\(v_{*}\)がわかれば、最適方策\(\mu_{*}\)が求まるが、\(v_{*}\)を知るには\(\mu_{*}\)が必要となることが分かる。</li>
                <li>ここで、「何らかの決定論的方策\(\mu\)」に対して上式を適用すると、以下のようになる。
                    <div class="scroll">
                    \begin{align}
                        \mu'(s)
                        &= arg\max_{a}q_{\mu}(s, a) \\
                        &= arg\max_{a}\sum_{s'}p(s'|s, a)\{r(s, a, s')+\gamma v_{\mu}(s')\}
                    \end{align}
                    </div>
                </li>
                <li>ここでは、現状の方策を\(\mu(s)\)、方策\(\mu(s)\)における状態価値関数を\(v_{\mu}(s)\)、新たな方策を\(\mu'(s)\)とした。</li>
                <li>もしすべての状態\(s\)において\(\mu(s)\)と\(\mu'(s)\)（greedy化された方策）が同じであれば、以下の式が成り立ち、方策\(\mu(s)\)は既に最適方策であるといえる。
                    <div class="scroll">
                    \begin{align}
                        \mu(s)
                        &= arg\max_{a}q_{\mu}(s, a)
                    \end{align}
                    </div>
                </li>
                <li>つまり、greedy化を行ってもすべての状態\(s\)において\(\mu'(s)\)が更新されないのであれば、\(\mu(s)\)は既に最適方策である。</li>
                <li>逆に、greedy化によって方策が更新される場合、すべての状態\(s\)において\(v_{\mu'}(s) \geqq v_{\mu}(s)\)が成り立ち、方策は常に改善される。</li>
                <li>方策が改善される数学的根拠は、<b>方策改善定理</b>によって与えられる。</li>
                <li>以上をまとめると、方策をgreedy化することで
                    <ul>
                        <li>方策は常に改善される</li>
                        <li>もし方策の改善（更新）がなければ、それが最適方策である</li>
                    </ul>
                    ということになる。
                </li>
                <li>したがって、最適方策を見つけるには、以下のような手順を踏めばよい。
                    <ul>
                        <li>まず、\(\pi_{0}\)という方策からスタートする。</li>
                        <li>次に、方策\(\pi_{0}\)における価値関数を評価して\(V_0\)を得る。（反復方策評価）</li>
                        <li>そして、価値関数\(V_0\)を使ってgreedy化を行う。greedy化された方策は常に一つの行動が選ばれるので、決定論的な方策として\(\mu_1\)が得られる。</li>
                    </ul>
                </li>
                <li>後はこのフローを続ければよい。</li>
                <li>これを続けると、greedy化によって方策が変更されない地点に到達する。</li>
                <li>その方策が最適方策であり、最適価値関数である。</li>
                <li>この評価と改善を繰り返すアルゴリズムを<b>方策反復法</b>と呼ぶ。</li>
                <li>環境は状態遷移確率\(p(s'|s, a)\)と報酬関数\(r(s, a, s')\)によって表される。</li>
                <li>強化学習の分野では、その2つを指して「環境のモデル」や単に「モデル」と呼ぶ。</li>
                <li>環境のモデルが既知であれば、エージェントは何も行動することなく、価値関数を評価することができる。</li>
                <li>そして方策反復法を使えば最適方策も見つけることができる。</li>
                <li>エージェントが実際の行動を行わずに最適方策を見つける問題は<b>プランニング問題</b>と呼ばれる。</li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
        </section>
    </main>
</body>
</html>
