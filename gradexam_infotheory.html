<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【院試対策|北海道大学 情報科学院】走る作曲家のAIカフェ～情報理論編～</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
          <p>「北海道大学大学院情報科学院修士課程入学試験」（令和6年8月実施）の「<b>情報理論</b>」対策ページです。</p>
        </section>
        <section>
          <h2>分野別対策</h2>
          <ul>
            <li><a href="gradexam.html">院試対策 ～はじめに～</a></li>
            <li><a href="gradexam_basicmath.html">院試対策 ～基礎数学編～</a></li>
            <li><a href="gradexam_infomath.html">院試対策 ～情報数学編～</a></li>
            <li><a href="gradexam_statistics.html">院試対策 ～確率・統計編～</a></li>
            <li><a href="gradexam_infotheory.html">院試対策 ～情報理論編～</a></li>
            <li><a href="gradexam_algo.html">院試対策 ～アルゴリズムとデータ構造編～</a></li>
            <li><a href="gradexam_ai.html">院試対策 ～人工知能編～</a></li>
            <li><a href="gradexam_cs.html">院試対策 ～コンピュータシステム編～</a></li>
          </ul>
        </section>
        <section id="algorithms-data-structures">
        <h2>情報理論</h2>
        <p>参考にしたサイトは以下のとおり。</p>
        <p><a href="https://www-alg.ist.hokudai.ac.jp/~atsu/info_theory.html">情報理論 (Information Theory).html</a></p>
        <h3>情報理論とは</h3>
            <p><b>情報理論が取り組む4つの課題</b></p>
            <ul>
                <li>できるだけよい情報源符号化法を見出すこと</li>
                <li>情報源符号化の限界を知ること</li>
                <li>できるだけよい通信路符号化法を見出すこと</li>
                <li>通信路符号化の限界を知ること</li>
            </ul>
        <h3>条件付き確率</h3>
            <p><b>条件付き確率</b></p>
            <p>事象\(B\)の条件のもとで事象\(A\)が起こる条件付き確率は</p>
            <div class="scroll">
                \begin{align}
                P(A|B) = \frac{P(A \cap B}{P(B)}
                \end{align}
            </div>
            で定義される。
            <p><b>乗法定理</b></p>
            <div class="scroll">
                \begin{align}
                P(A \cap B) = P(A|B) P(B)
                \end{align}
            </div>
        <h3>情報量とエントロピー(1)</h3>
            <p><b>情報量</b></p>
            <p>確率\(p\)の事象の生起を知った時に得られる<b>情報量</b>を\(I(p)\)とする。</p>
            <p>\(I(p)\)は次のような性質を満たすべき。</p>
            <ul>
                <li>\(I(p)\)は\( 0 < p \leq 1 \)で単調現象な関数である</li>
                <li>確率\(p_1, p_2\)で起こる二つの互いに独立な事象が同時に起こる確率\(p_1, p_2\)について\(I(p_1 p_2) = I(p_1) + I(p_2)\)</li>
                <li>\(I(p)\)は\( 0 < p \leq 1 \)で連続な関数である</li>
            </ul>
            <p>これらを満たす関数\(I(p)\)は\(I(p) = - \log_a p\)という形しかありえない（\(a > 1\)）</p>
            <p>確率\(p\)で生起する事象が起きたことを知ったときに得られる情報量\(I(p)\)を<b>自己情報量</b>と呼び、\(I(p) = - \log_a p\)と定義する。ただし、\(a > 1\)である。</p>
            <p>\(a = 2\)のとき、単位は<b>ビット</b>という。つまり、確率\(\frac{1}{2}\)で生じる結果を知った時の情報量は1 [bit]である。</p>
            <p><b>平均情報量</b></p>
            <p>\(M\)個の互いに排反な事象\(a_1, a_2, \cdots, a_M\)が起こる確率を\(p_1, p_2, \cdots, p_M\)とする（ただし、\(p_1 + p_2 + \cdots + p_M = 1\)）。</p>
            <p>このうち１つの事象が起こったことを知ったときに得る情報量は\(- \log_2 p_i\)であるから、これを平均した期待値\(\overline{I}\)は、</p>
            <div class="scroll">
                \begin{align}
                \overline{I} &= p_1 (- \log_2 p_1) + p_2 (- \log_2 p_12) + \cdots + p_M (- \log_2 p_M) \\
                &= - \sum_{i = 1}^{M} p_i \log_2 p_i
                \end{align}
            </div>
            <p>となる。これを<b>平均情報量</b>（単位はビット）という。</p>
            <p><b>エントロピー</b></p>
            <p>確率変数\(X\)が取り得る値が\(x_1, x_2, \cdots, p_M\)（ただし、\(p_1 + p_2 + \cdots + p_M = 1\)）であるとき、確率変数\(X\)の<b>エントロピー</b>を</p>
            <div class="scroll">
                \begin{align}
                H(X) = - \sum_{i = 1}^{M} p_i \log_2 p_i
                \end{align}
            </div>
            <p>ビットと定義する。</p>
            <p>一般に、確率分布の偏りが非常に大きいときは平均情報量が小さく、確率分布が均一の時に平均情報量が最大になる。</p>
            <p>エントロピーと平均情報量は、完全に同じ形をしている。</p>
            <p>情報を受け取ると曖昧さが減るので、受け取った量だけエントロピーは減って、0に近づく（<b>受け取った情報量 = エントロピーの変化</b>）</p>
            <p><b>エントロピーの性質</b></p>
            <p>\(M\)個の値を取る確率変数\(X\)のエントロピー\(H(X)\)は次の性質を満たす。</p>
            <ol>
                <li>\(0 \leq H(X) \leq \log_2 M\)</li>
                <li>\(H(X)\)が最小値0となるのは、ある値を取る確率が1で、他のM-1個の値を取る確率がすべて0の時に限る。すなわち、\(X\)の取る値がはじめから確定している場合のみである。</li>
                <li>\(H(X)\)が最大値\(\log_2 M\)となるのは、\(M\)個の値がすべて\(\frac{1}{M}\)で等しい場合に限る。</li>
            </ol>
            <p>エントロピーは「答えのあてにくさ」を表す数値ともいえる。</p>
            <p><b>2つ以上の確率変数を扱う場合</b></p>
            <P>２つの確率変数\(X, Y\)を考える。\(X\)は\(x_1, x_2, \cdots, x_{M_X}\)の値をとり、\(Y\)は\(y_1, y_2, \cdots, y_{M_Y}\)の値を取るものとする。</P>
            <p>確率変数の組\((X, Y)\)の値が\((x, y)\)となる<b>結合確率分布</b>を\(P(x, y)\)と書く。</p>
            <p>組\((X, Y)\)をまとめて考えると、４つの値を取る確率変数\(Z\)のエントロピー\(H(Z)\)として考えることができる。</p>
            <p>確率変数\(X\)と\(Y\)の結合エントロピー\(H(X, Y)\)は、</p>
            <div class="scroll">
                \begin{align}
                H(X, Y) = - \sum_{i = 1}^{M_X} \sum_{j = 1}^{M_Y} P(x_i, y_j) \log_2 P(x_i, y_j)
                \end{align}
            </div>
            <p>により定義される。これを<b>結合エントロピー</b>と呼ぶ。ただし、\({x_1, x_2, \cdots, x_{M_X}}\)および\({y_1, y_2, \cdots, y_{M_Y}}\)は、それぞれ\(X\)と\(Y\)が取り得る値の集合とする。</p>
            <p><b>結合エントロピーの性質</b></p>
            <p>確率変数\(X\)と\(Y\)の結合エントロピー\(H(X, Y)\)に対し、</p>
            <div class="scroll">
                \begin{align}
                0 \leq H(X, Y) \leq H(X) + H(Y)
                \end{align}
            </div>
            <p>が成り立つ。また、\(H(X, Y) = H(X) + H(Y)\)となるのは、\(X\)と\(Y\)が<b>独立のときのみ</b>である。</p>
            <p><b>ベイズの定理</b></p>
            <div class="scroll">
                \begin{align}
                P(B|A) = \frac{P(B)P(A|B)}{P(B)P(A|B)+P(\overline{B})P(A|\overline{B})}
                \end{align}
            </div>
        <h3>情報量とエントロピー(2)</h3>
            <p><b>条件付きエントロピー</b></p>
            <p>確率変数\(Y\)で条件を付けた\(X\)の<b>条件付きエントロピー</b>\(H(X|Y)\)は、</p>
            <div class="scroll">
                \begin{align}
                H(X|Y) = - \sum_{j = 1}^{M_Y} P(y_j) \sum_{i = 1}^{M_X} P(x_i|y_j) \log_2 P(x_i|y_j)
                \end{align}
            </div>
            <p>により定義される。ただし、\({x_1, x_2, \cdots, x_{M_X}}\)および\({y_1, y_2, \cdots, y_{M_Y}}\)は、それぞれ\(X\)と\(Y\)が取り得る値の集合とする。</p>
            <p><b>条件付きエントロピーの性質</b></p>
            <p>\({x_1, x_2, \cdots, x_{M_X}}\)および\({y_1, y_2, \cdots, y_{M_Y}}\)を取り得る値の集合とする確率変数\(X\)と\(Y\)に関し、以下が成り立つ。</p>
            <ol>
                <li>
                    <div class="scroll">
                        \begin{align}
                        H(X|Y) = - \sum_{j = 1}^{M_Y} \sum_{i = 1}^{M_X} P(x_i, y_j) \log_2 P(x_i|y_j)
                        \end{align}
                    </div>
                </li>
                <li>
                    <div class="scroll">
                        \begin{align}
                        H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
                        \end{align}
                    </div>
                （この式は、\(H(X)\)と\(H(Y)\)に関するベン図を描くと分かりやすい。）</li>
                <li>\(0 \leq H(X|Y) \leq H(X)\)（\(H(X|Y) = H(X)\)は、\(X\)と\(Y\)が独立の時のみ成立。）</li>
                <li>\(0 \leq H(Y|X) \leq H(Y)\)（\(H(Y|X) = H(Y)\)は、\(X\)と\(Y\)が独立の時のみ成立。）</li>
            </ol>
            <p><b>相互情報量と、その性質</b></p>
            <p>確率変数\(X\)と\(Y\)の<b>相互情報量</b>\(I(X;Y)\)は、</p>
            <div class="scroll">
                \begin{align}
                I(X;Y) = H(X) - H(X|Y)
                \end{align}
            </div>
            <p>によって定義される。</p>
            <p>確率変数\(X\)と\(Y\)と相互情報量\(I(X;Y)\)に関して以下が成り立つ。</p>
            <ol>
                <li>
                    <div class="scroll">
                        \begin{align}
                        I(X;Y) &= H(X) - H(X|Y) \\
                        &= H(Y) - H(Y|X) \\
                        &= H(X) + H(Y) - H(X, Y)
                        \end{align}
                    </div>
                </li>
                <li>\(0 \leq I(X;Y) \leq \min {H(X), H(Y)}\)（\I(X;Y)=0\)は、\(X\)と\(Y\)が独立の時のみ成立。）</li>
            </ol>
            <p><b>シャノンの補助定理</b></p>
            <p>\(p_1, p_2, \cdots, p_M\)および\(q_1, q_2, \cdots, q_M\)を</p>
            <div class="scroll">
                \begin{align}
                p_1 + p_2 + \cdots + p_M = 1 \\
                q_1 + q_2 + \cdots + q_M \leq 1
                \end{align}
            </div>
            <p>を満たす任意の非負の数とする（ただし、\(p_i \neq 0\)のときは\(q_i \neq 0\)とする）。このとき、</p>
            <div class="scroll">
                \begin{align}
                - \sum_{i = 1}^{M} p_i \log_2 q_i \geq - \sum_{i = 1}^{M} p_i \log_2 p_i
                \end{align}
            </div>
            <p>が成立する。等号は\(q_i = p_i (i = 1, 2, \cdots, M)\)のとき、またそのときに限って成立する。</p>
            <p>つまり、確率分布\(P = {p_i}_{i = 1}^{M}\)とちょっと違う分布\(q_i\)（ただし総和が1以下）を持ってきて、\(\log_2\)の内側の\(p_i\)と置き換えると、<b>元より少し大きくなる</b>。</p>
        <h3>情報源のモデル(1)</h3>
            <p><b>離散的\(M\)元情報源</b></p>
            <p>\(M\)個の元からなる記号の有限集合\(A = {a_1, a_2, \cdots, a_M}\)を考える。</p>
            <p>\(A\)を<b>情報源アルファベット</b>、各元\(a_i \in A\)を<b>情報源記号</b>と呼ぶ。</p>
            <p>時点0より毎時点（時点は整数値）で、\(A\)上の情報源記号を、ある確率に従って１個ずつ出力する。</p>
            <p>情報源から出力されるデータ = 情報源記号が並んだ記号列（<b>情報源系列</b>と呼ぶ。）</p>
            <p><b>情報源系列の確率分布</b></p>
            <p>時点0から時点\(n-1\)まで（長さ\(n\)）の情報源系列について、</p>
            <ul>
                <li>時点\(i\)での情報源の出力を、確率変数\(X_i\)で表す。つまり、\(X_i\)は\(a_1, a_2, \cdots, a_M\)のいずれか</li>
                <li>情報源系列は\(X_0 X_1 \cdots X_{n-1}\)</li>
            </ul>
            <p>\(X_0, X_1, \cdots, X_{n-1}\)の<b>結合確率分布</b>:</p>
            <div class="scroll">
                \begin{align}
                P_{X_0 X_1 \cdots X_{n-1}} (x_0, x_1, \cdots, x_{n-1}) = [X_0 = x_0, X_1 = x_1, \cdots, X_{n-1} = x_{n-1} \text{となる確率}]
                \end{align}
            </div>
            <p>\(p(x_0, x_1, \cdots, x_{n-1})\)とも書く。</p>
            <P>結合確率分布から、統計的性質は完全に定まる。</P>
            <p><b>情報源の各種モデル</b></p>
            <p>どんなに大きい\(n\)についても、\(X_0, X_1, \cdots, X_{n-1}\)の結合確率分布を与えることができれば、この情報源の統計的性質は完全に記述できる。しかし、一般には困難。</p>
            <p>議論を進めるためには、情報源に何らかの<b>扱いやすい性質</b>を仮定する必要がある。</p>
            <p><b>記憶のない情報源</b></p>
            <p>各時点における情報源記号の発生が、<b>他の時点と独立</b>である情報源</p>
            <p><b>記憶のない定常情報源</b></p>
            <p>記憶のない情報源において、各時点における情報源記号の発生が<b>同一の確率分布</b>\(P_X (x)\)に従う情報源</p>
            <p>記憶のない定常情報源における長さ\(n\)の系列の結合確率分布は、以下の式で表される。</p>
            <div class="scroll">
                \begin{align}
                P_{X_0 X_1 \cdots X_{n-1}} (x_0, x_1, \cdots, x_{n-1}) = \prod_{i = 0}^{n-1} P_X (x_i)
                \end{align}
            </div>
            <p><b>定常情報源の定義</b></p>
            <p>時間をずらしても、統計的性質の変わらない情報源</p>
            <p>定常情報源の出力は、各時点において同一の確率分布に従う。この確率分布を<b>定常分布</b>と呼ぶ。</p>
            <p><b>エルゴード情報源</b></p>
            <p>十分長い任意の出力系列に、その情報源の統計的性質が完全に現れている定常情報源。</p>
            <p>エルゴード的であれば、十分に長い任意の系列は必ず一定の統計的性質を満たす。</p>
            <p><b>情報源の各種モデル</b></p>
            <p>記憶のない定常情報源は必ずエルゴード情報源になる。</p>
            <p>記憶のある定常情報源でエルゴード的なものはある（例：正規マルコフ情報源）</p>
            <p>定常情報源であってもエルゴード的でないものもある。</p>
            <p>例：確率\(\frac{1}{2}\)で0だけからなる系列か、1だけからなる系列を発生する情報源</p>
            <p>情報理論では、ほとんどの場合、エルゴード情報源を扱う。</p>
            <p><b>マルコフ情報源</b></p>
            <p>マルコフ情報源：記憶のある情報源の基本的なモデル。</p>
            <p><b>\(m\)重マルコフ情報源</b></p>
            <p>系列中の<b>過去\(m\)文字の並びにしたがって、次の文字の生起確率が決まるモデル。</b></p>
            <p><b>\(m\)重マルコフ情報源の状態遷移図</b></p>
            <p>離散的\(q\)元情報源を考える。これが\(m\)重マルコフ情報源だとする。</p>
            <p>\(q^m\)個の<b>状態</b>を持ち、各状態において記号の出力確率が定まっていると見なせる。</p>
            <p>出力を一つ発生する度に\(q^m\)個の状態の中を<b>遷移</b>していく。</p>
            <p><b>一般化されたマルコフ情報源</b></p>
            <p>単に、マルコフ情報源と呼ぶ。</p>
            <p>「直前の\(m\)個の出力に対応する」という条件をなくし、より抽象的に（自由な状態図で）情報源を定義できる。</p>
            <p>マルコフ連鎖モデルともいう。</p>
            <p><b>状態の分類</b></p>
            <p><b>過渡状態</b>：十分時間が経過すれば抜け出てしまい、戻ることのない状態。</p>
            <p><b>閉じた状態集合</b>：任意の状態から任意の状態へ遷移可能な（矢印でたどっていくことのできる）状態の集合。</p>
            <p><b>既約マルコフ情報源</b>：任意の状態から任意の状態へ遷移可能なマルコフ情報源。</p>
            <p><b>非周期的状態集合</b>：閉じた状態集合で、ある時間が経過した後の任意の時点において、どの状態にある確率も0でないようなもの。</p>
            <p><b>周期的状態集合</b>：閉じた状態集合がいくつかの部分集合に分割され、その各々が周期的な時点においてのみ現れ得るもの。</p>
            <p><b>正規マルコフ情報源</b>：非周期的状態集合のモデルで定義されるマルコフ情報源。十分な時間が経過した後に、定常情報源として扱えて有益。</p>
        <h3>情報源のモデル(2)</h3>
            <p><b>遷移確率行列</b></p>
            <p>\(N\)個の状態\(s_0, s_1, \cdots, s_{N-1}\)を持つ<b>正規マルコフ情報源</b>を考える。</p>
            <p>状態遷移の仕方は、状態\(s_i\)にあるとき、次の時点で状態\(s_j\)に遷移する確率\(p_{i,j}=P(s_j|s_i)\)により決まる。これを<b>遷移確率</b>という。</p>
            <p>遷移確率\(p_{i, j}\)を\((i, j)\)要素とする\(N \times N\)行列を<b>遷移確率行列</b>と呼ぶ。</p>
            <p>行が\(i\)側（現在状態）で、列が1\(j\)側（次状態）である。</p>
            <p>行ごとに総和が1である。</p>
            <p><b>正規マルコフ情報源の極限分布</b></p>
            <p>正規マルコフ情報源の定義より、\(t > t_0\)となる任意の\(t\)について</p>
            <div class="scroll">
                \begin{align}
                p_{i, j}^{(t)} > 0 \quad (i, j = 0, 1, \cdots, N-1)
                \end{align}
            </div>
            <p>が成り立つようなある正定数\(t_0\)が存在する。</p>
            <p>正規マルコフ情報源では、\(t \rightarrow \infty\)とするとき、\(p_{i, j}^{(t)}\)は\(i\)には無関係な値に収束する。すなわち、</p>
            <div class="scroll">
                \begin{align}
                \lim_{t \rightarrow \infty} p_{i, j}^{(t)} = u_j \quad (j = 0, 1, \cdots, N-1)
                \end{align}
            </div>
            <p>となる\(u_j\)が存在する。</p>
            <p>遷移行列を用いて上式を書き直すと、</p>
            <div class="scroll">
                \begin{align}
                \lim_{t \rightarrow \infty} \Pi = U \quad
                \end{align}
            </div>
            <p>となる。ここで、\(U\)はすべての行が\(\mathbf{u} = (u_0, u_1, \cdots, u_{N-1})\)となる\(N \times N\)行列である。</p>
            <p>時点\(t\)において状態\(s_j\)にいる確率\(P(s^{(t)} = s_j)\)を\(w_{j}^{(t)}\)で表し、</p>
            <div class="scroll">
                \begin{align}
                \mathbf{w}_t = (w_{0}^{(t)}, w_{1}^{(t)}, \cdots, w_{N-1}^{(t)})
                \end{align}
            </div>
            <p>という\(N\)次元ベクトルを定義する。</p>
            <p>時点\(t-1\)で状態\(s_i\)にいる確率が\(w_{i}^{t-1}\)で、状態</p>
        </section>
    </main>
</body>
</html>
