<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【院試対策|北海道大学 情報科学院】走る作曲家のAIカフェ～情報理論編～</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
          <p>「北海道大学大学院情報科学院修士課程入学試験」（令和6年8月実施）の「<b>情報理論</b>」対策ページです。</p>
        </section>
        <section>
          <h2>分野別対策</h2>
          <ul>
            <li><a href="gradexam.html">院試対策 ～はじめに～</a></li>
            <li><a href="gradexam_basicmath.html">院試対策 ～基礎数学編～</a></li>
            <li><a href="gradexam_infomath.html">院試対策 ～情報数学編～</a></li>
            <li><a href="gradexam_statistics.html">院試対策 ～確率・統計編～</a></li>
            <li><a href="gradexam_infotheory.html">院試対策 ～情報理論編～</a></li>
            <li><a href="gradexam_algo.html">院試対策 ～アルゴリズムとデータ構造編～</a></li>
            <li><a href="gradexam_ai.html">院試対策 ～人工知能編～</a></li>
            <li><a href="gradexam_cs.html">院試対策 ～コンピュータシステム編～</a></li>
          </ul>
        </section>
        <section id="algorithms-data-structures">
        <h2>情報理論</h2>
        <p>参考にしたサイトは以下のとおり。</p>
        <p><a href="https://www-alg.ist.hokudai.ac.jp/~atsu/info_theory.html">情報理論 (Information Theory).html</a></p>
        <h3>情報理論とは</h3>
            <p><b>情報理論が取り組む4つの課題</b></p>
            <ul>
                <li>できるだけよい情報源符号化法を見出すこと</li>
                <li>情報源符号化の限界を知ること</li>
                <li>できるだけよい通信路符号化法を見出すこと</li>
                <li>通信路符号化の限界を知ること</li>
            </ul>
        <h3>条件付き確率</h3>
            <p><b>条件付き確率</b></p>
            <p>事象\(B\)の条件のもとで事象\(A\)が起こる条件付き確率は</p>
            <div class="scroll">
                \begin{align}
                P(A|B) = \frac{P(A \cap B}{P(B)}
                \end{align}
            </div>
            で定義される。
            <p><b>乗法定理</b></p>
            <div class="scroll">
                \begin{align}
                P(A \cap B) = P(A|B) P(B)
                \end{align}
            </div>
        <h3>情報量とエントロピー(1)</h3>
            <p><b>情報量</b></p>
            <p>確率\(p\)の事象の生起を知った時に得られる<b>情報量</b>を\(I(p)\)とする。</p>
            <p>\(I(p)\)は次のような性質を満たすべき。</p>
            <ul>
                <li>\(I(p)\)は\( 0 < p \leq 1 \)で単調現象な関数である</li>
                <li>確率\(p_1, p_2\)で起こる二つの互いに独立な事象が同時に起こる確率\(p_1, p_2\)について\(I(p_1 p_2) = I(p_1) + I(p_2)\)</li>
                <li>\(I(p)\)は\( 0 < p \leq 1 \)で連続な関数である</li>
            </ul>
            <p>これらを満たす関数\(I(p)\)は\(I(p) = - \log_a p\)という形しかありえない（\(a > 1\)）</p>
            <p>確率\(p\)で生起する事象が起きたことを知ったときに得られる情報量\(I(p)\)を<b>自己情報量</b>と呼び、\(I(p) = - \log_a p\)と定義する。ただし、\(a > 1\)である。</p>
            <p>\(a = 2\)のとき、単位は<b>ビット</b>という。つまり、確率\(\frac{1}{2}\)で生じる結果を知った時の情報量は1 [bit]である。</p>
            <p><b>平均情報量</b></p>
            <p>\(M\)個の互いに排反な事象\(a_1, a_2, \cdots, a_M\)が起こる確率を\(p_1, p_2, \cdots, p_M\)とする（ただし、\(p_1 + p_2 + \cdots + p_M = 1\)）。</p>
            <p>このうち１つの事象が起こったことを知ったときに得る情報量は\(- \log_2 p_i\)であるから、これを平均した期待値\(\overline{I}\)は、</p>
            <div class="scroll">
                \begin{align}
                \overline{I} &= p_1 (- \log_2 p_1) + p_2 (- \log_2 p_12) + \cdots + p_M (- \log_2 p_M) \\
                &= - \sum_{i = 1}^{M} p_i \log_2 p_i
                \end{align}
            </div>
            <p>となる。これを<b>平均情報量</b>（単位はビット）という。</p>
            <p><b>エントロピー</b></p>
            <p>確率変数\(X\)が取り得る値が\(x_1, x_2, \cdots, p_M\)（ただし、\(p_1 + p_2 + \cdots + p_M = 1\)）であるとき、確率変数\(X\)の<b>エントロピー</b>を</p>
            <div class="scroll">
                \begin{align}
                H(X) = - \sum_{i = 1}^{M} p_i \log_2 p_i
                \end{align}
            </div>
            <p>ビットと定義する。</p>
            <p>一般に、確率分布の偏りが非常に大きいときは平均情報量が小さく、確率分布が均一の時に平均情報量が最大になる。</p>
            <p>エントロピーと平均情報量は、完全に同じ形をしている。</p>
            <p>情報を受け取ると曖昧さが減るので、受け取った量だけエントロピーは減って、0に近づく（<b>受け取った情報量 = エントロピーの変化</b>）</p>
            <p><b>エントロピーの性質</b></p>
            <p>\(M\)個の値を取る確率変数\(X\)のエントロピー\(H(X)\)は次の性質を満たす。</p>
            <ol>
                <li>\(0 \leq H(X) \leq \log_2 M\)</li>
                <li>\(H(X)\)が最小値0となるのは、ある値を取る確率が1で、他のM-1個の値を取る確率がすべて0の時に限る。すなわち、\(X\)の取る値がはじめから確定している場合のみである。</li>
                <li>\(H(X)\)が最大値\(\log_2 M\)となるのは、\(M\)個の値がすべて\(\frac{1}{M}\)で等しい場合に限る。</li>
            </ol>
            <p>エントロピーは「答えのあてにくさ」を表す数値ともいえる。</p>
            <p><b>2つ以上の確率変数を扱う場合</b></p>
            <P>２つの確率変数\(X, Y\)を考える。\(X\)は\(x_1, x_2, \cdots, x_{M_X}\)の値をとり、\(Y\)は\(y_1, y_2, \cdots, y_{M_Y}\)の値を取るものとする。</P>
            <p>確率変数の組\((X, Y)\)の値が\((x, y)\)となる<b>結合確率分布</b>を\(P(x, y)\)と書く。</p>
            <p>組\((X, Y)\)をまとめて考えると、４つの値を取る確率変数\(Z\)のエントロピー\(H(Z)\)として考えることができる。</p>
            <p>確率変数\(X\)と\(Y\)の結合エントロピー\(H(X, Y)\)は、</p>
            <div class="scroll">
                \begin{align}
                H(X, Y) = - \sum_{i = 1}^{M_X} \sum_{j = 1}^{M_Y} P(x_i, y_j) \log_2 P(x_i, y_j)
                \end{align}
            </div>
            <p>により定義される。これを<b>結合エントロピー</b>と呼ぶ。ただし、\({x_1, x_2, \cdots, x_{M_X}}\)および\({y_1, y_2, \cdots, y_{M_Y}}\)は、それぞれ\(X\)と\(Y\)が取り得る値の集合とする。</p>
            <p><b>結合エントロピーの性質</b></p>
            <p>確率変数\(X\)と\(Y\)の結合エントロピー\(H(X, Y)\)に対し、</p>
            <div class="scroll">
                \begin{align}
                0 \leq H(X, Y) \leq H(X) + H(Y)
                \end{align}
            </div>
            <p>が成り立つ。また、\(H(X, Y) = H(X) + H(Y)\)となるのは、\(X\)と\(Y\)が<b>独立のときのみ</b>である。</p>
            <p><b>ベイズの定理</b></p>
            <div class="scroll">
                \begin{align}
                P(B|A) = \frac{P(B)P(A|B)}{P(B)P(A|B)+P(\overline{B})P(A|\overline{B})}
                \end{align}
            </div>
        <h3>情報量とエントロピー(2)</h3>
            <p><b>条件付きエントロピー</b></p>
            <p>確率変数\(Y\)で条件を付けた\(X\)の<b>条件付きエントロピー</b>\(H(X|Y)\)は、</p>
            <div class="scroll">
                \begin{align}
                H(X|Y) = - \sum_{j = 1}^{M_Y} P(y_j) \sum_{i = 1}^{M_X} P(x_i|y_j) \log_2 P(x_i|y_j)
                \end{align}
            </div>
            <p>により定義される。ただし、\({x_1, x_2, \cdots, x_{M_X}}\)および\({y_1, y_2, \cdots, y_{M_Y}}\)は、それぞれ\(X\)と\(Y\)が取り得る値の集合とする。</p>
            <p><b>条件付きエントロピーの性質</b></p>
            <p>\({x_1, x_2, \cdots, x_{M_X}}\)および\({y_1, y_2, \cdots, y_{M_Y}}\)を取り得る値の集合とする確率変数\(X\)と\(Y\)に関し、以下が成り立つ。</p>
            <ol>
                <li>
                    <div class="scroll">
                        \begin{align}
                        H(X|Y) = - \sum_{j = 1}^{M_Y} \sum_{i = 1}^{M_X} P(x_i, y_j) \log_2 P(x_i|y_j)
                        \end{align}
                    </div>
                </li>
                <li>
                    <div class="scroll">
                        \begin{align}
                        H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
                        \end{align}
                    </div>
                （この式は、\(H(X)\)と\(H(Y)\)に関するベン図を描くと分かりやすい。）</li>
                <li>\(0 \leq H(X|Y) \leq H(X)\)（\(H(X|Y) = H(X)\)は、\(X\)と\(Y\)が独立の時のみ成立。）</li>
                <li>\(0 \leq H(Y|X) \leq H(Y)\)（\(H(Y|X) = H(Y)\)は、\(X\)と\(Y\)が独立の時のみ成立。）</li>
            </ol>
            <p><b>相互情報量と、その性質</b></p>
            <p>確率変数\(X\)と\(Y\)の<b>相互情報量</b>\(I(X;Y)\)は、</p>
            <div class="scroll">
                \begin{align}
                I(X;Y) = H(X) - H(X|Y)
                \end{align}
            </div>
            <p>によって定義される。</p>
            <p>確率変数\(X\)と\(Y\)と相互情報量\(I(X;Y)\)に関して以下が成り立つ。</p>
            <ol>
                <li>
                    <div class="scroll">
                        \begin{align}
                        I(X;Y) &= H(X) - H(X|Y) \\
                        &= H(Y) - H(Y|X) \\
                        &= H(X) + H(Y) - H(X, Y)
                        \end{align}
                    </div>
                </li>
                <li>\(0 \leq I(X;Y) \leq \min {H(X), H(Y)}\)（\(I(X;Y)=0\)は、\(X\)と\(Y\)が独立の時のみ成立。）</li>
            </ol>
            <p><b>シャノンの補助定理</b></p>
            <p>\(p_1, p_2, \cdots, p_M\)および\(q_1, q_2, \cdots, q_M\)を</p>
            <div class="scroll">
                \begin{align}
                p_1 + p_2 + \cdots + p_M = 1 \\
                q_1 + q_2 + \cdots + q_M \leq 1
                \end{align}
            </div>
            <p>を満たす任意の非負の数とする（ただし、\(p_i \neq 0\)のときは\(q_i \neq 0\)とする）。このとき、</p>
            <div class="scroll">
                \begin{align}
                - \sum_{i = 1}^{M} p_i \log_2 q_i \geq - \sum_{i = 1}^{M} p_i \log_2 p_i
                \end{align}
            </div>
            <p>が成立する。等号は\(q_i = p_i (i = 1, 2, \cdots, M)\)のとき、またそのときに限って成立する。</p>
            <p>つまり、確率分布\(P = {p_i}_{i = 1}^{M}\)とちょっと違う分布\(q_i\)（ただし総和が1以下）を持ってきて、\(\log_2\)の内側の\(p_i\)と置き換えると、<b>元より少し大きくなる</b>。</p>
        <h3>情報源のモデル(1)</h3>
            <p><b>離散的\(M\)元情報源</b></p>
            <p>\(M\)個の元からなる記号の有限集合\(A = {a_1, a_2, \cdots, a_M}\)を考える。</p>
            <p>\(A\)を<b>情報源アルファベット</b>、各元\(a_i \in A\)を<b>情報源記号</b>と呼ぶ。</p>
            <p>時点0より毎時点（時点は整数値）で、\(A\)上の情報源記号を、ある確率に従って１個ずつ出力する。</p>
            <p>情報源から出力されるデータ = 情報源記号が並んだ記号列（<b>情報源系列</b>と呼ぶ。）</p>
            <p><b>情報源系列の確率分布</b></p>
            <p>時点0から時点\(n-1\)まで（長さ\(n\)）の情報源系列について、</p>
            <ul>
                <li>時点\(i\)での情報源の出力を、確率変数\(X_i\)で表す。つまり、\(X_i\)は\(a_1, a_2, \cdots, a_M\)のいずれか</li>
                <li>情報源系列は\(X_0 X_1 \cdots X_{n-1}\)</li>
            </ul>
            <p>\(X_0, X_1, \cdots, X_{n-1}\)の<b>結合確率分布</b>:</p>
            <div class="scroll">
                \begin{align}
                P_{X_0 X_1 \cdots X_{n-1}} (x_0, x_1, \cdots, x_{n-1}) = [X_0 = x_0, X_1 = x_1, \cdots, X_{n-1} = x_{n-1} \text{となる確率}]
                \end{align}
            </div>
            <p>\(p(x_0, x_1, \cdots, x_{n-1})\)とも書く。</p>
            <P>結合確率分布から、統計的性質は完全に定まる。</P>
            <p><b>情報源の各種モデル</b></p>
            <p>どんなに大きい\(n\)についても、\(X_0, X_1, \cdots, X_{n-1}\)の結合確率分布を与えることができれば、この情報源の統計的性質は完全に記述できる。しかし、一般には困難。</p>
            <p>議論を進めるためには、情報源に何らかの<b>扱いやすい性質</b>を仮定する必要がある。</p>
            <p><b>記憶のない情報源</b></p>
            <p>各時点における情報源記号の発生が、<b>他の時点と独立</b>である情報源</p>
            <p><b>記憶のない定常情報源</b></p>
            <p>記憶のない情報源において、各時点における情報源記号の発生が<b>同一の確率分布</b>\(P_X (x)\)に従う情報源</p>
            <p>記憶のない定常情報源における長さ\(n\)の系列の結合確率分布は、以下の式で表される。</p>
            <div class="scroll">
                \begin{align}
                P_{X_0 X_1 \cdots X_{n-1}} (x_0, x_1, \cdots, x_{n-1}) = \prod_{i = 0}^{n-1} P_X (x_i)
                \end{align}
            </div>
            <p><b>定常情報源の定義</b></p>
            <p>時間をずらしても、統計的性質の変わらない情報源</p>
            <p>定常情報源の出力は、各時点において同一の確率分布に従う。この確率分布を<b>定常分布</b>と呼ぶ。</p>
            <p><b>エルゴード情報源</b></p>
            <p>十分長い任意の出力系列に、その情報源の統計的性質が完全に現れている定常情報源。</p>
            <p>エルゴード的であれば、十分に長い任意の系列は必ず一定の統計的性質を満たす。</p>
            <p><b>情報源の各種モデル</b></p>
            <p>記憶のない定常情報源は必ずエルゴード情報源になる。</p>
            <p>記憶のある定常情報源でエルゴード的なものはある（例：正規マルコフ情報源）</p>
            <p>定常情報源であってもエルゴード的でないものもある。</p>
            <p>例：確率\(\frac{1}{2}\)で0だけからなる系列か、1だけからなる系列を発生する情報源</p>
            <p>情報理論では、ほとんどの場合、エルゴード情報源を扱う。</p>
            <p><b>マルコフ情報源</b></p>
            <p>マルコフ情報源：記憶のある情報源の基本的なモデル。</p>
            <p><b>\(m\)重マルコフ情報源</b></p>
            <p>系列中の<b>過去\(m\)文字の並びにしたがって、次の文字の生起確率が決まるモデル。</b></p>
            <p><b>\(m\)重マルコフ情報源の状態遷移図</b></p>
            <p>離散的\(q\)元情報源を考える。これが\(m\)重マルコフ情報源だとする。</p>
            <p>\(q^m\)個の<b>状態</b>を持ち、各状態において記号の出力確率が定まっていると見なせる。</p>
            <p>出力を一つ発生する度に\(q^m\)個の状態の中を<b>遷移</b>していく。</p>
            <p><b>一般化されたマルコフ情報源</b></p>
            <p>単に、マルコフ情報源と呼ぶ。</p>
            <p>「直前の\(m\)個の出力に対応する」という条件をなくし、より抽象的に（自由な状態図で）情報源を定義できる。</p>
            <p>マルコフ連鎖モデルともいう。</p>
            <p><b>状態の分類</b></p>
            <p><b>過渡状態</b>：十分時間が経過すれば抜け出てしまい、戻ることのない状態。</p>
            <p><b>閉じた状態集合</b>：任意の状態から任意の状態へ遷移可能な（矢印でたどっていくことのできる）状態の集合。</p>
            <p><b>既約マルコフ情報源</b>：任意の状態から任意の状態へ遷移可能なマルコフ情報源。</p>
            <p><b>非周期的状態集合</b>：閉じた状態集合で、ある時間が経過した後の任意の時点において、どの状態にある確率も0でないようなもの。</p>
            <p><b>周期的状態集合</b>：閉じた状態集合がいくつかの部分集合に分割され、その各々が周期的な時点においてのみ現れ得るもの。</p>
            <p><b>正規マルコフ情報源</b>：非周期的状態集合のモデルで定義されるマルコフ情報源。十分な時間が経過した後に、定常情報源として扱えて有益。</p>
        <h3>情報源のモデル(2)</h3>
            <p><b>遷移確率行列</b></p>
            <p>\(N\)個の状態\(s_0, s_1, \cdots, s_{N-1}\)を持つ<b>正規マルコフ情報源</b>を考える。</p>
            <p>状態遷移の仕方は、状態\(s_i\)にあるとき、次の時点で状態\(s_j\)に遷移する確率\(p_{i,j}=P(s_j|s_i)\)により決まる。これを<b>遷移確率</b>という。</p>
            <p>遷移確率\(p_{i, j}\)を\((i, j)\)要素とする\(N \times N\)行列を<b>遷移確率行列</b>と呼ぶ。</p>
            <p>行が\(i\)側（現在状態）で、列が\(j\)側（次状態）である。</p>
            <p>行ごとに総和が1である。</p>
            <p><b>正規マルコフ情報源の極限分布</b></p>
            <p>正規マルコフ情報源の定義より、\(t > t_0\)となる任意の\(t\)について</p>
            <div class="scroll">
                \begin{align}
                p_{i, j}^{(t)} > 0 \quad (i, j = 0, 1, \cdots, N-1)
                \end{align}
            </div>
            <p>が成り立つようなある正定数\(t_0\)が存在する。</p>
            <p>正規マルコフ情報源では、\(t \rightarrow \infty\)とするとき、\(p_{i, j}^{(t)}\)は\(i\)には無関係な値に収束する。すなわち、</p>
            <div class="scroll">
                \begin{align}
                \lim_{t \rightarrow \infty} p_{i, j}^{(t)} = u_j \quad (j = 0, 1, \cdots, N-1)
                \end{align}
            </div>
            <p>となる\(u_j\)が存在する。</p>
            <p>遷移行列を用いて上式を書き直すと、</p>
            <div class="scroll">
                \begin{align}
                \lim_{t \rightarrow \infty} \Pi = U \quad
                \end{align}
            </div>
            <p>となる。ここで、\(U\)はすべての行が\(\mathbf{u} = (u_0, u_1, \cdots, u_{N-1})\)となる\(N \times N\)行列である。</p>
            <p>時点\(t\)において状態\(s_j\)にいる確率\(P(s^{(t)} = s_j)\)を\(w_{j}^{(t)}\)で表し、</p>
            <div class="scroll">
                \begin{align}
                \mathbf{w}_t = (w_{0}^{(t)}, w_{1}^{(t)}, \cdots, w_{N-1}^{(t)})
                \end{align}
            </div>
            <p>という\(N\)次元ベクトルを定義する。</p>
            <p>時点\(t-1\)で状態\(s_i\)にいる確率が\(w_{i}^{t-1}\)で、状態\(s_i\)にいるときに次の時点で状態\(s_j\)へと遷移する確率が\(p_{i, j}\)であるから、</p>
            <div class="scroll">
                \begin{align}
                w_{j}^{(t)} &= \sum_{i = 0}^{N-1} w_{i}^{t-1} p_{i, j} \\
                \mathbf{w}_t &= \mathbf{w}_{t-1} \Pi
                \end{align}
            </div>
            <p>これを繰り返せば\(\mathbf{w}_{t} = \mathbf{w}_{0} \Pi^{t}\)を得る。ここで、\(\mathbf{w}_0\)は時点0における状態分布（<b>初期分布</b>）である。</p>
            <p>\(mathbf{w}_t\)の\(t \rightarrow \infty\)とした極限を<b>極限分布</b>と呼ぶ。</p>
            <p>正規マルコフ情報源では、次式が成り立つ。</p>
            <div class="scroll">
                \begin{align}
                \lim_{t \rightarrow \infty} \mathbf{w}_t = \mathbf{w}_0 \lim_{t \rightarrow \infty} \Pi^t = \mathbf{w}_0 U = \mathbf{u}
                \end{align}
            </div>
            <p>十分時間が経過すれば、初期分布がどうであれ、状態分布は定常的な確率分布（定常分布）に落ち着く</p>
            <p>正規マルコフ情報源が落ち着く定常分布を</p>
            <div class="scroll">
                \begin{align}
                \mathbf{w} = (w_0, w_1, \cdots, w_{N-1})とする。
                \end{align}
            </div>
            <p>とする。\(w_i\)は確率なので、当然ながら</p>
            <div class="scroll">
                \begin{align}
                w_0 + w_1 + \cdots + w_{N-1} = 1
                \end{align}
            </div>
            <p>が成り立つ。</p>
            <p>ある時点の状態分布が定常的で\(\mathbf{w}\)であるとすれば、次の時点の状態分布も\(\mathbf{w}\)でなければならないので、\(\mathbf{w}\)は</p>
            <div class="scroll">
                \begin{align}
                \mathbf{w} \Pi = \mathbf{w}
                \end{align}
            </div>
            <p>を満たさなければならない。</p>
            <p>正規マルコフ情報源の遷移確率行列\(\Pi\)に対しては、この式を満たす\(\mathbf{w}\)が唯一存在し、極限分布と一致する。</p>
            <p><b>情報源の1次エントロピー</b></p>
            <p>次のような\(M\)元定常情報源\(S\)を考える。</p>
            <ul>
                <li>情報源アルファベット\(A = {a_1, a_2, \cdots, a_M}\)</li>
                <li>記号\(a_k\)の生起確率\(p_k\)</li>
            </ul>
            <p>このとき、</p>
            <div class="scroll">
                \begin{align}
                H_1 (S) = - \sum_{k = 1}^{M} p_k \log_2 p_k
                \end{align}
            </div>
            <p>を情報源\(S\)の<b>1次エントロピー</b>と呼ぶ。</p>
            <p>記憶のない定常情報源の場合、その1次エントロピーは、情報源からの出力を1個受け取ったときに得られる平均的な情報量と考えることができる。</p>
            <p><b>情報源の\(n\)次エントロピー</b></p>
            <p>長さ\(n\)の系列を考え、その系列全体のエントロピーから1記号あたりのエントロピーを算出することを考える。</p>
            <p>\(M\)元情報源の\(n\)個の出力をまとめて1個の記号とみなすと、\(M^n\)元の情報源とみなせる。</p>
            <p>これを<b>\(n\)次拡大情報源</b>といい、\(S^n\)で表す。</p>
            <p>このとき、</p>
            <div class="scroll">
                \begin{align}
                H_n (S) = \frac{H_1 (S^n)}{n}
                \end{align}
            </div>
            <p>を情報源\(S\)の<b>\(n\)次エントロピー</b>と呼ぶ。</p>
            <p>記憶のない定常情報源の場合、その\(n\)次エントロピーは、1次エントロピーと一致する。</p>
            <p><b>情報源のエントロピー</b></p>
            <p>情報源\(S\)の\(n\)次エントロピーの極限</p>
            <div class="scroll">
                \begin{align}
                H(s) = \lim_{n \rightarrow \infty} H_n (s) = \lim_{n \rightarrow \infty} \frac{H_1 (S^n)}{n}
                \end{align}
            </div>
            <p>を情報源\(S\)の<b>エントロピー</b>と呼ぶ。</p>
            <p>これは、十分長い時間をかけて系列を観測した時の1記号当たりの平均情報量と考えることができる。</p>
            <p>記憶のない定常情報源の場合、そのエントロピーは1次エントロピーと一致する。</p>
            <p><b>正規マルコフ情報源のエントロピー</b></p>
            <p>次のような正規マルコフ情報源\(S\)を考える。</p>
            <ul>
                <li>情報源アルファベット \(a_1, a_2, \cdots, a_M\)</li>
                <li>\(N\)個の状態\(s_0, s_1, \cdots, s_{N-1}\)</li>
                <li>定常分布\((w_0, w_1, \cdots, w_{N-1})\)</li>
                <li>状態\(s_i\)にあるときに記号\(a_k\)を発生する確率\(P(a_k | s_i)\)</li>
            </ul>
            <p>この情報源\(S\)に対するエントロピー\(H(S)\)は</p>
            <div class="scroll">
                \begin{align}
                H(s) = - \sum_{i = 0}^{N-1} w_i (\sum_{k = 1}^{M} P(a_k | s_i) \log_2 P(a_k | s_i))
                \end{align}
            </div>
            <p>である。</p>
            <p>結局、正規マルコフ情報源\(S\)のエントロピー\(H(S)\)の求め方は以下の通りである。</p>
            <ol>
                <li>定常分布を求める</li>
                <li>各状態\(s_i\)において、記憶のない情報源とみなし、その場合のエントロピー\(H_{s_i} (S)\)を求める</li>
                <li>上で求めたエントロピー\(H_{s_i} (S)\)を、定常分布にしたがった割合で合算する</li>
            </ol>
        <h3>情報源符号化とその限界</h3>
            <p><b>平均符号長とは</b></p>
            <p>情報源符号化の目的の1つは、できるだけ「良い」情報源符号化法を設計することである。</p>
            <p>われわれの目的は、通信路をできるだけ効率よく圧縮することであるから、同じ情報系列を平均として短い符号列系に符号化できれば、「良い」符号化となる。</p>
            <p>そこで、言い換えると、1情報源あたり符号系列の長さの平均値をできるだけ小さくしたい。これを<b>平均符号長</b>と呼ぶ。</p>
            <p><b>平均符号長の定義</b></p>
            <p>情報源アルファベットが\(\Sigma = {a_1, a_2, \cdots, a_M}\)で、発生確率が\(P(a_i) = p_i (i = 1, 2, \cdots, M)\)で与えられる記憶のない情報源\(S\)を考える。</p>
            <p>符号\(C\)によって、情報源記号\(a_1, a_2, \cdots, a_M\)のそれぞれに対応付けられた符号語の長さを\(l_1, l_2, \cdots, l_M\)とする。このとき、1情報源記号当たりの<b>平均符号長</b>は、</p>
            <div class="scroll">
                \begin{align}
                L &= p_1 l_1 + \cdots + p_M l_M \\
                &= \sum_{i = 1}^{M} p_i l_i
                \end{align}
            </div>
            <p>で与えられる。</p>
            <p><b>瞬時符号とは</b></p>
            <p>符号化は、一意に復号できても、前から読んで戻せるとは限らない。そこで、符号化は、一意復号可能かどうかだけでなく、前から読んだときに、そのまま後戻りせずに復号可能だと便利である。</p>
            <p>符号\(C\)が<b>瞬時符号</b>であるとは、符号化を前から読んでいったときに、ある符号語のパターンが現れたら、それを直ちに復号できるときをいう。そうでない符号を<b>非瞬時符号</b>という。</p>
            <p><b>瞬時符号の条件</b></p>
            <p>ある符号語\(x\)が別の符号語\(y\)の頭の部分のパターンと一致するとき、\(x\)は\(y\)の<b>語頭</b>という。</p>
            <p>瞬時符号であるためには、どの符号語も他の符号語の語頭であってはならない。これを<b>語頭条件</b>と呼ぶ。</p>
            <p><b>符号の木と瞬時符号の関係</b></p>
            <p>瞬時符号は、符号語がすべて葉に対応付けられている。非瞬時符号は、葉以外の節点にも対応付けられている。</p>
            <p><b>クラフトの不等式</b></p>
            <p>長さが\(l_1, l_2, \cdots, l_M\)となる\(M\)個の符号語を持つ\(q\)元符号で瞬時符号となるものが存在するための必要十分条件は、</p>
            <div class="scroll">
                \begin{align}
                q^{-l_1} + q^{-l_2} + \cdots + q^{-l_M} \leq 1 
                \end{align}
            </div>
            <p>が満たされることである。</p>
            <p>※「存在する」としか言っていないことに注意。「この不等式を満たすから瞬時符号」とは言えない。</p>
            <p>※実は、一意復号可能である必要十分条件も、この式を満たすことであり、<b>マクラミンの不等式</b>と呼ぶ。</p>
            <p><b>平均符号長\(L\)の限界に関する定理その１</b></p>
            <p>定常分布を持つ情報源\(S\)の各情報源記号を一意復号可能な\(r\)元符号に符号化したとき、その平均符号長\(L\)は</p>
            <div class="scroll">
                \begin{align}
                \frac{H_1 (S)}{\log_2 r} \leq L
                \end{align}
            </div>
            <p>を満たす。また、平均符号長\(L\)が</p>
            <div class="scroll">
                \begin{align}
                L < \frac{H_1 (S)}{\log_2 r} + 1
                \end{align}
            </div>
            <p>となる\(r\)元瞬時符号を作ることができる。</p>
            <p><b>平均符号長\(L\)の限界に関する定理その２</b></p>
            <p>情報源\(S\)は、任意の一意復号可能な\(r\)元符号で符号化する場合、その平均符号長\(L\)は、</p>
            <div class="scroll">
                \begin{align}
                \frac{H (S)}{\log_2 r} \leq L
                \end{align}
            </div>
            <p>を満たす。また、任意の正数\(\epsilon > 0\)について平均符号長\(L\)が</p>
            <div class="scroll">
                \begin{align}
                L < \frac{H (S)}{\log_2 r} + \epsilon
                \end{align}
            </div>
            <p>となる\(r\)元瞬時符号を作ることができる。</p>
            <p>つまり、どんなに工夫しても、平均符号長\(L\)はエントロピー\(H(S)\)までしか改善できない。（2元符号の場合、\(\log_2 r = 1\)）</p>
        <h3>情報源符号化法(1)</h3>
            <p><b>ハフマン符号はなぜ大事か？</b></p>
            <p>ハフマン符号は<b>コンパクト符号</b>である。</p>
            <p>コンパクト符号とは、1記号ずつ符号化する際、その平均符号長を最小とする効率の良い符号のこと。</p>
            <p><b>ハフマン符号の作り方</b></p>
            <p><b>基本アイディア</b>：最も確率が低い2つの情報源記号を繰り返し選んでまとめながら、下から順に符号木を作る。</p>
            <p>具体的な手順は以下のとおり。</p>
            <ol>
                <li>まず、確率の高い順に記号を並び替える。</li>
                <li>各記号に対応する符号木の葉を作る。葉には確率を添えてかいておく。</li>
                <li>最も確率が小さい葉を2つ選び、それを集約するためのノードを新たに作って枝で結ぶ。そのノードを新しい葉として扱い、元の二つの葉の確率を足し合わせたものを添える。</li>
                <li>3. を繰り返して符号木を作る。</li>
                <li>各ノードから葉へ向かう方向の2本の枝に、0と1のラベルを割り当てる（割り当て方は任意で良い）。</li>
            </ol>
            <p>あとは、構成した符号木を用いて、根から各々の葉へ至るパスをなぞりながら、ラベルの列を符号語として記号に割り当てればよい。</p>
            <p>ハフマン符号は数通り作成できる。</p>
            <p>符号語が符号の木の葉にだけ割り当てられているハフマン符号は瞬時符号である。</p>
            <p><b>ブロック符号化</b></p>
            <p>1,0をそれぞれ確率0.2, 0.8で発生する記憶のない2元定常情報源からの系列を圧縮したい（確率に偏りがあるので圧縮できるはず）。</p>
            <p>情報源の一記号ごとに符号化すると、</p>
            <p>（2元情報源）0, 1 → 0, 1（2元符号化）</p>
            <p>となって、全く効率が上がらない。</p>
            <p>この解決方法として、連続する何個かの情報源符号をまとめて符号化する、というのがある。</p>
            <p>一定個数の情報源記号ごとにまとめて符号化する方法を<b>ブロック符号化</b>と呼ぶ。</p>
            <p>特に、元の情報源\(S\)に対し、\(n\)次拡大情報源\(S^n\)を考え、その上の記号に対してハフマン符号化を行う方法を、<b>ブロックハフマン符号化</b>と呼ぶ。</p>
        <h3>情報源符号化法(2)</h3>
            <p><b>ブロックハフマン符号化の問題点</b></p>
            <p>ブロックハフマン符号化におけるブロック長\(n\)を十分大きくすれば、1情報源記号あたりの平均符号長をいくらでも下限に近づけられる。</p>
            <p>しかし、\(n\)を大きくすると、記号の数が急増する。</p>
            <p>\(M\)元情報源の場合、符号化すべき長さ\(n\)の情報源系列の数が\(M^n\)個に増大する（ハフマン符号化が困難になる）。</p>
            <p><b>非等長情報系列に対する符号化</b></p>
            <p>符号化すべき情報源系列を非等長にすることを考える。</p>
            <p>すなわち、長い情報源系列と短い情報源系列を組み合わせ、長いがよく発生する系列に、より短い符号語を割り当てる。</p>
            <p>利点：符号化する情報源系列の数を減らして、符号化のために記憶すべき表を削減できる。</p>
            <p>情報源から出力される任意の系列が、一意に分解できなければならない。</p>
            <p><b>分節木を用いた情報源系列の分割</b></p>
            <p>分節木とよばれる木構造を用いて、情報源系列を長さの異なる系列（ブロック）に分割し、各ブロックに対して符号化を行う。分節木の各葉ノードはある1つの系列に対応している。</p>
            <p>1記号あたりの平均符号長は、分節木を用いて算出した各ブロックの平均長で、符号木を用いて算出した平均符号長を割ることによって求められる。</p>
            <p><b>ランレングス・ハフマン符号化</b></p>
            <p>系列中に同じ記号が連続するとき、その連続する長さを符号化して送る方法を一般に、<b>ランレングス符号化</b>と呼ぶ。</p>
            <p>ランでブロック化してからハフマン符号化する方法を、<b>ランレングス・ハフマン符号化</b>と呼ぶ。</p>
            <p>例として、1,0の出現確率が、それぞれ\(p, 1-p\)の無記憶定常情報源\(S\)について、\(N-1\)個までの0のランを符号化することを考える。</p>
            <p>つまり、1, 10, 100, 1000, ... という系列を考える。
            <p>これら\(N\)個の系列の平均長\(\overline{n}\)は、</p>
            <div class="scroll">
                \begin{align}
                \overline{n} &= \sum_{i = 0}^{N-2} (i+1)(1-p)^i p + (N-1)(1-p)^{N-1} \\
                &= \frac{1-(1-p)^{N-1}}{p}
                \end{align}
            </div>
            <p>これらの系列をハフマン符号化したときの平均符号長\(L_N\)は次を満たす。</p>
            <div class="scroll">
                \begin{align}
                L_N &< - \sum_{i = 0}^{N-1} p_i \log_2 p_i + 1 \\
                    &= H(S) \overline{n} + 1
                \end{align}
            </div>
            <p>よって1記号あたりの平均符号長\(L_r\)は</p>
            <div class="scroll">
                \begin{align}
                L_r = \frac{L_N}{\overline{n}} < H(S) + \frac{1}{overline{n}}
                \end{align}
            </div>
            <p>となる。ブロックハフマン符号化の場合は</p>
            <div class="scroll">
                \begin{align}
                L_h < H(S) + \frac{1}{n} = H(S) + \frac{1}{\log_2 N}
                \end{align}
            </div>
            <p>である。</p>
            <p><b>ひずみを入れた情報源符号化</b></p>
            <p>通信路でひずみが入るのではなく、符号化時に（わざと）ひずみを入れる。</p>
            <p>元の情報量を削って通信することに相当し、そうすることで圧縮率を向上させられる。</p>
            <p><b>どのくらい平均符号長の限界を下げられるか？</b></p>
            <p>1情報源記号あたりの平均符号長の下限 = エントロピー \(H(S)\)だが、ひずみを許した場合、出力\(Y\)の値を知っても、元の入力\(X\)に関してなお平均して\(H(X|Y)\)のあいまいさが残る。</p>
            <p>そのため、伝えられる情報の量は\(H(X) - H(X|Y) = I(X;Y)\)となって、ひずみを許した場合の限界は相互情報量で表されることがわかる。</p>
            <p><b>ひずみ測度</b></p>
            <p>ひずみ測度とは、\(x\)と\(y\)の相違を評価する関数\(d(x, y)\)のことである。この関数の値が大きいほど、ひずみが大きい。次の性質を持つ。</p>
            <div class="scroll">
                \begin{align}
                d(x, y) \geq 0 \\
                x = y \text{のとき} d(x, y) = 0
                \end{align}
            </div>
            <p>ひずみ測度の平均値を<b>平均ひずみ</b>と呼び、\(\overline{d}\)で表す。</p>
            <div class="scroll">
                \begin{align}
                \overline{d} = \sum_{x} \sum_{y} d(x, y) P_{XY} (x, y)
                \end{align}
            </div>
            <p><b>ひずみ測度の例</b></p>
            <p>例１）情報源アルファベットを\(\Sigma = {0,1}\)とし、ひずみ測度を</p>
            <div class="scroll">
                \begin{align}
                d(x, y) = 
                \begin{cases}
                0; \quad x=y \\
                1; \quad x \neq y
                \end{cases}
                \end{align}
            </div>
            <p>とする。このとき、平均ひずみは</p>
            <div class="scroll">
                \begin{align}
                \overline{d} &= \sum_{x} \sum_{y} d(x, y) P_{XY} (x, y) \\
                &= P(1, 0) + P(0, 1)
                \end{align}
            </div>
            <p>となる。これは要するに、符号器の出力が元の情報源の出力と異なる確率であり、通常<b>ビット誤り率</b>と呼ばれる。</p>
            <p>例２）情報源アルファベットを有限個の整数または実数の集合とする。このとき、ひずみ測度を\(d(x, y) = |x- y|^2\)とすれば、平均ひずみは<b>2乗平均誤差</b>と呼ばれる量になる。</p>
            <p><b>平均ひずみと相互情報量の関係</b></p>
            <p>相互情報量\(I(X;Y)\)が同じでも、平均ひずみ\(\overline{d}\)は同じとは限らない。</p>
            <p>つまり、平均ひずみ\(overline{d}\)が同じでも、\(I(X;Y)\)は符号の仕方で異なる。</p>
            <p>ある与えられた値\(D\)に対し、平均ひずみ\(\overline{d}\)が\(\overline{d} \leq D\)を満たす条件下で、あらゆる情報源符号化法を考えたときの相互情報量\(I(X;Y)\)の最小値を考え、これを\(R(D)\)と表す。</p>
            <p>すなわち、</p>
            <div class="scroll">
                \begin{align}
                R(D) = \min_{\overline{d} \leq D} {I(X;Y)}
                \end{align}
            </div>
            <p>これを情報源\(S\)の<b>測度・ひずみ関数</b>と呼ぶ。つまり、これが平均符号長の下限になる。</p>
            <p><b>ひずみが許される場合の情報源符号化定理</b></p>
            <p>平均ひずみ\(\overline{d}\)を\(D\)以下に抑えるという条件の下で、任意の正数\(\epsilon\)に対して、情報源\(S\)を1情報源記号あたりの平均符号長\(L\)が</p>
            <div class="scroll">
                \begin{align}
                R(D) \leq L < R(D) + \epsilon
                \end{align}
            </div>
            <p>となるような2元符号へ符号化できる。しかし、どのような符号化を行っても、\(\overline{d} \leq D\)である限り、\(L\)をこの式の左辺より小さくすることはできない。</p>
            <p>つまり、1情報源記号当たりの平均符号長が、測度・ひずみ関数\(R(D)\)にいくらでも近づく符号化法が存在する。</p>
        <h3>通信路のモデル</h3>
            <p><b>通信路符号化の考え方</b></p>
            <p>今、自分が、ある通信路を通じて、0と1のビット列で表した情報を相手に送るとする。</p>
            <p>しかし、通信路は誤りを起こす可能性がある。</p>
            <p>もし送った情報に誤りが入ると、元のデータに戻せなくて、正しくない情報が相手に伝わってしまう。</p>
            <p>次のことは可能か？</p>
            <ol>
                <li>受け取ったデータを見るだけで、誤りが入っているかどうかがわかる（<b>誤り検出</b>）</li>
                <li>データに誤りがあるとわかったら、その誤りを除いて、元のデータを復元できる（<b>誤り訂正</b>）</li>
            </ol>
            <p>これらは、元のデータに少し「ムダ」を入れて符号化して、少しくらい誤りが入ってもわかるようにすることで、可能となる。</p>
            <p><b>通信路の統計的表現</b></p>
            <p>雑音のある離散的通信路の定義：時点ごとに一つの記号が入力され、一つの記号が出力される。出力は入力から一意に定まるのではなく、確率的に決まる。</p>
            <p>つまり、以下のようなイメージ。</p>
            <div class="scroll">
                \begin{align}
                \text{　入力アルファベット　} A = {a_1, a_2, \cdots, a_r} \xrightarrow{X_t} \text{　通信路　} \xrightarrow{Y_t} \text{　出力アルファベット　} B = {b_1, b_2, \cdots, b_s}
                \end{align}
            </div>
            <p>\(|A| = |B| = r\)のときは、<b>\(r\)元通信路</b>という。</p>
            <p><b>記憶のない定常通信路</b></p>
            <p>各時点の出力の現れ方が、その時点の入力には関係するが、それ以外の時点の入力と出力とは独立であるような通信路を、<b>記憶のない通信路</b>という。</p>
            <p>さらに、時間をずらしても統計的性質が変わらないとき、これを<b>記憶のない定常通信路</b>と呼ぶ。</p>
            <p>記憶のない定常通信路では、入力\(X\)が通信路に投入されたときに出力\(Y\)が出る条件付き確率\(P_{Y|X} (y|x)\)が、すべての時点において同一である。したがって、</p>
            <div class="scroll">
                \begin{align}
                P_{Y_0 \cdots Y_{n-1} | X_0 \cdots X_{n-1}} (y_0, \cdots, y_{n-1} | x_0, \cdots, x_{n-1}) = \prod_{i = 0}^{n-1} P_{Y|X} (y_i | x_i)
                \end{align}
            </div>
            <p><b>通信路行列</b></p>
            <p>\(r\)元入力アルファベット\(A = {a_1, a_2, \cdots, a_r}\)、\(s\)元出力アルファベット\(B = {b_1, b_2, \cdots, b_s}\)、入出力の関係が条件付き確率\(p_{ij} = P_{Y|X} (b_j |a_i)\)で与えられる記憶のない定常通信路を考える。</p>
            <p>つまり、以下のようなイメージ。</p>
            <div class="scroll">
                \begin{align}
                \text{　入力　} a_i \xrightarrow{p_{ij}} \text{　出力　}b_j
                \end{align}
            </div>
            <p>このとき、通信路行列は以下のようにかける。行が入力で、列が出力である。</p>
            <div class="scroll">
                \begin{align}
                T = \begin{bmatrix}
                p_{11} & p_{12} & \cdots & p_{1s} \\
                p_{21} & p_{22} & \cdots & p_{2s} \\
                \vdots & \vdots & \ddots & \vdots \\
                p_{r1} & p_{r2} & \cdots & p_{rs}
                \end{bmatrix}
                \end{align}
            </div>
            <p><b>加法的2元通信路</b></p>
            <p>入力と出力のアルファベットが共に{0,1}であるような2元通信路は、誤りの有無を用いて表すことができる。</p>
            <p>\(t\)時点での誤りを確率変数\(E_t \in {0,1}\)で表すと、出力\(Y_t\)は入力\(X_t\)に誤り\(E_t\)を加えたものとみなせる。</p>
            <div class="scroll">
                \begin{align}
                Y_t = X_t \oplus E_t \\
                E_t = 
                \begin{cases}
                0 \quad \text{誤りなし} \\
                1 \quad \text{誤り発生}
                \end{cases}
                \end{align}
            </div>
            <ul>
                <li>入力0　→　出力0</li>
                <li>入力0　→　出力1</li>
                <li>入力1　→　出力0</li>
                <li>入力1　→　出力1</li>
            </ul>
            <p>誤りの発生は入力と統計的に独立であると仮定される。</p>
            <p><b>ランダム誤り通信路</b></p>
            <p>加法的2元通信路の誤り源\(S_E\)が、1,0をそれぞれ確率\(p, 1-p\)で発生させる記憶のない定常2元情報源とする。このとき、0から1への誤りも、その逆も、他の時点の入出力とは無関係に確率\(p\)で発生する。これは<b>2元対称通信路</b>に他ならない。</p>
            <p>このような誤りを<b>ランダム誤り</b>といい、誤りの発生確率\(p\)を<b>ビット誤り率</b>という。</p>
            <p><b>バースト誤り通信路</b></p>
            <p>誤りが一度生じると、その後しばらくの間は連続して誤りが発生すると考えるモデル（誤り源に記憶がある代表的なモデル）。</p>
            <p>密集して生じる誤りを<b>バースト誤り</b>と呼ぶ。</p>
            <p><b>ソリッドバースト誤りの平均長</b></p>
            <p>誤り系列における1の連続（ラン）を任意に一つ取り出し、その長さが\(l\)となる確率\(P_L (l)\)を求めると、</p>
            <div class="scroll">
                \begin{align}
                P_L (l) = (1 - p)^{l-1} p \quad (l = 1,2,\cdots)
                \end{align}
            </div>
            <p>となる。</p>
            <p>バースト誤りの長さ（バースト長）の平均値\(\overline{l}\)は次のようになる。</p>
            <div class="scroll">
                \begin{align}
                \overline{l} &= \sum_{l=1}^{\infty} l P_L (l) \\
                &= p \sum_{l=1}^{\infty} l (1-p)^{l-1} \\
                &= \frac{1}{p}
                \end{align}
            </div>
            <p><b>その他のバースト誤りモデル</b></p>
            <p><b>ギルバートモデル</b>：正誤が混在するバースト誤り</p>
            <p><b>フリッチマンモデル</b>：ギルバートモデルの良状態を増やしたもの</p>
        <h3>通信路符号化の限界(1)</h3>
            <p><b>通信路符号の基礎概念</b></p>
            <p>通信路符号化の目的：信頼性の向上　→　そのために冗長性を付加</p>
            <p>長さ\(k\)のブロック\(\mathbf{x} \in \Sigma^k\)を長さ\(n\)の<b>符号語</b>\(\mathbf{w_x} \in A^n\)に符号化（\(A\)は\(r\)元アルファベット）</p>
            <p>\(q^k < r^n\)：符号語として\(A^n\)の一部のみ使用（冗長性の付加）</p>
            <p><b>通信路符号</b>：\(A^n\)の中から選ばれた系列の集合\(C\)</p>
            <p><b>情報速度と冗長度</b></p>
            <p>符号\(C\)に含まれる符号語の数を\(M\)とする。</p>
            <p>符号\(C\)の<b>情報（伝送）速度</b>\(R\):</p>
            <ul>
                <li>符号\(C\)を用いて伝達可能な1記号あたりの最大平均情報量</li>
                <li>\(R = \frac{\log_2 M}{n}\)　（ビット/記号）</li>
                <li>符号語を受け取ることにより得られる最大平均情報量\(\log_2 M\)を符号語の長さ\(n\)で割った値</li>
            </ul>
            <p>\(A^n\)に含まれる\(r^n\)個の系列すべてを符号語とすれば、情報速度は最大値</p>
            <div class="scroll">
                \begin{align}
                R_{max} = \frac{\log_2 r^n}{n} = \log_2 r
                \end{align}
            </div>
            <p>となる。\(R < R_{max}\)とすることで、誤りの訂正や検出が可能となる（すべての系列を使うと、誤りを検知できない）。</p>
            <p>情報速度\(R\)の符号\(C\)の効率（符号化率）\(\eta\)と冗長度\(\rho\):</p>
            <div class="scroll">
                \begin{align}
                \eta = \frac{R}{R_{max}}, \quad \rho = 1 - \eta
                \end{align}
            </div>
            <p>（ただし、\(0 \leq \eta \leq 1, \quad 0 \leq \rho \leq 1\)）が成り立つ。</p>
            <p><b>最尤復号法(1)</b></p>
            <p>\(\mathbf{\Omega_1}, \mathbf{\Omega_2}, \cdots, \mathbf{\Omega_M},\): 符号語\(\mathbf{w_1}, \mathbf{w_2}, \cdots, \mathbf{w_M}\)の復号領域</p>
            <p>正しく復号される確率を最大にする復号領域の求め方は、（符号語の発生確率が等確率の場合は）<b>最尤復号法</b></p>
            <p>\(P(\mathbf{y}|\mathbf{w_i})\): \(\mathbf{w_i}\)を送ったとき\(\mathbf{y}\)が受信される確率</p>
            <p>最尤復号法：\(\mathbf{y}\)が受信されたとき、\(P(\mathbf{y}|\mathbf{w_i})\)が最大の\(\mathbf{w_i}\)に復号</p>
            <p>最尤復号法の復号領域：\(\mathbf{\Omega_i} = \{\mathbf{y} | i = \max_j P(\mathbf{y} | \mathbf{w_j})\}\)</p>
            <p>\(P_C(\mathbf{w_i}) = \sum_{\mathbf{y} \in \mathbf{\Omega_i}}\):符号語\(\mathbf{w_i}\)を送った場合に正しく復号される確率</p>
            <p>どの符号語が送られてくる確率も等しく\(\frac{1}{M}\)であると仮定すると、正しく復号される確率\(P_C\)は</p>
            <div class="scroll">
                \begin{align}
                P_C = \sum_{i = 1}^{M} \frac{1}{M} \times P_C (\mathbf{w_i}) = \frac{1}{M} \sum_{i = 1}^{M} \sum_{\mathbf{y} \in \mathbf{\Omega_i}} P(\mathbf{y} | \mathbf{w_i})
                \end{align}
            </div>
            <p>とかける。\(\mathbf{\Omega_i} = \{\mathbf{y} | i = \max_j P(\mathbf{y} | \mathbf{w_i})\}\)とすれば\(P_C\)が最大になる。</p>
            <p>\(\mathbf{w_i}\)を\(\mathbf{y}\)のパラメータだとみなせば、\(P(\mathbf{y} | \mathbf{w_i})\)は\(\mathbf{w_i}\)の<b>尤度</b>である。</p>
            <p>尤度を最大化する復号法　→　最尤復号法</p>
            <p>最尤復号法は、各符号語が等確率で送られるとき、正しく復号される確率\(P_C\)を最大とするという意味で、最良の復号法である。しかし、すべての符号語\(\mathbf{w_i}\)に対し、\(P(\mathbf{y} | \mathbf{w_i})\)を計算し比較しなければならず、符号語数\(M\)が大きい場合には実用的ではない。</p>
            <p><b>通信路容量</b></p>
            <p>通信路の通信路容量：通信路ごとに定まる、その通信路を介して伝送できる情報速度の限界値。</p>
            <p>つまり、その通信路を介して、送ることができる1記号あたりの平均情報量の上限。</p>
            <p>\(P_{Y|X}\):定常通信路の定常条件付き確率分布</p>
            <ul>
                <li>1記号\(X\)を送ると1記号\(Y\)が受信される。→　平均的には1記号あたり相互情報量\(I(X;Y)\)だけ情報が送られる。</li>
                <li>入力\(X\)の分布は自由に変えることができるが、\(X\)の分布\(P_X\)が定まれば\(Y\)の分布は\(P_X\)と\(P_{Y|X}\)から定まる。</li>
            </ul>
            <p>この通信路を介して送ることができる1記号当たりの平均情報量の上限は</p>
            <div class="scroll">
                \begin{align}
                \max_{P_X} I(X;Y)
                \end{align}
            </div>
            <p>以上の考察から、通信路容量\(C\)の定義は\(\max_{P_X} I(X;Y)\)？</p>
            <ul>
                <li>無記憶通信路の場合はYES。記憶のある通信路の場合はNO。</li>
                <li>記憶のある通信路の場合、\(n\)（nは1より大きい）記号ごとにみるともっと多くの情報を送れる。</li>
            </ul>
            <p>\(P_{Y_n | X_n}\)：定常通信路の定常条件付き確率分布</p>
            <ul>
                <li>\(n\)記号\(X_n\)を送ると\(n\)記号\(Y_n\)が受信される。→　平均的には1記号あたり相互情報量\(\frac{I(X_n;Y_n)}{n}\)だけ情報が送られる。</li>
                <li>入力\(X_n\)の分布は自由に変えることができるが、\(X_n\)の分布\(P_{X_n}\)が定まれば\(Y_n\)の分布は\(P_{X_n}\)と\(P_{Y_n | X_n}\)から定まる。</li>
            </ul>
            <p>この通信路を介して送ることができる1記号当たりの平均情報量の上限は</p>
            <div class="scroll">
                \begin{align}
                \max_{P_{X_n}} \frac{I(X_n;Y_n)}{n}
                \end{align}
            </div>
            <p>長さ\(n\)の入力系列\(X_n\)に対する通信路の出力系列\(Y_n\)に対し、</p>
            <div class="scroll">
                \begin{align}
                C = \lim_{n \rightarrow \infty} \max_{P_{X_n}} \frac{I(X_n;Y_n)}{n}
                \end{align}
            </div>
            <p>で定義される\(C\)を<b>通信路容量</b>と呼ぶ。</p>
            <p>通信路容量の単位：<b>ビット</b>あるいは<b>ビット/通信路記号</b></p>
            <p>入力記号\(X\)に対し記号\(Y\)が出力される記憶のない通信路の通信路容量は\(C=\max_{P_X} I(X;Y)\)で計算できる。</p>
        <h3>通信路符号化の限界(2)</h3>
            <p><b>記憶のない一様通信路の通信路容量</b></p>
            <p>各時刻に\(r\)元アルファベットに属する記号\(X\)を入力し、\(s\)元アルファベットに属する記号\(Y\)を出力する、入力について一様な記憶のない定常通信路容量を\(C\)とする。</p>
            <p>この通信路の通信路行列の各行の要素の集合を{\(p_1, p_2, \cdots, p_s\)}とすれば、</p>
            <div class="scroll">
                \begin{align}
                C = \max_{P_X} H(Y) + \sum_{i = 1}^{s} p_i \log_2 p_i
                \end{align}
            </div>
            <p>が成り立つ。通信路が2重に一様であれば、</p>
            <div class="scroll">
                \begin{align}
                C = \log_2 s + \sum_{i = 1}^{s} p_i \log_2 p_i
                \end{align}
            </div>
            <p>が成り立つ。</p>
            <p>記憶のない定常通信路が入力（出力）について一様 \(\xleftrightarrow{def}\) 通信と行列のどの行（列）も同じ要素を並び替えたものになっている。</p>
            <p>記憶のない定常通信路が2重に一様 \(\xleftrightarrow{def}\) 通信路が入力について一様かつ出力について一様。</p>
            <p>例：2元対称通信路</p>
            <p><b>加法的2元通信路の通信路容量</b></p>
            <p>誤り源\(S_E\)により定義される加法的2元通信路の通信路容量\(C\)は</p>
            <div class="scroll">
                \begin{align}
                C = 1 - H(S_E)
                \end{align}
            </div>
            <p>である。</p>
            <p>誤り源\(S_E\)により定義される加法的2元通信路</p>
            <ul>
                <li>通信路内の誤り源\(S_E\)が各時刻に\(E \in {0,1}\)を発生。</li>
                <li>通信路の各時刻の入力\(X \in {0,1}\)に対し出力\(Y \in {0,1}\)は\(Y = X \oplus E\)により定まる。</li>
                <li>\(X\)と\(E\)は独立</li>
                <li>誤り源\(S_E\)が記憶のある定常情報源のとき加法的2元通信路は記憶のある定常通信路となる。</li>
            </ul>
            <p><b>誤り源が無記憶定常情報源の場合</b></p>
            <p>このとき、加法的2元通信路は<b>2元対称通信路</b>になる。</p>
            <p>\(P(E = 1) = p, P(E = 0) = 1-p\)とすれば、</p>
            <div class="scroll">
                \begin{align}
                H(S_E) = -p \log_2 p - (1-p \log_2 (1-p) = \mathcal{H}(p))
                \end{align}
            </div>
            <p>よって、\(C = 1 - H(S_E)\)を用いて、</p>
             <div class="scroll">
                \begin{align}
                C = 1 - H(S_E) = 1 - \mathcal{H}(p)
                \end{align}
            </div>
            <p><b>通信路容量と情報速度の関係についての考察</b></p>
            <p>情報速度\(R \xleftrightarrow{def}\)通信路符号系列に埋め込まれた1通信路記号あたりの平均情報量（ビット/記号）</p>
            <p>通信路容量\(R \xleftrightarrow{def}\)通信路を介して伝送できる1通信路記号あたり平均情報量（ビット/記号）</p>
            <p>情報速度\(R\) \(<\) 通信路容量\(C\)であれば情報をすべて送ることができるのでは？　→　復号誤り率を0にできるということ？</p>]
            <p><b>通信路符号化定理</b></p>
            <p>通信路容量は\(C\)である通信路に対し、\(R < C\)であれば、情報速度\(R\)の符号で復号誤り率がいくらでも小さいものが存在する。\(R>C\)であれば、そのような符号は存在しない。</p>
            <p></p>
        </section>
    </main>
</body>
</html>
