<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【北海道大学|情報科学院|院試対策|情報理論】走る作曲家のAIカフェ</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
          <p>「北海道大学大学院情報科学院修士課程入学試験」（令和6年8月実施）の「<b>情報理論</b>」対策ページです。</p>
        </section>
        <section>
          <h2>分野別対策</h2>
          <ul>
            <li><a href="gradexam.html">院試対策 ～はじめに～</a></li>
            <li><a href="gradexam_basicmath.html">院試対策 ～基礎数学編～</a></li>
            <li><a href="gradexam_infomath.html">院試対策 ～情報数学編～</a></li>
            <li><a href="gradexam_statistics.html">院試対策 ～確率・統計編～</a></li>
            <li><a href="gradexam_infotheory.html">院試対策 ～情報理論編～</a></li>
            <li><a href="gradexam_algo.html">院試対策 ～アルゴリズムとデータ構造編～</a></li>
            <li><a href="gradexam_ai.html">院試対策 ～人工知能編～</a></li>
            <li><a href="gradexam_cs.html">院試対策 ～コンピュータシステム編～</a></li>
          </ul>
        </section>
        <section id="algorithms-data-structures">
        <h2>情報理論</h2>
        <p>参考にしたサイトは以下のとおり。</p>
        <p><a href="https://www-alg.ist.hokudai.ac.jp/~atsu/info_theory.html">情報理論 (Information Theory).html</a></p>
        <h3>情報理論とは</h3>
            <p><b>情報理論が取り組む4つの課題</b></p>
            <ul>
                <li>できるだけよい情報源符号化法を見出すこと</li>
                <li>情報源符号化の限界を知ること</li>
                <li>できるだけよい通信路符号化法を見出すこと</li>
                <li>通信路符号化の限界を知ること</li>
            </ul>
        <h3>条件付き確率</h3>
            <p><b>条件付き確率</b></p>
            <p>事象\(B\)の条件のもとで事象\(A\)が起こる条件付き確率は</p>
            <div class="scroll">
                \begin{align}
                P(A|B) = \frac{P(A \cap B}{P(B)}
                \end{align}
            </div>
            で定義される。
            <p><b>乗法定理</b></p>
            <div class="scroll">
                \begin{align}
                P(A \cap B) = P(A|B) P(B)
                \end{align}
            </div>
        <h3>情報量とエントロピー(1)</h3>
            <p><b>情報量</b></p>
            <p>確率\(p\)の事象の生起を知った時に得られる<b>情報量</b>を\(I(p)\)とする。</p>
            <p>\(I(p)\)は次のような性質を満たすべき。</p>
            <ul>
                <li>\(I(p)\)は\( 0 < p \leq 1 \)で単調現象な関数である</li>
                <li>確率\(p_1, p_2\)で起こる二つの互いに独立な事象が同時に起こる確率\(p_1, p_2\)について\(I(p_1 p_2) = I(p_1) + I(p_2)\)</li>
                <li>\(I(p)\)は\( 0 < p \leq 1 \)で連続な関数である</li>
            </ul>
            <p>これらを満たす関数\(I(p)\)は\(I(p) = - \log_a p\)という形しかありえない（\(a > 1\)）</p>
            <p>確率\(p\)で生起する事象が起きたことを知ったときに得られる情報量\(I(p)\)を<b>自己情報量</b>と呼び、\(I(p) = - \log_a p\)と定義する。ただし、\(a > 1\)である。</p>
            <p>\(a = 2\)のとき、単位は<b>ビット</b>という。つまり、確率\(\frac{1}{2}\)で生じる結果を知った時の情報量は1 [bit]である。</p>
            <p><b>平均情報量</b></p>
            <p>\(M\)個の互いに排反な事象\(a_1, a_2, \cdots, a_M\)が起こる確率を\(p_1, p_2, \cdots, p_M\)とする（ただし、\(p_1 + p_2 + \cdots + p_M = 1\)）。</p>
            <p>このうち１つの事象が起こったことを知ったときに得る情報量は\(- \log_2 p_i\)であるから、これを平均した期待値\(\overline{I}\)は、</p>
            <div class="scroll">
                \begin{align}
                \overline{I} &= p_1 (- \log_2 p_1) + p_2 (- \log_2 p_12) + \cdots + p_M (- \log_2 p_M) \\
                &= - \sum_{i = 1}^{M} p_i \log_2 p_i
                \end{align}
            </div>
            <p>となる。これを<b>平均情報量</b>（単位はビット）という。</p>
            <p><b>エントロピー</b></p>
            <p>確率変数\(X\)が取り得る値が\(x_1, x_2, \cdots, p_M\)（ただし、\(p_1 + p_2 + \cdots + p_M = 1\)）であるとき、確率変数\(X\)の<b>エントロピー</b>を</p>
            <div class="scroll">
                \begin{align}
                H(X) = - \sum_{i = 1}^{M} p_i \log_2 p_i
                \end{align}
            </div>
            <p>ビットと定義する。</p>
            <p>一般に、確率分布の偏りが非常に大きいときは平均情報量が小さく、確率分布が均一の時に平均情報量が最大になる。</p>
            <p>エントロピーと平均情報量は、完全に同じ形をしている。</p>
            <p>情報を受け取ると曖昧さが減るので、受け取った量だけエントロピーは減って、0に近づく（<b>受け取った情報量 = エントロピーの変化</b>）</p>
            <p><b>エントロピーの性質</b></p>
            <p>\(M\)個の値を取る確率変数\(X\)のエントロピー\(H(X)\)は次の性質を満たす。</p>
            <ol>
                <li>\(0 \leq H(X) \leq \log_2 M\)</li>
                <li>\(H(X)\)が最小値0となるのは、ある値を取る確率が1で、他のM-1個の値を取る確率がすべて0の時に限る。すなわち、\(X\)の取る値がはじめから確定している場合のみである。</li>
                <li>\(H(X)\)が最大値\(\log_2 M\)となるのは、\(M\)個の値がすべて\(\frac{1}{M}\)で等しい場合に限る。</li>
            </ol>
            <p>エントロピーは「答えのあてにくさ」を表す数値ともいえる。</p>
            <p><b>2つ以上の確率変数を扱う場合</b></p>
            <P>２つの確率変数\(X, Y\)を考える。\(X\)は\(x_1, x_2, \cdots, x_{M_X}\)の値をとり、\(Y\)は\(y_1, y_2, \cdots, y_{M_Y}\)の値を取るものとする。</P>
            <p>確率変数の組\((X, Y)\)の値が\((x, y)\)となる<b>結合確率分布</b>を\(P(x, y)\)と書く。</p>
            <p>組\((X, Y)\)をまとめて考えると、４つの値を取る確率変数\(Z\)のエントロピー\(H(Z)\)として考えることができる。</p>
            <p>確率変数\(X\)と\(Y\)の結合エントロピー\(H(X, Y)\)は、</p>
            <div class="scroll">
                \begin{align}
                H(X, Y) = - \sum_{i = 1}^{M_X} \sum_{j = 1}^{M_Y} P(x_i, y_j) \log_2 P(x_i, y_j)
                \end{align}
            </div>
            <p>により定義される。これを<b>結合エントロピー</b>と呼ぶ。ただし、\({x_1, x_2, \cdots, x_{M_X}}\)および\({y_1, y_2, \cdots, y_{M_Y}}\)は、それぞれ\(X\)と\(Y\)が取り得る値の集合とする。</p>
            <p><b>結合エントロピーの性質</b></p>
            <p>確率変数\(X\)と\(Y\)の結合エントロピー\(H(X, Y)\)に対し、</p>
            <div class="scroll">
                \begin{align}
                0 \leq H(X, Y) \leq H(X) + H(Y)
                \end{align}
            </div>
            <p>が成り立つ。また、\(H(X, Y) = H(X) + H(Y)\)となるのは、\(X\)と\(Y\)が<b>独立のときのみ</b>である。</p>
            <p><b>ベイズの定理</b></p>
            <div class="scroll">
                \begin{align}
                P(B|A) = \frac{P(B)P(A|B)}{P(B)P(A|B)+P(\overline{B})P(A|\overline{B})}
                \end{align}
            </div>
        <h3>情報量とエントロピー(2)</h3>
            <p><b>条件付きエントロピー</b></p>
            <p>確率変数\(Y\)で条件を付けた\(X\)の<b>条件付きエントロピー</b>\(H(X|Y)\)は、</p>
            <div class="scroll">
                \begin{align}
                H(X|Y) = - \sum_{j = 1}^{M_Y} P(y_j) \sum_{i = 1}^{M_X} P(x_i|y_j) \log_2 P(x_i|y_j)
                \end{align}
            </div>
            <p>により定義される。ただし、\({x_1, x_2, \cdots, x_{M_X}}\)および\({y_1, y_2, \cdots, y_{M_Y}}\)は、それぞれ\(X\)と\(Y\)が取り得る値の集合とする。</p>
            <p><b>条件付きエントロピーの性質</b></p>
            <p>\({x_1, x_2, \cdots, x_{M_X}}\)および\({y_1, y_2, \cdots, y_{M_Y}}\)を取り得る値の集合とする確率変数\(X\)と\(Y\)に関し、以下が成り立つ。</p>
            <ol>
                <li>
                    <div class="scroll">
                        \begin{align}
                        H(X|Y) = - \sum_{j = 1}^{M_Y} \sum_{i = 1}^{M_X} P(x_i, y_j) \log_2 P(x_i|y_j)
                        \end{align}
                    </div>
                </li>
                <li>
                    <div class="scroll">
                        \begin{align}
                        H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
                        \end{align}
                    </div>
                （この式は、\(H(X)\)と\(H(Y)\)に関するベン図を描くと分かりやすい。）</li>
                <li>\(0 \leq H(X|Y) \leq H(X)\)（\(H(X|Y) = H(X)\)は、\(X\)と\(Y\)が独立の時のみ成立。）</li>
                <li>\(0 \leq H(Y|X) \leq H(Y)\)（\(H(Y|X) = H(Y)\)は、\(X\)と\(Y\)が独立の時のみ成立。）</li>
            </ol>
            <p><b>相互情報量と、その性質</b></p>
            <p>確率変数\(X\)と\(Y\)の<b>相互情報量</b>\(I(X;Y)\)は、</p>
            <div class="scroll">
                \begin{align}
                I(X;Y) = H(X) - H(X|Y)
                \end{align}
            </div>
            <p>によって定義される。</p>
            <p>確率変数\(X\)と\(Y\)と相互情報量\(I(X;Y)\)に関して以下が成り立つ。</p>
            <ol>
                <li>
                    <div class="scroll">
                        \begin{align}
                        I(X;Y) &= H(X) - H(X|Y) \\
                        &= H(Y) - H(Y|X) \\
                        &= H(X) + H(Y) - H(X, Y)
                        \end{align}
                    </div>
                </li>
                <li>\(0 \leq I(X;Y) \leq \min {H(X), H(Y)}\)（\(I(X;Y)=0\)は、\(X\)と\(Y\)が独立の時のみ成立。）</li>
            </ol>
            <p><b>シャノンの補助定理</b></p>
            <p>\(p_1, p_2, \cdots, p_M\)および\(q_1, q_2, \cdots, q_M\)を</p>
            <div class="scroll">
                \begin{align}
                p_1 + p_2 + \cdots + p_M = 1 \\
                q_1 + q_2 + \cdots + q_M \leq 1
                \end{align}
            </div>
            <p>を満たす任意の非負の数とする（ただし、\(p_i \neq 0\)のときは\(q_i \neq 0\)とする）。このとき、</p>
            <div class="scroll">
                \begin{align}
                - \sum_{i = 1}^{M} p_i \log_2 q_i \geq - \sum_{i = 1}^{M} p_i \log_2 p_i
                \end{align}
            </div>
            <p>が成立する。等号は\(q_i = p_i (i = 1, 2, \cdots, M)\)のとき、またそのときに限って成立する。</p>
            <p>つまり、確率分布\(P = {p_i}_{i = 1}^{M}\)とちょっと違う分布\(q_i\)（ただし総和が1以下）を持ってきて、\(\log_2\)の内側の\(p_i\)と置き換えると、<b>元より少し大きくなる</b>。</p>
        <h3>情報源のモデル(1)</h3>
            <p><b>離散的\(M\)元情報源</b></p>
            <p>\(M\)個の元からなる記号の有限集合\(A = {a_1, a_2, \cdots, a_M}\)を考える。</p>
            <p>\(A\)を<b>情報源アルファベット</b>、各元\(a_i \in A\)を<b>情報源記号</b>と呼ぶ。</p>
            <p>時点0より毎時点（時点は整数値）で、\(A\)上の情報源記号を、ある確率に従って１個ずつ出力する。</p>
            <p>情報源から出力されるデータ = 情報源記号が並んだ記号列（<b>情報源系列</b>と呼ぶ。）</p>
            <p><b>情報源系列の確率分布</b></p>
            <p>時点0から時点\(n-1\)まで（長さ\(n\)）の情報源系列について、</p>
            <ul>
                <li>時点\(i\)での情報源の出力を、確率変数\(X_i\)で表す。つまり、\(X_i\)は\(a_1, a_2, \cdots, a_M\)のいずれか</li>
                <li>情報源系列は\(X_0 X_1 \cdots X_{n-1}\)</li>
            </ul>
            <p>\(X_0, X_1, \cdots, X_{n-1}\)の<b>結合確率分布</b>:</p>
            <div class="scroll">
                \begin{align}
                P_{X_0 X_1 \cdots X_{n-1}} (x_0, x_1, \cdots, x_{n-1}) = [X_0 = x_0, X_1 = x_1, \cdots, X_{n-1} = x_{n-1} \text{となる確率}]
                \end{align}
            </div>
            <p>\(p(x_0, x_1, \cdots, x_{n-1})\)とも書く。</p>
            <P>結合確率分布から、統計的性質は完全に定まる。</P>
            <p><b>情報源の各種モデル</b></p>
            <p>どんなに大きい\(n\)についても、\(X_0, X_1, \cdots, X_{n-1}\)の結合確率分布を与えることができれば、この情報源の統計的性質は完全に記述できる。しかし、一般には困難。</p>
            <p>議論を進めるためには、情報源に何らかの<b>扱いやすい性質</b>を仮定する必要がある。</p>
            <p><b>記憶のない情報源</b></p>
            <p>各時点における情報源記号の発生が、<b>他の時点と独立</b>である情報源</p>
            <p><b>記憶のない定常情報源</b></p>
            <p>記憶のない情報源において、各時点における情報源記号の発生が<b>同一の確率分布</b>\(P_X (x)\)に従う情報源</p>
            <p>記憶のない定常情報源における長さ\(n\)の系列の結合確率分布は、以下の式で表される。</p>
            <div class="scroll">
                \begin{align}
                P_{X_0 X_1 \cdots X_{n-1}} (x_0, x_1, \cdots, x_{n-1}) = \prod_{i = 0}^{n-1} P_X (x_i)
                \end{align}
            </div>
            <p><b>定常情報源の定義</b></p>
            <p>時間をずらしても、統計的性質の変わらない情報源</p>
            <p>定常情報源の出力は、各時点において同一の確率分布に従う。この確率分布を<b>定常分布</b>と呼ぶ。</p>
            <p><b>エルゴード情報源</b></p>
            <p>十分長い任意の出力系列に、その情報源の統計的性質が完全に現れている定常情報源。</p>
            <p>エルゴード的であれば、十分に長い任意の系列は必ず一定の統計的性質を満たす。</p>
            <p><b>情報源の各種モデル</b></p>
            <p>記憶のない定常情報源は必ずエルゴード情報源になる。</p>
            <p>記憶のある定常情報源でエルゴード的なものはある（例：正規マルコフ情報源）</p>
            <p>定常情報源であってもエルゴード的でないものもある。</p>
            <p>例：確率\(\frac{1}{2}\)で0だけからなる系列か、1だけからなる系列を発生する情報源</p>
            <p>情報理論では、ほとんどの場合、エルゴード情報源を扱う。</p>
            <p><b>マルコフ情報源</b></p>
            <p>マルコフ情報源：記憶のある情報源の基本的なモデル。</p>
            <p><b>\(m\)重マルコフ情報源</b></p>
            <p>系列中の<b>過去\(m\)文字の並びにしたがって、次の文字の生起確率が決まるモデル。</b></p>
            <p><b>\(m\)重マルコフ情報源の状態遷移図</b></p>
            <p>離散的\(q\)元情報源を考える。これが\(m\)重マルコフ情報源だとする。</p>
            <p>\(q^m\)個の<b>状態</b>を持ち、各状態において記号の出力確率が定まっていると見なせる。</p>
            <p>出力を一つ発生する度に\(q^m\)個の状態の中を<b>遷移</b>していく。</p>
            <p><b>一般化されたマルコフ情報源</b></p>
            <p>単に、マルコフ情報源と呼ぶ。</p>
            <p>「直前の\(m\)個の出力に対応する」という条件をなくし、より抽象的に（自由な状態図で）情報源を定義できる。</p>
            <p>マルコフ連鎖モデルともいう。</p>
            <p><b>状態の分類</b></p>
            <p><b>過渡状態</b>：十分時間が経過すれば抜け出てしまい、戻ることのない状態。</p>
            <p><b>閉じた状態集合</b>：任意の状態から任意の状態へ遷移可能な（矢印でたどっていくことのできる）状態の集合。</p>
            <p><b>既約マルコフ情報源</b>：任意の状態から任意の状態へ遷移可能なマルコフ情報源。</p>
            <p><b>非周期的状態集合</b>：閉じた状態集合で、ある時間が経過した後の任意の時点において、どの状態にある確率も0でないようなもの。</p>
            <p><b>周期的状態集合</b>：閉じた状態集合がいくつかの部分集合に分割され、その各々が周期的な時点においてのみ現れ得るもの。</p>
            <p><b>正規マルコフ情報源</b>：非周期的状態集合のモデルで定義されるマルコフ情報源。十分な時間が経過した後に、定常情報源として扱えて有益。</p>
        <h3>情報源のモデル(2)</h3>
            <p><b>遷移確率行列</b></p>
            <p>\(N\)個の状態\(s_0, s_1, \cdots, s_{N-1}\)を持つ<b>正規マルコフ情報源</b>を考える。</p>
            <p>状態遷移の仕方は、状態\(s_i\)にあるとき、次の時点で状態\(s_j\)に遷移する確率\(p_{i,j}=P(s_j|s_i)\)により決まる。これを<b>遷移確率</b>という。</p>
            <p>遷移確率\(p_{i, j}\)を\((i, j)\)要素とする\(N \times N\)行列を<b>遷移確率行列</b>と呼ぶ。</p>
            <p>行が\(i\)側（現在状態）で、列が\(j\)側（次状態）である。</p>
            <p>行ごとに総和が1である。</p>
            <p><b>正規マルコフ情報源の極限分布</b></p>
            <p>正規マルコフ情報源の定義より、\(t > t_0\)となる任意の\(t\)について</p>
            <div class="scroll">
                \begin{align}
                p_{i, j}^{(t)} > 0 \quad (i, j = 0, 1, \cdots, N-1)
                \end{align}
            </div>
            <p>が成り立つようなある正定数\(t_0\)が存在する。</p>
            <p>正規マルコフ情報源では、\(t \rightarrow \infty\)とするとき、\(p_{i, j}^{(t)}\)は\(i\)には無関係な値に収束する。すなわち、</p>
            <div class="scroll">
                \begin{align}
                \lim_{t \rightarrow \infty} p_{i, j}^{(t)} = u_j \quad (j = 0, 1, \cdots, N-1)
                \end{align}
            </div>
            <p>となる\(u_j\)が存在する。</p>
            <p>遷移行列を用いて上式を書き直すと、</p>
            <div class="scroll">
                \begin{align}
                \lim_{t \rightarrow \infty} \Pi = U \quad
                \end{align}
            </div>
            <p>となる。ここで、\(U\)はすべての行が\(\mathbf{u} = (u_0, u_1, \cdots, u_{N-1})\)となる\(N \times N\)行列である。</p>
            <p>時点\(t\)において状態\(s_j\)にいる確率\(P(s^{(t)} = s_j)\)を\(w_{j}^{(t)}\)で表し、</p>
            <div class="scroll">
                \begin{align}
                \mathbf{w}_t = (w_{0}^{(t)}, w_{1}^{(t)}, \cdots, w_{N-1}^{(t)})
                \end{align}
            </div>
            <p>という\(N\)次元ベクトルを定義する。</p>
            <p>時点\(t-1\)で状態\(s_i\)にいる確率が\(w_{i}^{t-1}\)で、状態\(s_i\)にいるときに次の時点で状態\(s_j\)へと遷移する確率が\(p_{i, j}\)であるから、</p>
            <div class="scroll">
                \begin{align}
                w_{j}^{(t)} &= \sum_{i = 0}^{N-1} w_{i}^{t-1} p_{i, j} \\
                \mathbf{w}_t &= \mathbf{w}_{t-1} \Pi
                \end{align}
            </div>
            <p>これを繰り返せば\(\mathbf{w}_{t} = \mathbf{w}_{0} \Pi^{t}\)を得る。ここで、\(\mathbf{w}_0\)は時点0における状態分布（<b>初期分布</b>）である。</p>
            <p>\(mathbf{w}_t\)の\(t \rightarrow \infty\)とした極限を<b>極限分布</b>と呼ぶ。</p>
            <p>正規マルコフ情報源では、次式が成り立つ。</p>
            <div class="scroll">
                \begin{align}
                \lim_{t \rightarrow \infty} \mathbf{w}_t = \mathbf{w}_0 \lim_{t \rightarrow \infty} \Pi^t = \mathbf{w}_0 U = \mathbf{u}
                \end{align}
            </div>
            <p>十分時間が経過すれば、初期分布がどうであれ、状態分布は定常的な確率分布（定常分布）に落ち着く</p>
            <p>正規マルコフ情報源が落ち着く定常分布を</p>
            <div class="scroll">
                \begin{align}
                \mathbf{w} = (w_0, w_1, \cdots, w_{N-1})とする。
                \end{align}
            </div>
            <p>とする。\(w_i\)は確率なので、当然ながら</p>
            <div class="scroll">
                \begin{align}
                w_0 + w_1 + \cdots + w_{N-1} = 1
                \end{align}
            </div>
            <p>が成り立つ。</p>
            <p>ある時点の状態分布が定常的で\(\mathbf{w}\)であるとすれば、次の時点の状態分布も\(\mathbf{w}\)でなければならないので、\(\mathbf{w}\)は</p>
            <div class="scroll">
                \begin{align}
                \mathbf{w} \Pi = \mathbf{w}
                \end{align}
            </div>
            <p>を満たさなければならない。</p>
            <p>正規マルコフ情報源の遷移確率行列\(\Pi\)に対しては、この式を満たす\(\mathbf{w}\)が唯一存在し、極限分布と一致する。</p>
            <p><b>情報源の1次エントロピー</b></p>
            <p>次のような\(M\)元定常情報源\(S\)を考える。</p>
            <ul>
                <li>情報源アルファベット\(A = {a_1, a_2, \cdots, a_M}\)</li>
                <li>記号\(a_k\)の生起確率\(p_k\)</li>
            </ul>
            <p>このとき、</p>
            <div class="scroll">
                \begin{align}
                H_1 (S) = - \sum_{k = 1}^{M} p_k \log_2 p_k
                \end{align}
            </div>
            <p>を情報源\(S\)の<b>1次エントロピー</b>と呼ぶ。</p>
            <p>記憶のない定常情報源の場合、その1次エントロピーは、情報源からの出力を1個受け取ったときに得られる平均的な情報量と考えることができる。</p>
            <p><b>情報源の\(n\)次エントロピー</b></p>
            <p>長さ\(n\)の系列を考え、その系列全体のエントロピーから1記号あたりのエントロピーを算出することを考える。</p>
            <p>\(M\)元情報源の\(n\)個の出力をまとめて1個の記号とみなすと、\(M^n\)元の情報源とみなせる。</p>
            <p>これを<b>\(n\)次拡大情報源</b>といい、\(S^n\)で表す。</p>
            <p>このとき、</p>
            <div class="scroll">
                \begin{align}
                H_n (S) = \frac{H_1 (S^n)}{n}
                \end{align}
            </div>
            <p>を情報源\(S\)の<b>\(n\)次エントロピー</b>と呼ぶ。</p>
            <p>記憶のない定常情報源の場合、その\(n\)次エントロピーは、1次エントロピーと一致する。</p>
            <p><b>情報源のエントロピー</b></p>
            <p>情報源\(S\)の\(n\)次エントロピーの極限</p>
            <div class="scroll">
                \begin{align}
                H(s) = \lim_{n \rightarrow \infty} H_n (s) = \lim_{n \rightarrow \infty} \frac{H_1 (S^n)}{n}
                \end{align}
            </div>
            <p>を情報源\(S\)の<b>エントロピー</b>と呼ぶ。</p>
            <p>これは、十分長い時間をかけて系列を観測した時の1記号当たりの平均情報量と考えることができる。</p>
            <p>記憶のない定常情報源の場合、そのエントロピーは1次エントロピーと一致する。</p>
            <p><b>正規マルコフ情報源のエントロピー</b></p>
            <p>次のような正規マルコフ情報源\(S\)を考える。</p>
            <ul>
                <li>情報源アルファベット \(a_1, a_2, \cdots, a_M\)</li>
                <li>\(N\)個の状態\(s_0, s_1, \cdots, s_{N-1}\)</li>
                <li>定常分布\((w_0, w_1, \cdots, w_{N-1})\)</li>
                <li>状態\(s_i\)にあるときに記号\(a_k\)を発生する確率\(P(a_k | s_i)\)</li>
            </ul>
            <p>この情報源\(S\)に対するエントロピー\(H(S)\)は</p>
            <div class="scroll">
                \begin{align}
                H(s) = - \sum_{i = 0}^{N-1} w_i (\sum_{k = 1}^{M} P(a_k | s_i) \log_2 P(a_k | s_i))
                \end{align}
            </div>
            <p>である。</p>
            <p>結局、正規マルコフ情報源\(S\)のエントロピー\(H(S)\)の求め方は以下の通りである。</p>
            <ol>
                <li>定常分布を求める</li>
                <li>各状態\(s_i\)において、記憶のない情報源とみなし、その場合のエントロピー\(H_{s_i} (S)\)を求める</li>
                <li>上で求めたエントロピー\(H_{s_i} (S)\)を、定常分布にしたがった割合で合算する</li>
            </ol>
        <h3>情報源符号化とその限界</h3>
            <p><b>平均符号長とは</b></p>
            <p>情報源符号化の目的の1つは、できるだけ「良い」情報源符号化法を設計することである。</p>
            <p>われわれの目的は、通信路をできるだけ効率よく圧縮することであるから、同じ情報系列を平均として短い符号列系に符号化できれば、「良い」符号化となる。</p>
            <p>そこで、言い換えると、1情報源あたり符号系列の長さの平均値をできるだけ小さくしたい。これを<b>平均符号長</b>と呼ぶ。</p>
            <p><b>平均符号長の定義</b></p>
            <p>情報源アルファベットが\(\Sigma = {a_1, a_2, \cdots, a_M}\)で、発生確率が\(P(a_i) = p_i (i = 1, 2, \cdots, M)\)で与えられる記憶のない情報源\(S\)を考える。</p>
            <p>符号\(C\)によって、情報源記号\(a_1, a_2, \cdots, a_M\)のそれぞれに対応付けられた符号語の長さを\(l_1, l_2, \cdots, l_M\)とする。このとき、1情報源記号当たりの<b>平均符号長</b>は、</p>
            <div class="scroll">
                \begin{align}
                L &= p_1 l_1 + \cdots + p_M l_M \\
                &= \sum_{i = 1}^{M} p_i l_i
                \end{align}
            </div>
            <p>で与えられる。</p>
            <p><b>瞬時符号とは</b></p>
            <p>符号化は、一意に復号できても、前から読んで戻せるとは限らない。そこで、符号化は、一意復号可能かどうかだけでなく、前から読んだときに、そのまま後戻りせずに復号可能だと便利である。</p>
            <p>符号\(C\)が<b>瞬時符号</b>であるとは、符号化を前から読んでいったときに、ある符号語のパターンが現れたら、それを直ちに復号できるときをいう。そうでない符号を<b>非瞬時符号</b>という。</p>
            <p><b>瞬時符号の条件</b></p>
            <p>ある符号語\(x\)が別の符号語\(y\)の頭の部分のパターンと一致するとき、\(x\)は\(y\)の<b>語頭</b>という。</p>
            <p>瞬時符号であるためには、どの符号語も他の符号語の語頭であってはならない。これを<b>語頭条件</b>と呼ぶ。</p>
            <p><b>符号の木と瞬時符号の関係</b></p>
            <p>瞬時符号は、符号語がすべて葉に対応付けられている。非瞬時符号は、葉以外の節点にも対応付けられている。</p>
            <p><b>クラフトの不等式</b></p>
            <p>長さが\(l_1, l_2, \cdots, l_M\)となる\(M\)個の符号語を持つ\(q\)元符号で瞬時符号となるものが存在するための必要十分条件は、</p>
            <div class="scroll">
                \begin{align}
                q^{-l_1} + q^{-l_2} + \cdots + q^{-l_M} \leq 1 
                \end{align}
            </div>
            <p>が満たされることである。</p>
            <p>※「存在する」としか言っていないことに注意。「この不等式を満たすから瞬時符号」とは言えない。</p>
            <p>※実は、一意復号可能である必要十分条件も、この式を満たすことであり、<b>マクラミンの不等式</b>と呼ぶ。</p>
            <p><b>平均符号長\(L\)の限界に関する定理その１</b></p>
            <p>定常分布を持つ情報源\(S\)の各情報源記号を一意復号可能な\(r\)元符号に符号化したとき、その平均符号長\(L\)は</p>
            <div class="scroll">
                \begin{align}
                \frac{H_1 (S)}{\log_2 r} \leq L
                \end{align}
            </div>
            <p>を満たす。また、平均符号長\(L\)が</p>
            <div class="scroll">
                \begin{align}
                L < \frac{H_1 (S)}{\log_2 r} + 1
                \end{align}
            </div>
            <p>となる\(r\)元瞬時符号を作ることができる。</p>
            <p><b>平均符号長\(L\)の限界に関する定理その２</b></p>
            <p>情報源\(S\)は、任意の一意復号可能な\(r\)元符号で符号化する場合、その平均符号長\(L\)は、</p>
            <div class="scroll">
                \begin{align}
                \frac{H (S)}{\log_2 r} \leq L
                \end{align}
            </div>
            <p>を満たす。また、任意の正数\(\epsilon > 0\)について平均符号長\(L\)が</p>
            <div class="scroll">
                \begin{align}
                L < \frac{H (S)}{\log_2 r} + \epsilon
                \end{align}
            </div>
            <p>となる\(r\)元瞬時符号を作ることができる。</p>
            <p>つまり、どんなに工夫しても、平均符号長\(L\)はエントロピー\(H(S)\)までしか改善できない。（2元符号の場合、\(\log_2 r = 1\)）</p>
        <h3>情報源符号化法(1)</h3>
            <p><b>ハフマン符号はなぜ大事か？</b></p>
            <p>ハフマン符号は<b>コンパクト符号</b>である。</p>
            <p>コンパクト符号とは、1記号ずつ符号化する際、その平均符号長を最小とする効率の良い符号のこと。</p>
            <p><b>ハフマン符号の作り方</b></p>
            <p><b>基本アイディア</b>：最も確率が低い2つの情報源記号を繰り返し選んでまとめながら、下から順に符号木を作る。</p>
            <p>具体的な手順は以下のとおり。</p>
            <ol>
                <li>まず、確率の高い順に記号を並び替える。</li>
                <li>各記号に対応する符号木の葉を作る。葉には確率を添えてかいておく。</li>
                <li>最も確率が小さい葉を2つ選び、それを集約するためのノードを新たに作って枝で結ぶ。そのノードを新しい葉として扱い、元の二つの葉の確率を足し合わせたものを添える。</li>
                <li>3. を繰り返して符号木を作る。</li>
                <li>各ノードから葉へ向かう方向の2本の枝に、0と1のラベルを割り当てる（割り当て方は任意で良い）。</li>
            </ol>
            <p>あとは、構成した符号木を用いて、根から各々の葉へ至るパスをなぞりながら、ラベルの列を符号語として記号に割り当てればよい。</p>
            <p>ハフマン符号は数通り作成できる。</p>
            <p>符号語が符号の木の葉にだけ割り当てられているハフマン符号は瞬時符号である。</p>
            <p><b>ブロック符号化</b></p>
            <p>1,0をそれぞれ確率0.2, 0.8で発生する記憶のない2元定常情報源からの系列を圧縮したい（確率に偏りがあるので圧縮できるはず）。</p>
            <p>情報源の一記号ごとに符号化すると、</p>
            <p>（2元情報源）0, 1 → 0, 1（2元符号化）</p>
            <p>となって、全く効率が上がらない。</p>
            <p>この解決方法として、連続する何個かの情報源符号をまとめて符号化する、というのがある。</p>
            <p>一定個数の情報源記号ごとにまとめて符号化する方法を<b>ブロック符号化</b>と呼ぶ。</p>
            <p>特に、元の情報源\(S\)に対し、\(n\)次拡大情報源\(S^n\)を考え、その上の記号に対してハフマン符号化を行う方法を、<b>ブロックハフマン符号化</b>と呼ぶ。</p>
        <h3>情報源符号化法(2)</h3>
            <p><b>ブロックハフマン符号化の問題点</b></p>
            <p>ブロックハフマン符号化におけるブロック長\(n\)を十分大きくすれば、1情報源記号あたりの平均符号長をいくらでも下限に近づけられる。</p>
            <p>しかし、\(n\)を大きくすると、記号の数が急増する。</p>
            <p>\(M\)元情報源の場合、符号化すべき長さ\(n\)の情報源系列の数が\(M^n\)個に増大する（ハフマン符号化が困難になる）。</p>
            <p><b>非等長情報系列に対する符号化</b></p>
            <p>符号化すべき情報源系列を非等長にすることを考える。</p>
            <p>すなわち、長い情報源系列と短い情報源系列を組み合わせ、長いがよく発生する系列に、より短い符号語を割り当てる。</p>
            <p>利点：符号化する情報源系列の数を減らして、符号化のために記憶すべき表を削減できる。</p>
            <p>情報源から出力される任意の系列が、一意に分解できなければならない。</p>
            <p><b>分節木を用いた情報源系列の分割</b></p>
            <p>分節木とよばれる木構造を用いて、情報源系列を長さの異なる系列（ブロック）に分割し、各ブロックに対して符号化を行う。分節木の各葉ノードはある1つの系列に対応している。</p>
            <p>1記号あたりの平均符号長は、分節木を用いて算出した各ブロックの平均長で、符号木を用いて算出した平均符号長を割ることによって求められる。</p>
            <p><b>ランレングス・ハフマン符号化</b></p>
            <p>系列中に同じ記号が連続するとき、その連続する長さを符号化して送る方法を一般に、<b>ランレングス符号化</b>と呼ぶ。</p>
            <p>ランでブロック化してからハフマン符号化する方法を、<b>ランレングス・ハフマン符号化</b>と呼ぶ。</p>
            <p>例として、1,0の出現確率が、それぞれ\(p, 1-p\)の無記憶定常情報源\(S\)について、\(N-1\)個までの0のランを符号化することを考える。</p>
            <p>つまり、1, 10, 100, 1000, ... という系列を考える。
            <p>これら\(N\)個の系列の平均長\(\overline{n}\)は、</p>
            <div class="scroll">
                \begin{align}
                \overline{n} &= \sum_{i = 0}^{N-2} (i+1)(1-p)^i p + (N-1)(1-p)^{N-1} \\
                &= \frac{1-(1-p)^{N-1}}{p}
                \end{align}
            </div>
            <p>これらの系列をハフマン符号化したときの平均符号長\(L_N\)は次を満たす。</p>
            <div class="scroll">
                \begin{align}
                L_N &< - \sum_{i = 0}^{N-1} p_i \log_2 p_i + 1 \\
                    &= H(S) \overline{n} + 1
                \end{align}
            </div>
            <p>よって1記号あたりの平均符号長\(L_r\)は</p>
            <div class="scroll">
                \begin{align}
                L_r = \frac{L_N}{\overline{n}} < H(S) + \frac{1}{overline{n}}
                \end{align}
            </div>
            <p>となる。ブロックハフマン符号化の場合は</p>
            <div class="scroll">
                \begin{align}
                L_h < H(S) + \frac{1}{n} = H(S) + \frac{1}{\log_2 N}
                \end{align}
            </div>
            <p>である。</p>
            <p><b>ひずみを入れた情報源符号化</b></p>
            <p>通信路でひずみが入るのではなく、符号化時に（わざと）ひずみを入れる。</p>
            <p>元の情報量を削って通信することに相当し、そうすることで圧縮率を向上させられる。</p>
            <p><b>どのくらい平均符号長の限界を下げられるか？</b></p>
            <p>1情報源記号あたりの平均符号長の下限 = エントロピー \(H(S)\)だが、ひずみを許した場合、出力\(Y\)の値を知っても、元の入力\(X\)に関してなお平均して\(H(X|Y)\)のあいまいさが残る。</p>
            <p>そのため、伝えられる情報の量は\(H(X) - H(X|Y) = I(X;Y)\)となって、ひずみを許した場合の限界は相互情報量で表されることがわかる。</p>
            <p><b>ひずみ測度</b></p>
            <p>ひずみ測度とは、\(x\)と\(y\)の相違を評価する関数\(d(x, y)\)のことである。この関数の値が大きいほど、ひずみが大きい。次の性質を持つ。</p>
            <div class="scroll">
                \begin{align}
                d(x, y) \geq 0 \\
                x = y \text{のとき} d(x, y) = 0
                \end{align}
            </div>
            <p>ひずみ測度の平均値を<b>平均ひずみ</b>と呼び、\(\overline{d}\)で表す。</p>
            <div class="scroll">
                \begin{align}
                \overline{d} = \sum_{x} \sum_{y} d(x, y) P_{XY} (x, y)
                \end{align}
            </div>
            <p><b>ひずみ測度の例</b></p>
            <p>例１）情報源アルファベットを\(\Sigma = {0,1}\)とし、ひずみ測度を</p>
            <div class="scroll">
                \begin{align}
                d(x, y) = 
                \begin{cases}
                0; \quad x=y \\
                1; \quad x \neq y
                \end{cases}
                \end{align}
            </div>
            <p>とする。このとき、平均ひずみは</p>
            <div class="scroll">
                \begin{align}
                \overline{d} &= \sum_{x} \sum_{y} d(x, y) P_{XY} (x, y) \\
                &= P(1, 0) + P(0, 1)
                \end{align}
            </div>
            <p>となる。これは要するに、符号器の出力が元の情報源の出力と異なる確率であり、通常<b>ビット誤り率</b>と呼ばれる。</p>
            <p>例２）情報源アルファベットを有限個の整数または実数の集合とする。このとき、ひずみ測度を\(d(x, y) = |x- y|^2\)とすれば、平均ひずみは<b>2乗平均誤差</b>と呼ばれる量になる。</p>
            <p><b>平均ひずみと相互情報量の関係</b></p>
            <p>相互情報量\(I(X;Y)\)が同じでも、平均ひずみ\(\overline{d}\)は同じとは限らない。</p>
            <p>つまり、平均ひずみ\(overline{d}\)が同じでも、\(I(X;Y)\)は符号の仕方で異なる。</p>
            <p>ある与えられた値\(D\)に対し、平均ひずみ\(\overline{d}\)が\(\overline{d} \leq D\)を満たす条件下で、あらゆる情報源符号化法を考えたときの相互情報量\(I(X;Y)\)の最小値を考え、これを\(R(D)\)と表す。</p>
            <p>すなわち、</p>
            <div class="scroll">
                \begin{align}
                R(D) = \min_{\overline{d} \leq D} {I(X;Y)}
                \end{align}
            </div>
            <p>これを情報源\(S\)の<b>測度・ひずみ関数</b>と呼ぶ。つまり、これが平均符号長の下限になる。</p>
            <p><b>ひずみが許される場合の情報源符号化定理</b></p>
            <p>平均ひずみ\(\overline{d}\)を\(D\)以下に抑えるという条件の下で、任意の正数\(\epsilon\)に対して、情報源\(S\)を1情報源記号あたりの平均符号長\(L\)が</p>
            <div class="scroll">
                \begin{align}
                R(D) \leq L < R(D) + \epsilon
                \end{align}
            </div>
            <p>となるような2元符号へ符号化できる。しかし、どのような符号化を行っても、\(\overline{d} \leq D\)である限り、\(L\)をこの式の左辺より小さくすることはできない。</p>
            <p>つまり、1情報源記号当たりの平均符号長が、測度・ひずみ関数\(R(D)\)にいくらでも近づく符号化法が存在する。</p>
        <h3>通信路のモデル</h3>
            <p><b>通信路符号化の考え方</b></p>
            <p>今、自分が、ある通信路を通じて、0と1のビット列で表した情報を相手に送るとする。</p>
            <p>しかし、通信路は誤りを起こす可能性がある。</p>
            <p>もし送った情報に誤りが入ると、元のデータに戻せなくて、正しくない情報が相手に伝わってしまう。</p>
            <p>次のことは可能か？</p>
            <ol>
                <li>受け取ったデータを見るだけで、誤りが入っているかどうかがわかる（<b>誤り検出</b>）</li>
                <li>データに誤りがあるとわかったら、その誤りを除いて、元のデータを復元できる（<b>誤り訂正</b>）</li>
            </ol>
            <p>これらは、元のデータに少し「ムダ」を入れて符号化して、少しくらい誤りが入ってもわかるようにすることで、可能となる。</p>
            <p><b>通信路の統計的表現</b></p>
            <p>雑音のある離散的通信路の定義：時点ごとに一つの記号が入力され、一つの記号が出力される。出力は入力から一意に定まるのではなく、確率的に決まる。</p>
            <p>つまり、以下のようなイメージ。</p>
            <div class="scroll">
                \begin{align}
                \text{　入力アルファベット　} A = {a_1, a_2, \cdots, a_r} \xrightarrow{X_t} \text{　通信路　} \xrightarrow{Y_t} \text{　出力アルファベット　} B = {b_1, b_2, \cdots, b_s}
                \end{align}
            </div>
            <p>\(|A| = |B| = r\)のときは、<b>\(r\)元通信路</b>という。</p>
            <p><b>記憶のない定常通信路</b></p>
            <p>各時点の出力の現れ方が、その時点の入力には関係するが、それ以外の時点の入力と出力とは独立であるような通信路を、<b>記憶のない通信路</b>という。</p>
            <p>さらに、時間をずらしても統計的性質が変わらないとき、これを<b>記憶のない定常通信路</b>と呼ぶ。</p>
            <p>記憶のない定常通信路では、入力\(X\)が通信路に投入されたときに出力\(Y\)が出る条件付き確率\(P_{Y|X} (y|x)\)が、すべての時点において同一である。したがって、</p>
            <div class="scroll">
                \begin{align}
                P_{Y_0 \cdots Y_{n-1} | X_0 \cdots X_{n-1}} (y_0, \cdots, y_{n-1} | x_0, \cdots, x_{n-1}) = \prod_{i = 0}^{n-1} P_{Y|X} (y_i | x_i)
                \end{align}
            </div>
            <p><b>通信路行列</b></p>
            <p>\(r\)元入力アルファベット\(A = {a_1, a_2, \cdots, a_r}\)、\(s\)元出力アルファベット\(B = {b_1, b_2, \cdots, b_s}\)、入出力の関係が条件付き確率\(p_{ij} = P_{Y|X} (b_j |a_i)\)で与えられる記憶のない定常通信路を考える。</p>
            <p>つまり、以下のようなイメージ。</p>
            <div class="scroll">
                \begin{align}
                \text{　入力　} a_i \xrightarrow{p_{ij}} \text{　出力　}b_j
                \end{align}
            </div>
            <p>このとき、通信路行列は以下のようにかける。行が入力で、列が出力である。</p>
            <div class="scroll">
                \begin{align}
                T = \begin{bmatrix}
                p_{11} & p_{12} & \cdots & p_{1s} \\
                p_{21} & p_{22} & \cdots & p_{2s} \\
                \vdots & \vdots & \ddots & \vdots \\
                p_{r1} & p_{r2} & \cdots & p_{rs}
                \end{bmatrix}
                \end{align}
            </div>
            <p><b>加法的2元通信路</b></p>
            <p>入力と出力のアルファベットが共に{0,1}であるような2元通信路は、誤りの有無を用いて表すことができる。</p>
            <p>\(t\)時点での誤りを確率変数\(E_t \in {0,1}\)で表すと、出力\(Y_t\)は入力\(X_t\)に誤り\(E_t\)を加えたものとみなせる。</p>
            <div class="scroll">
                \begin{align}
                Y_t = X_t \oplus E_t \\
                E_t = 
                \begin{cases}
                0 \quad \text{誤りなし} \\
                1 \quad \text{誤り発生}
                \end{cases}
                \end{align}
            </div>
            <ul>
                <li>入力0　→　出力0</li>
                <li>入力0　→　出力1</li>
                <li>入力1　→　出力0</li>
                <li>入力1　→　出力1</li>
            </ul>
            <p>誤りの発生は入力と統計的に独立であると仮定される。</p>
            <p><b>ランダム誤り通信路</b></p>
            <p>加法的2元通信路の誤り源\(S_E\)が、1,0をそれぞれ確率\(p, 1-p\)で発生させる記憶のない定常2元情報源とする。このとき、0から1への誤りも、その逆も、他の時点の入出力とは無関係に確率\(p\)で発生する。これは<b>2元対称通信路</b>に他ならない。</p>
            <p>このような誤りを<b>ランダム誤り</b>といい、誤りの発生確率\(p\)を<b>ビット誤り率</b>という。</p>
            <p><b>バースト誤り通信路</b></p>
            <p>誤りが一度生じると、その後しばらくの間は連続して誤りが発生すると考えるモデル（誤り源に記憶がある代表的なモデル）。</p>
            <p>密集して生じる誤りを<b>バースト誤り</b>と呼ぶ。</p>
            <p><b>ソリッドバースト誤りの平均長</b></p>
            <p>誤り系列における1の連続（ラン）を任意に一つ取り出し、その長さが\(l\)となる確率\(P_L (l)\)を求めると、</p>
            <div class="scroll">
                \begin{align}
                P_L (l) = (1 - p)^{l-1} p \quad (l = 1,2,\cdots)
                \end{align}
            </div>
            <p>となる。</p>
            <p>バースト誤りの長さ（バースト長）の平均値\(\overline{l}\)は次のようになる。</p>
            <div class="scroll">
                \begin{align}
                \overline{l} &= \sum_{l=1}^{\infty} l P_L (l) \\
                &= p \sum_{l=1}^{\infty} l (1-p)^{l-1} \\
                &= \frac{1}{p}
                \end{align}
            </div>
            <p><b>その他のバースト誤りモデル</b></p>
            <p><b>ギルバートモデル</b>：正誤が混在するバースト誤り</p>
            <p><b>フリッチマンモデル</b>：ギルバートモデルの良状態を増やしたもの</p>
        <h3>通信路符号化の限界(1)</h3>
            <p><b>通信路符号の基礎概念</b></p>
            <p>通信路符号化の目的：信頼性の向上　→　そのために冗長性を付加</p>
            <p>長さ\(k\)のブロック\(\mathbf{x} \in \Sigma^k\)を長さ\(n\)の<b>符号語</b>\(\mathbf{w_x} \in A^n\)に符号化（\(A\)は\(r\)元アルファベット）</p>
            <p>\(q^k < r^n\)：符号語として\(A^n\)の一部のみ使用（冗長性の付加）</p>
            <p><b>通信路符号</b>：\(A^n\)の中から選ばれた系列の集合\(C\)</p>
            <p><b>情報速度と冗長度</b></p>
            <p>符号\(C\)に含まれる符号語の数を\(M\)とする。</p>
            <p>符号\(C\)の<b>情報（伝送）速度</b>\(R\):</p>
            <ul>
                <li>符号\(C\)を用いて伝達可能な1記号あたりの最大平均情報量</li>
                <li>\(R = \frac{\log_2 M}{n}\)　（ビット/記号）</li>
                <li>符号語を受け取ることにより得られる最大平均情報量\(\log_2 M\)を符号語の長さ\(n\)で割った値</li>
            </ul>
            <p>\(A^n\)に含まれる\(r^n\)個の系列すべてを符号語とすれば、情報速度は最大値</p>
            <div class="scroll">
                \begin{align}
                R_{max} = \frac{\log_2 r^n}{n} = \log_2 r
                \end{align}
            </div>
            <p>となる。\(R < R_{max}\)とすることで、誤りの訂正や検出が可能となる（すべての系列を使うと、誤りを検知できない）。</p>
            <p>情報速度\(R\)の符号\(C\)の効率（符号化率）\(\eta\)と冗長度\(\rho\):</p>
            <div class="scroll">
                \begin{align}
                \eta = \frac{R}{R_{max}}, \quad \rho = 1 - \eta
                \end{align}
            </div>
            <p>（ただし、\(0 \leq \eta \leq 1, \quad 0 \leq \rho \leq 1\)）が成り立つ。</p>
            <p><b>最尤復号法(1)</b></p>
            <p>\(\mathbf{\Omega_1}, \mathbf{\Omega_2}, \cdots, \mathbf{\Omega_M},\): 符号語\(\mathbf{w_1}, \mathbf{w_2}, \cdots, \mathbf{w_M}\)の復号領域</p>
            <p>正しく復号される確率を最大にする復号領域の求め方は、（符号語の発生確率が等確率の場合は）<b>最尤復号法</b></p>
            <p>\(P(\mathbf{y}|\mathbf{w_i})\): \(\mathbf{w_i}\)を送ったとき\(\mathbf{y}\)が受信される確率</p>
            <p>最尤復号法：\(\mathbf{y}\)が受信されたとき、\(P(\mathbf{y}|\mathbf{w_i})\)が最大の\(\mathbf{w_i}\)に復号</p>
            <p>最尤復号法の復号領域：\(\mathbf{\Omega_i} = \{\mathbf{y} | i = \max_j P(\mathbf{y} | \mathbf{w_j})\}\)</p>
            <p>\(P_C(\mathbf{w_i}) = \sum_{\mathbf{y} \in \mathbf{\Omega_i}}\):符号語\(\mathbf{w_i}\)を送った場合に正しく復号される確率</p>
            <p>どの符号語が送られてくる確率も等しく\(\frac{1}{M}\)であると仮定すると、正しく復号される確率\(P_C\)は</p>
            <div class="scroll">
                \begin{align}
                P_C = \sum_{i = 1}^{M} \frac{1}{M} \times P_C (\mathbf{w_i}) = \frac{1}{M} \sum_{i = 1}^{M} \sum_{\mathbf{y} \in \mathbf{\Omega_i}} P(\mathbf{y} | \mathbf{w_i})
                \end{align}
            </div>
            <p>とかける。\(\mathbf{\Omega_i} = \{\mathbf{y} | i = \max_j P(\mathbf{y} | \mathbf{w_i})\}\)とすれば\(P_C\)が最大になる。</p>
            <p>\(\mathbf{w_i}\)を\(\mathbf{y}\)のパラメータだとみなせば、\(P(\mathbf{y} | \mathbf{w_i})\)は\(\mathbf{w_i}\)の<b>尤度</b>である。</p>
            <p>尤度を最大化する復号法　→　最尤復号法</p>
            <p>最尤復号法は、各符号語が等確率で送られるとき、正しく復号される確率\(P_C\)を最大とするという意味で、最良の復号法である。しかし、すべての符号語\(\mathbf{w_i}\)に対し、\(P(\mathbf{y} | \mathbf{w_i})\)を計算し比較しなければならず、符号語数\(M\)が大きい場合には実用的ではない。</p>
            <p><b>通信路容量</b></p>
            <p>通信路の通信路容量：通信路ごとに定まる、その通信路を介して伝送できる情報速度の限界値。</p>
            <p>つまり、その通信路を介して、送ることができる1記号あたりの平均情報量の上限。</p>
            <p>\(P_{Y|X}\):定常通信路の定常条件付き確率分布</p>
            <ul>
                <li>1記号\(X\)を送ると1記号\(Y\)が受信される。→　平均的には1記号あたり相互情報量\(I(X;Y)\)だけ情報が送られる。</li>
                <li>入力\(X\)の分布は自由に変えることができるが、\(X\)の分布\(P_X\)が定まれば\(Y\)の分布は\(P_X\)と\(P_{Y|X}\)から定まる。</li>
            </ul>
            <p>この通信路を介して送ることができる1記号当たりの平均情報量の上限は</p>
            <div class="scroll">
                \begin{align}
                \max_{P_X} I(X;Y)
                \end{align}
            </div>
            <p>以上の考察から、通信路容量\(C\)の定義は\(\max_{P_X} I(X;Y)\)？</p>
            <ul>
                <li>無記憶通信路の場合はYES。記憶のある通信路の場合はNO。</li>
                <li>記憶のある通信路の場合、\(n\)（nは1より大きい）記号ごとにみるともっと多くの情報を送れる。</li>
            </ul>
            <p>\(P_{Y_n | X_n}\)：定常通信路の定常条件付き確率分布</p>
            <ul>
                <li>\(n\)記号\(X_n\)を送ると\(n\)記号\(Y_n\)が受信される。→　平均的には1記号あたり相互情報量\(\frac{I(X_n;Y_n)}{n}\)だけ情報が送られる。</li>
                <li>入力\(X_n\)の分布は自由に変えることができるが、\(X_n\)の分布\(P_{X_n}\)が定まれば\(Y_n\)の分布は\(P_{X_n}\)と\(P_{Y_n | X_n}\)から定まる。</li>
            </ul>
            <p>この通信路を介して送ることができる1記号当たりの平均情報量の上限は</p>
            <div class="scroll">
                \begin{align}
                \max_{P_{X_n}} \frac{I(X_n;Y_n)}{n}
                \end{align}
            </div>
            <p>長さ\(n\)の入力系列\(X_n\)に対する通信路の出力系列\(Y_n\)に対し、</p>
            <div class="scroll">
                \begin{align}
                C = \lim_{n \rightarrow \infty} \max_{P_{X_n}} \frac{I(X_n;Y_n)}{n}
                \end{align}
            </div>
            <p>で定義される\(C\)を<b>通信路容量</b>と呼ぶ。</p>
            <p>通信路容量の単位：<b>ビット</b>あるいは<b>ビット/通信路記号</b></p>
            <p>入力記号\(X\)に対し記号\(Y\)が出力される記憶のない通信路の通信路容量は\(C=\max_{P_X} I(X;Y)\)で計算できる。</p>
        <h3>通信路符号化の限界(2)</h3>
            <p><b>記憶のない一様通信路の通信路容量</b></p>
            <p>各時刻に\(r\)元アルファベットに属する記号\(X\)を入力し、\(s\)元アルファベットに属する記号\(Y\)を出力する、入力について一様な記憶のない定常通信路容量を\(C\)とする。</p>
            <p>この通信路の通信路行列の各行の要素の集合を{\(p_1, p_2, \cdots, p_s\)}とすれば、</p>
            <div class="scroll">
                \begin{align}
                C = \max_{P_X} H(Y) + \sum_{i = 1}^{s} p_i \log_2 p_i
                \end{align}
            </div>
            <p>が成り立つ。通信路が2重に一様であれば、</p>
            <div class="scroll">
                \begin{align}
                C = \log_2 s + \sum_{i = 1}^{s} p_i \log_2 p_i
                \end{align}
            </div>
            <p>が成り立つ。</p>
            <p>記憶のない定常通信路が入力（出力）について一様 \(\overset{\mathrm{def}}{\iff}\) 通信と行列のどの行（列）も同じ要素を並び替えたものになっている。</p>
            <p>記憶のない定常通信路が2重に一様 \(\overset{\mathrm{def}}{\iff}\) 通信路が入力について一様かつ出力について一様。</p>
            <p>例：2元対称通信路</p>
            <p><b>加法的2元通信路の通信路容量</b></p>
            <p>誤り源\(S_E\)により定義される加法的2元通信路の通信路容量\(C\)は</p>
            <div class="scroll">
                \begin{align}
                C = 1 - H(S_E)
                \end{align}
            </div>
            <p>である。</p>
            <p>誤り源\(S_E\)により定義される加法的2元通信路</p>
            <ul>
                <li>通信路内の誤り源\(S_E\)が各時刻に\(E \in {0,1}\)を発生。</li>
                <li>通信路の各時刻の入力\(X \in {0,1}\)に対し出力\(Y \in {0,1}\)は\(Y = X \oplus E\)により定まる。</li>
                <li>\(X\)と\(E\)は独立</li>
                <li>誤り源\(S_E\)が記憶のある定常情報源のとき加法的2元通信路は記憶のある定常通信路となる。</li>
            </ul>
            <p><b>誤り源が無記憶定常情報源の場合</b></p>
            <p>このとき、加法的2元通信路は<b>2元対称通信路</b>になる。</p>
            <p>\(P(E = 1) = p, P(E = 0) = 1-p\)とすれば、</p>
            <div class="scroll">
                \begin{align}
                H(S_E) = -p \log_2 p - (1-p) \log_2 (1-p) = \mathcal{H}(p)
                \end{align}
            </div>
            <p>よって、\(C = 1 - H(S_E)\)を用いて、</p>
            <div class="scroll">
                \begin{align}
                C = 1 - H(S_E) = 1 - \mathcal{H}(p)
                \end{align}
            </div>
            <p><b>通信路容量と情報速度の関係についての考察</b></p>
            <p>情報速度\(R \overset{\mathrm{def}}{\iff}\)通信路符号系列に埋め込まれた1通信路記号あたりの平均情報量（ビット/記号）</p>
            <p>通信路容量\(C \overset{\mathrm{def}}{\iff}\)通信路を介して伝送できる1通信路記号あたり平均情報量（ビット/記号）</p>
            <p>情報速度\(R\) \(<\) 通信路容量\(C\)であれば情報をすべて送ることができるのでは？　→　復号誤り率を0にできるということ？</p>
            <p><b>通信路符号化定理</b></p>
            <p>通信路容量は\(C\)である通信路に対し、\(R < C\)であれば、情報速度\(R\)の符号で復号誤り率がいくらでも小さいものが存在する。\(R>C\)であれば、そのような符号は存在しない。</p>
            <p><b>通信の限界</b></p>
            <p>エントロピーが\(H(S)\)（ビット/情報源記号）情報源と通信路容量は\(C\)（ビット/情報源記号）の通信路の通信システムは全体としてどれだけ効率的に信頼性高く情報が伝送できるか？</p>
            <p>情報源から情報源記号が毎秒α個発生し、通信路では毎秒β個の通信路記号が伝送されているとする。</p>
            <p>このとき、情報源からは\(\mathcal{R} = \alpha H(S)\)（ビット/秒）の速度で情報が発生する。</p>
            <p>また、通信路容量は1秒あたり\(\mathcal{C} = \beta C\)（ビット/秒）となる。</p>
            <p>もし、\(\mathcal{R} < \mathcal{C}\)ならば、任意に小さい誤り率で情報を宛先まで送ることができる。</p>
            <p>しかし、\(\mathcal{R} > \mathcal{C}\)の場合は、ひずみが生じる。</p>
            <p>情報源\(S\)の速度・ひずみ関数を\(R(D)\)（ビット/情報源記号）とし、\(\alpha R(D*) = \mathcal{C}\)を満たす\(D^*\)を考える。</p>
            <p>このとき、任意の\(\epsilon > 0\)に対し、\(\alpha R(D^* + \epsilon) < \alpha R(D^*) = \mathcal{C}\)が成り立つ。</p>
            <p>このとき、通信路符号化定理より、情報速度\(R(D* + \epsilon)\)の符号でいくらでも復号誤り率が小さな符号が存在するので、\(D^* + \epsilon\)にいくらでも近い平均ひずみで情報を送ることができる。</p>
            <p>これは、任意の\(\epsilon > 0\)について成り立つので、\(D^*\)にいくらでも近い平均ひずみで情報を送ることができる。</p>
            <p>情報速度\(\mathcal{R}\)（ビット/秒）で発生する情報を、通信路容量（\(\mathcal{C}\)ビット/秒）の通信路を介して送るとき、\(\mathcal{R} < \mathcal{C}\)であれば、任意に小さい誤り率で情報を伝送できる。</p>
            <p>また、\(\mathcal{R} > \mathcal{C}\)であれば、情報源の速度・ひずみ関数の値が\(\mathcal{R}(D^*) = \mathcal{C}\)（ビット/秒）を満たす\(D^*\)に対し、\(D^*\)に任意に近い平均ひずみで情報を伝送できるが、\(D^*\)より小さい平均ひずみでは伝送できない。</p>
        <h3>通信路符号化法</h3>
            <p><b>単一パリティ検査符号の定義</b></p>
            <p>問題：0,1からなる長さ\(k\)の系列\(x_1, x_2, \cdots, x_k\)を2元通信路を介して伝送したい。1個の誤りが生じた場合、それを検出するにはどうすればよいか？</p>
            <p>単一パリティ検査符号：系列\(x_1, x_2, \cdots, x_k\)を、含まれる1の個数が偶数になるように、もう一つ記号\(c\)を付け加えた符号語\(\mathbf{w} = x_1 x_2 \cdots x_k c\)に対応させる符号化</p>
            <p>このとき、加える符号\(c\)は、\(c = x_1 + x_2 + \cdots + x_k\)とかける。ただし、+ は排他的論理和\(\oplus\)である。</p>
            <p>符号語\(\mathbf{w} = x_1 x_2 \cdots x_k c\)について、\(x_1 x_2 \cdots x_k\)の部分は<b>情報記号</b>、\(c\)は<b>（パリティ）検査記号</b>という。</p>
            <p>符号語は\(\mathbf{w} = (x_1, x_2, \cdots, x_k, c)\)とベクトルの形で表すこともある。</p>
            <p><b>単一パリティ検査符号の性質</b></p>
            <ul>
                <li>長さ\(k\)の系列を入力とする単一パリティ検査符号</li>
                <li>符号長\(n = k + 1\)、符号語数\(M = 2^k\)→情報速度\(R = \frac{k}{k+1}\)</li>
                <li>一つの誤りの検出が可能な<b>誤り検出符号</b></li>
                <li>\((k+1, k)\)組織符号</li>
                <li>線形符号</li>
            </ul>
            <p><b>組織符号</b></p>
            <p><b>組織符号</b>：\(k\)個の情報記号から、\(n - k\)個の検査記号を一定の方法で求め、付加することにより符号化される符号長\(n\)の符号</p>
            <p><b>\(n, k\)符号</b>：符号長\(n\)、情報記号数\(k\)の組織符号</p>
            <p>\((n, k)\)符号の効率は、</p>
            <div class="scroll">
                \begin{align}
                \eta = \frac{R}{R_{max}} = \frac{\log_2 M}{n} = \frac{log_2 2^k}{n} = \frac{k}{n}
                \end{align}
            </div>
            <p>である。</p>
            <p>単一パリティ検査符号は\((k + 1, k)\)符号であり、効率は\(\eta = \frac{k}{k+1}\)である。</p>
            <p><b>線形符号</b></p>
            <p>\(c = x_1 + x_2 + \cdots + x_k\)のように、検査記号が情報記号の線形な式で与えられる符号</p>
            <p>線形符号の最も基本的な性質：「任意の2つの符号語について、その成分ごとの和をとると、それがまた符号語になる。」（線形符号となるための必要十分条件）</p>
            <p><b>パリティ検査方程式</b></p>
            <p> = 0 という形で線形符号の符号語となるための必要十分条件を与える式（または式の組）</p>
            <p>単一パリティ検査符号のパリティ検査方程式：長さ\(n\)の系列\(\mathbf{w} = (w_1, w_2, \cdots, w_n)\)が単一パリティ検査符号の符号語となるための必要十分条件は以下のパリティ検査方程式を満たすことである。</p>
            <div class="scroll">
                \begin{align}
                w_1 + w_2 + \cdots + w_n = 0
                \end{align}
            </div>
            <p>（符号語に含まれる1の個数が偶数）</p>
            <p><b>シンドローム</b></p>
            <p>誤りパターン：送信した符号語\(\mathbf{w} = (w_1, w_2, \cdots, w_n)\)と受信語\(\mathbf{y} = (y_1, y_2, \cdots, y_n)\)との差<div class="scroll">\(\mathbf{e} = \mathbf{w} + \mathbf{y} = (w_1 + y_1, w_2 + y_2, \cdots, w_n + y_n)\)</div></p>
            <p>誤りパターン\(\mathbf{e} = (e_1, e_2, \cdots, e_n)\)の各ビットは以下のような値になる。</p>
            <div class="scroll">
                \begin{align}
                e_i = 
                \begin{cases}
                1 \quad \text{（第\(i\)成分に誤りが生じたとき）} \\
                0 \quad \text{（そうでないとき）}
                \end{cases}
                \end{align}
            </div>
            <p>誤りパターン\(\mathbf{e}\)を用いると\(\mathbf{y} = \mathbf{w} + \mathbf{e}\)と表現できる。</p>
            <p>シンドローム：受信語\(\mathbf{y}\)をパリティ検査方程式の左辺に代入した結果\(\mathbf{s} = y_1 + y_2 + \cdots + y_n\)</p>
            <p>符号語\(\mathbf{w}\)はパリティ検査方程式を満たすので、</p>
            <div class="scroll">
                \begin{align}
                \mathbf{s} = w_1 + e_1 + w_2 + e_2 + \cdots + w_n + e_n = e_1 + e_2 + \cdots + e_n
                \end{align}
            </div>
            <p><b>\(2 \times 2\)水平垂直パリティ検査符号</b></p>
            <p>与えられた長さ4の系列\(x_1 x_2 x_3 x_4\)に対し、長さ9の符号語\((x_1, x_2, x_3, x_4, c_1, c_2, c_3, c_4, c_5)\)を、以下のように生成する符号。</p>
            <p>下の表のように4個の情報ビットを\(2 \times 2\)の配列に並べ、各行各列に一つずつ検査ビットを付け加える。</p>
            <table>
                <tr>
                    <td>x1</td>
                    <td>x2</td>
                    <td>c1</td>
                </tr>
                <tr>
                    <td>x3</td>
                    <td>x4</td>
                    <td>c2</td>
                </tr>
                <tr>
                    <td>c3</td>
                    <td>c4</td>
                    <td>c5</td>
                </tr>
            </table>
            <p>すなわち、</p>
            <div class="scroll">
                \begin{align}
                c_1 = x_1 + x_2 \\
                c_2 = x_3 + x_4 \\
                c_3 = x_1 + x_3 \\
                c_4 = x_2 + x_4
                \end{align}
            </div>
            <p>さらに、検査ビットの行の1の数が偶数になるように、検査ビットの検査ビットを右隅におく。</p>
            <div class="scroll">
                \begin{align}
                c_5 = c_1 + c_2 = x_1 + x_2 + x_3 + x_4 = c_3 + c_4
                \end{align}
            </div>
            <p><b>\(2 \times 2\)水平垂直パリティ検査符号の性質</b></p>
            <p>符号長\(n = 9\)、情報記号長4の\((9, 4)\)組織符号である。</p>
            <p>符号語数\(M = 2^4\)であるから、情報速度\(R = \frac{\log_2 M}{n} = \frac{4}{9}\)、効率\(\eta = \frac{4}{9}\)</p>
            <p>検査符号が情報記号の線形式で計算できるので線形符号である。</p>
            <p>パリティ検査方程式：長さ9の系列\(\mathbf{w} = (w_1, w_2, \cdots, w_9)\)に対し</p>
            <div class="scroll">
                \begin{align}
                \left\{
                \begin{array}{l}
                w_1 + w_2 + w_5 &= 0 \\
                w_3 + w_4 + w_6 &= 0 \\
                w_1 + w_3 + w_7 &= 0 \\
                w_2 + w_4 + w_8 &= 0 \\
                w_1 + w_2 + w_3 + w_4 + w_9 &= 0
                \end{array}
                \right.
                \end{align}
            </div>
            <p>シンドローム：受信語\(\mathbf{y} = (y_1, y_2, \cdots, y_9)\)に対するシンドローム\(\mathbf{s} = (s_1, s_2, \cdots, s_5)\)</p>
            <div class="scroll">
                \begin{align}
                \left\{
                \begin{array}{l}
                s_1 &= y_1 + y_2 + y_5 \\
                s_2 &= y_3 + y_4 + y_6 \\
                s_3 &= y_1 + y_3 + y_7 \\
                s_4 &= y_2 + y_4 + y_8 \\
                s_5 &= y_1 + y_2 + y_3 + y_4 + y_9
                \end{array}
                \right.
                \end{align}
            </div>
            <p><b>\(2 \times 2\)水平垂直パリティ検査符号の誤り訂正能力</b></p>
            <p>誤り訂正まで行える<b>誤り（検出）訂正符号</b></p>
            <p>1個の誤りが訂正でき、同時に2個の誤りを検出することができる（検出のみに徹すれば、3個の誤りまで検出することが可能）。</p>
            <p><b>一般の水平垂直パリティ検査符号</b></p>
            <p>符号長\(n = (m_1 + 1)(m_2 + 1)\)、情報記号長\(k = m_1 m_2\)の\(((m_1 + 1)(m_2 + 1), m_1 m_2)\)組織記号</p>
            <p>符号語数\(M = 2^{m_1 m_2}\)、情報速度\(R = \frac{\log_2 M}{n} = \frac{m_1 m_2}{(m_1 + 1)(m_2 + 1)}\)、効率\(\eta = \frac{m_1 m_2}{(m_1 + 1)(m_2 + 1)}\)</p>
            <p>誤り（検出）訂正符号</p>
            <p>訂正をする場合：1個の誤り訂正ができ、同時に2個の誤りまで検出可能。</p>
            <p>訂正しない場合：3個の誤りまで検出可能</p>
        <h3>通信路符号化法</h3>
            <p><b>ハミング符号(1)</b></p>
            <p>水平垂直パリティ検査符号の問題点：(9,4)符号であり、情報ビットよりも検査ビットが多く、効率がよくない。</p>
            <p><b>(7,4)ハミング符号</b>：4個の情報ビット\(x_1, x_2, x_3, x_4\)に対し、</p>
            <div class="scroll">
                \begin{align}
                c_1 = x_1 + x_2 + x_3 \\
                c_2 = x_2 + x_3 + x_4 \\
                c_3 = x_1 + x_2 + x_4
                \end{align}
            </div>
            <p>により、検査ビット\(c_1, c_2, c_3\)を作り、\(w = (x_1, x_2, x_3, x_4, c_1, c_2, c_3)\)という符号語に符号化する符号。</p>
            <p>情報ビットが4個であるから、符号語は\(2^4 = 16個\)</p>
            <p>符号を\(\mathbf{w} = (w_1, w_2, \cdots, w_7)\)とする。</p>
            <p>(7,4)ハミング符号のパリティ検査方程式</p>
            <div class="scroll">
                \begin{align}
                w_1 + w_2 + w_3 + w_5 &= 0 \\
                w_2 + w_3 + w_4 + w_6 &= 0 \\
                w_1 + w_2 + w_4 + w_7 &= 0
                \end{align}
            </div>
            <p>受信語\(\mathbf{y} = (y_1, y_2, \cdots, y_7)\)に対するシンドローム</p>
            <div class="scroll">
                \begin{align}
                s_1 &= y_1 + y_2 + y_3 + y_5 \\
                s_2 &= y_2 + y_3 + y_4 + y_6 \\
                s_3 &= y_1 + y_2 + y_4 + y_7
                \end{align}
            </div>
            <p>誤りパターンを\(e = (e_1, e_2, \cdots, e_7)\)とすると、</p>
            <div class="scroll">
                \begin{align}
                s_1 &= e_1 + e_2 + e_3 + e_5 \\
                s_2 &= e_2 + e_3 + e_4 + e_6 \\
                s_3 &= e_1 + e_2 + e_4 + e_7
                \end{align}
            </div>
            <p><b>生成行列</b></p>
            <p>(7,4)のハミング符号の符号語\(\mathbf{w}\)は情報記号のみで表すと</p>
            <div class="scroll">
                \begin{align}
                \mathbf{w} = (x_1, x_2, x_3, x_4, x_1 + x_2 + x_3, x_2 + x_3 + x_4, x_1 + x_2 + x_4)
                \end{align}
            </div>
            <p>という形でかける。ここで、</p>
            <div class="scroll">
                \begin{align}
                G = \begin{bmatrix}
                1 & 0 & 0 & 0 & 1 & 0 & 1 \\
                0 & 1 & 0 & 0 & 1 & 1 & 1 \\
                0 & 0 & 1 & 0 & 1 & 1 & 0 \\
                0 & 0 & 0 & 1 & 0 & 1 & 1
                \end{bmatrix}
                \end{align}
            </div>
            <p>という行列を考えれば、符号語\(w\)を\(\mathbf{w} = \mathbf{x} G\)と書くことができる。ただし、\(\mathbf{x} = (x_1, x_2, x_3, x_4)\)である。</p>
            <p><b>生成行列</b>：\(k\)個の情報記号からなるベクトル\(\mathbf{x}\)をかけたとき、対応する符号語が生成されるような行列。</p>
            <p>\((n, k)\)線形符号の生成行列は\(k \times n\)行列となる。</p>
            <p><b>検査行列</b></p>
            <p>(7,4)ハミング符号のパリティ検査方程式の係数行列\(\mathbf{H}\)</p>
            <div class="scroll">
                \begin{align}
                \mathbf{H} = \begin{bmatrix}
                1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 \\
                0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 \\
                1 & 1 & 1 & 0 & 1 & 0 & 0 & 1
                \end{bmatrix}
                \end{align}
            </div>
            <p>これを用いれば、パリティ検査方程式は\(\mathbf{w} \mathbf{H}^T = 0\)とかける。</p>
            <p>\((n, k)\)線形符号のパリティ検査行列は\((n-k) \times n\)行列</p>
            <p>検査行列\(\mathbf{H}\)を用いれば、(7,4)ハミング符号のシンドロームの計算式は\(\mathbf{s} = \mathbf{y} \mathbf{H}^T\)とかける。</p>
            <p>ここで、\(\mathbf{s} = (s_1, s_2, s_3)\)であり、<b>シンドロームパターン</b>または単に<b>シンドローム</b>と呼ばれる。</p>
            <div class="scroll">
                \begin{align}
                s = (w+e)H^T = w H^T + e H^T = e H^T
                \end{align}
            </div>
            <p><b>ハミング距離とハミング重み</b></p>
            <p>2つの\(n\)次元ベクトル\(\mathbf{u} = (u_1, u_2, \cdots, u_n), \mathbf{v} = (v_1, v_2, \cdots, v_n)\)間の<b>ハミング距離</b>\(d_H (\mathbf{u} ,\mathbf{v})\)</p>
            <div class="scroll">
                \begin{align}
                \overset{\mathrm{def}}{\iff} d_H (u, v) = \sum_{i = 1}^{n} \delta (u_i, v_i) \quad \text{ただし、} \delta (u_i, v_i) = \begin{cases} 0 \quad \text{if \(u = v\)} \\ 1 \quad \text{if \(u \neq v\)} \end{cases} 
                \end{align}
            </div>
            <p>\(d_H (\mathbf{u}, \mathbf{v})\)は\(\mathbf{u}\)と\(\mathbf{v}\)の対応する位置にある成分の対のうち、互いに異なるものの数。</p>
            <p>ハミング距離は<b>距離の3公理</b>を満たす。</p>
            <p>距離の3公理：任意の\(n\)次元ベクトル\(\mathbf{v_1, v_2, v_3}\)に対して以下のことが成り立つ。</p>
            <ol>
                <li>\(d_H (\mathbf{v_1, v_2}) \geq 0\)であり、等号が成立するのは\(\mathbf{v_1} = \mathbf{v_2}\)のときに限る。</li>
                <li>\(d_H (\mathbf{v_1, v_2}) = d_H (\mathbf{v_2, v_1})\)</li>
                <li><div class="scroll">\(d_H (\mathbf{v_1, v_2}) + d_H (\mathbf{v_2, v_3}) \geq  d_H (\mathbf{v_1, v_3})\)（三角不等式）</div></li>
            </ol>
            <p>\(n\)次元ベクトル\(v\)の<b>ハミング重み</b>\(w_H (\mathbf{v}) \overset{\mathrm{def}}{\iff} w_H (\mathbf{v}) = d_H (\mathbf{v, 0})\)</p>
            <p>\(w_H (\mathbf{v})\)は\(\mathbf{v}\)の0でない成分の数</p>
            <p>ハミング距離はハミング重みを用いて次のように表せる。</p>
            <div class="scroll">
                \begin{align}
                d_H (\mathbf{u}, \mathbf{v}) = w_H (\mathbf{u} - \mathbf{v})
                \end{align}
            </div>
            <p><b>最小距離と誤り訂正能力</b></p>
            <p>符号\(C\)の<b>最小ハミング距離</b>または<b>最小距離</b>\(d_{min} \overset{\mathrm{def}}{\iff} d_{min} = \min_{u \neq v} d_H (\mathbf{u, v})\)</p>
            <p><b>限界距離復号法</b>：式\(d_{min} \geq 2t_1 + 1\)を満たす整数\(t_1\)を定め、\(t_1\)以下の誤り訂正を行う復号法。</p>
            <p>\(t_1\)の最大値\(t_0 = \lfloor (d_{min}- 1)/2 \rfloor\)を<b>誤り訂正能力</b>という。</p>
            <p>\(t_2 = d_{min} - 2t_1 - 1\)とおけば\(t_1 + 1 \leq t \leq t_1 + t_2\)個の誤りは訂正できないが検出は可能。</p>
            <p>\(0 \leq t_1 \leq t_0\)をどのように選ぶかは重要な問題。</p>
            <p>\(t_1\)を大きくする　→　正しく復号される確率は増大するが、誤って復号される確率も増大</p>
            <p>検出さえできれば、再送要求などの救済措置が可能</p>
            <p>例）\(d_{min} = 5\)の符号による誤りの訂正と検出</p>
            <table>
                <tr>
                    <th>t<sub>1</sub></th>
                    <th>訂正可能な誤り</th>
                    <th>訂正できないが検出可能な誤り</th>
                </tr>
                <tr>
                    <td>0</td>
                    <td>－</td>
                    <td>1～4個</td>
                </tr>
                <tr>
                    <td>1</td>
                    <td>1個</td>
                    <td>2～3個</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>2個</td>
                    <td>－</td>
                </tr>
            </table>
            <p>線形符号の最小距離 = 最小ハミング重み（0でない符号語のハミング重みの最小値）</p>
            <p>例）(7,4)ハミング符号の場合：最小距離\(d_{min} =\)最小ハミング重み\(= 3\)</p>
        <h3>通信路符号化法(3)</h3>
            <p><b>巡回符号の特徴</b></p>
            <ul>
                <li>組織符号・線形符号</li>
                <li>符号化・シンドローム計算の装置化が容易</li>
                <li>誤り検出能力に優れる</li>
            </ul>
            <p>最も実用的な線形符号</p>
            <p><b>巡回符号の定義</b></p>
            <p>符号多項式：符号語の多項式表現</p>
            <p>0,1からなる長さ\(n\)の符号語\(\mathbf{v} = (v_{n-1}, v_{n-2}, \cdots, v_1, v_0)\)を</p>
            <div class="scroll">
                \begin{align}
                V(x) = v_{n-1} x^{n-1} + v_{n-2} x^{n-2} + \cdots + v_1 x + v_0
                \end{align}
            </div>
            <p>で表す（係数は0か1しか取らない）。</p>
            <p>符号長\(n\)の符号は、\(n-1\)次以下の多項式の集合として表せる。</p>
            <p>巡回符号：定数項が1の\(m\)次の多項式</p>
            <div class="scroll">
                \begin{align}
                G(x) = x^m + g_{m-1} x^{m-1} + \cdots + g_1 x + 1
                \end{align}
            </div>
            <p>の\(n-1\)次以下の倍多項式（\(W(x) = A(x) G(x)\)という形の符号多項式）すべての集合\(C\)</p>
            <p><b>巡回符号の例</b></p>
            <p>\(G(x) = x^4 + x^3 + x^2 + 1\)によって作られる長さ7の符号\(C\)</p>
            <p>符号多項式\(W(x) = w_6 x^6 + \cdots + w_1 x + w_0\)</p>
            <p>および符号語\(\mathbf{w} = (w_6, \cdots, w_1, w_0)\)</p>
            <table>
                <tr>
                    <th>A(x)</th>
                    <th>W<sub>1</sub>(x) = A<sub>1</sub>(x)G(x)</th>
                    <th>w</th>
                </tr>
                <tr>
                    <td>0</td>
                    <td>0</td>
                    <td>0000000</td>
                </tr>
                <tr>
                    <td>1</td>
                    <td>x<sup>4</sup> + x<sup>3</sup> + x<sup>2</sup> + 1</td>
                    <td>0011101</td>
                </tr>
                <tr>
                    <td>x</td>
                    <td>x<sup>5</sup> + x<sup>4</sup> + x<sup>3</sup> + x</td>
                    <td>0111010</td>
                </tr>
                <tr>
                    <td>x + 1</td>
                    <td>x<sup>5</sup> + x<sup>2</sup> + x + 1</td>
                    <td>0100111</td>
                </tr>
                <tr>
                    <td>x<sup>2</sup></td>
                    <td>x<sup>6</sup> + x<sup>5</sup> + x<sup>4</sup> + x<sup>2</sup></td>
                    <td>1110100</td>
                </tr>
                <tr>
                    <td>x<sup>2</sup> + 1</td>
                    <td>x<sup>6</sup> + x<sup>5</sup> x<sup>3</sup> + 1</td>
                    <td>1101001</td>
                </tr>
                <tr>
                    <td>x<sup>2</sup> + x</td>
                    <td>x<sup>6</sup> + x<sup>3</sup> + x<sup>2</sup> + x</td>
                    <td>1001110</td>
                </tr>
                <tr>
                    <td>x<sup>2</sup> + x + 1</td>
                    <td>x<sup>6</sup> + x<sup>4</sup> + x + 1</td>
                    <td>1010011</td>
                </tr>
            </table>
            <p>\(A(x)\)と\(G(x)\)の乗算：ひっ算して足し算するときに、項が偶数個だと消える（繰り上がらない）ことに注意。</p>
            <p>巡回符号\(C\)は\(G(x)\)によって生成される　→　\(G(x)\)：\(C\)の<b>生成多項式</b></p>
            <p><b>巡回符号は線形符号</b></p>
            <p>線形符号となるための必要十分条件：任意の二つの符号語の和が符号語</p>
            <p>巡回符号\(C\)が線形符号かどうか</p>
            <ol>
                <li>\(W_1 (x)\)と\(W_2 (x)\)は\(C\)の符号多項式</li>
                <li><div class="scroll">\(W_1 (x) = A_1 (x) G(x), \quad W_2 (x) = A_2 (x) G(x)\)とかける</div></li>
                <li><div class="scroll">\(W_1 (x) + W_2 (x) = [A_1 (x) + A_2 (x)] G(x)\)</div></li>
                <li>\(W_1 (x) + W_2 (x)\)は\(C\)の符号多項式</li>
            </ol>
            <p>線形符号である　→　検査符号が情報記号の線形式でかける　→　組織符号の形で符号語を作ることができる</p>
            <p><b>巡回符号の組織符号化法</b></p>
            <p>\(n-m\)個の情報ビット列\(x_{n-m-1}, \cdots, x_1, x_0\)を\(C\)の符号語に組織符号化する方法</p>
            <ol>
                <li>
                    情報ビット列を係数とする多項式
                    <div class="scroll">
                        \begin{align}
                        X(x) = x_{n-m-1} x^{n-m-1} + \cdots + x_1 x + x_0
                        \end{align}
                    </div>
                    に\(x^m\)をかけ、それを\(m\)次の\(G(x)\)で割った剰余多項式
                    <div class="scroll">
                        \begin{align}
                        C(x) = c_{m-1} x^{m-1} + \cdots + c_1 x + c_0
                        \end{align}
                    </div>
                    を計算。\(C(x)\)は
                    <div class="scroll">
                        \begin{align}
                        X(x) x^m = A(x) G(x) + C(x) \quad \text{(1)}
                        \end{align}
                    </div>
                    となる\(m-1\)次以下の多項式。
                </li>
                <li>
                    <div class="scroll">
                        \begin{align}
                        W(x) = X(x) x^m - C(x) = X(x) x^m + C(x)
                        \end{align}
                    </div>
                    により\(W(x)\)を計算。
                    式(1)から\(W(x) = A(x) G(x) \Rightarrow W(x)\)は\(C\)の符号多項式 <br>
                    \(W(x)\)のベクトル表現：
                    <div class="scroll">
                        \begin{align}
                        \mathbf{w} = (x_{n-m-1}, \cdots, x_1, x_0, c_{m-1}, \cdots, c_1, c_0)
                        \end{align}
                    </div>
                </li>
            </ol>
            <p><b>符号化の例</b></p>
            <p>生成多項式：\(G(x) = x^4 + x^3 + x^2 + 1\)、符号長：\(n = 7\)、情報ビット数：\(k = 3\)（生成多項式は4次なので、\(7-4=3\)）</p>
            <p>情報ビット101の符号化</p>
            <ol>
                <li>情報ビットを係数とする多項式：\(X(x) = x^2 + 1\)</li>
                <li>\(x^{n-k} = x^4\)をかける：\(X(x) x^4 = x^6 + x^4\)</li>
                <li>\(G(x)\)で割った剰余：\(C(x) = x + 1\)</li>
                <li>符号多項式：<div class="scroll">\(W(x) = X(x) x^4 + C(x) = x^6 + x^4 + x + 1\)</div></li>
                <li>符号語：1010011</li>
            </ol>
            <p><b>なぜ「巡回」なのか？</b></p>
            <p>符号長\(n\)、生成多項式\(G(x)\)の巡回符号において、\(G(x)\)が\(x^n - 1\)を割りきれば符号語\(\mathbf{w}\)の成分を巡回置換して得られる\(\mathbf{w'}\)も符号語となる。</p>
            <p>\(G(x)\)が\(x^n - 1\)を割り切れば</p>
            <div class="scroll">
                \begin{align}
                W(x) = w_{n-1} x^{n-1} + \cdots + w_1 x + w_0
                \end{align}
            </div>
            <p>が符号多項式。</p>
            <div class="scroll">
                \begin{align}
                W'(x) = w_{n-2} x^{n-1} + \cdots + w_0 x + w_{n-1}
                \end{align}
            </div>
            <p>という多項式もまた符号多項式。</p>
            <p>本来、長さ\(n\)の巡回符号の生成多項式\(G(x)\)は\(x^n -1\)を割り切らなければならない（厳密には、成立しないものを<b>擬巡回符号</b>と呼ぶ）。</p>
            <p>\(G(x)\)で生成される符号は、\(G(x)\)が\(x^n -1\)を割り切らなくても、ほとんど同様に扱えるため、ここでは擬巡回符号も含めて、単に巡回符号と呼ぶ。</p>
            <p><b>\(G(x)\)の周期</b></p>
            <p>多項式\(G(x)\)の周期：\(G(x)\)が\(x^n - 1\)を割り切る最小の正整数\(n\)</p>
            <p>\(G(x)\)で生成される巡回符号\(C\)の符号長\(n\)は、通常、\(G(x)\)の周期\(p\)以下に選ばれる。</p>
            <p>\(n > p\)であると、誤り訂正できない。</p>
            <p>例）\(G(x) = x^4 + x^3  + x^2 + 1\)を生成多項式とする長さ7の巡回符号：\(G(x)\)の周期は7（∵\(G(x)\)は\(x^7 - 1\)を割り切るが、\(x^l - 1 \quad (l = 1,2,\cdots,6)\)は割り切らない）。</p>
            <p><b>巡回符号の最小距離</b></p>
            <p>定理：周期\(p\)の生成多項式\(G(x)\)による符号長\(n\)の巡回符号の最小ハミング距離\(d_{min}\)は、\(n \leq p\)であれば3以上である。</p>
            <p><b>巡回符号による誤りの検出</b></p>
            <p>誤りの検出：受信語\(\mathbf{y}\)が符号語になるかどうか調べる。</p>
            <p>\(n - 1\)次以下の多項式\(Y(x)\)が長さ\(n\)、生成多項式\(G(x)\)の巡回符号の符号多項式　⇔　\(Y(x)\)が\(G(x)\)で割り切れる</p>
            <p><b>巡回冗長検査方式</b>：受信語\(\mathbf{y} = (y_{n-1}, \cdots, y_1, y_0)\)を表す多項式</p>
            <div class="scroll">
                \begin{align}
                Y(x) = y_{n-1} x^{n-1} + \cdots + y_1 x + y_0 
                \end{align}
            </div>
            <p>が\(G(x)\)で割り切れない（受信語を\(G(x)\)で割る割り算回路に読み込ませて、剰余が0にならない）　→　誤りがある</p>
            <p><b>巡回ハミング符号</b></p>
            <p>0,1を係数とする\(m\)次の多項式の周期\(\leq 2^m -1\)</p>
            <p>原始多項式：周期がちょうど\(2^m - 1\)となる\(m\)次の多項式</p>
            <p>巡回ハミング符号：\(m\)次の原始多項式を生成多項式とする符号長\(n = 2^m - 1\)の符号</p>
            <ul>
                <li>符号長：\(n = 2^m - 1\)</li>
                <li>情報ビット数：\(k = 2^m - 1 - m\)</li>
                <li>検査ビット数：\(m\)</li>
                <li>最小距離：\(d_{min} = 3\)　→　単一誤り訂正符号</li>
            </ul>
            <p>（これらの特徴はハミング符号と同じ）</p>
            <p><b>巡回ハミング符号の例</b></p>
            <p>\(G(x) = x^3 + x + 1\)を生成多項式とする長さ7の巡回ハミング符号</p>
            <p>この符号の検査行列を求める。</p>
            <p>\(R_i (x) : x^i (i = 0, 1, \cdots, 6)\)を\(G(x)\)で割った剰余多項式</p>
            <p>これを実際に計算すると表のようになる。</p>
            <table>
                <tr>
                    <th>i</th>
                    <th>R<sub>i</sub>(x)</th>
                </tr>
                <tr>
                    <td>0</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>1</td>
                    <td>x</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>x<sup>2</sup></td>
                </tr>
                <tr>
                    <td>3</td>
                    <td>x + 1</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>x<sup>2</sup> + x</td>
                </tr>
                <tr>
                    <td>5</td>
                    <td>x<sup>2</sup> + x + 1</td>
                </tr>
                <tr>
                    <td>6</td>
                    <td>x<sup>2</sup> + 1</td>
                </tr>
            </table>
            <p>\(W(x) = w_6 x^6 + \cdots + w_1 x + w_0\)が\(G(x)\)で割り切れる　⇔　\(w_i x^i\)を\(G(x)\)で割った剰余多項式の和が0　⇔　\(\sum_{i = 0}^{6} w_i R_i (x) = 0\)</p>
            <p>この式の左辺を\(x\)の\(2, 1, 0\)次の項の係数ごとに書けば、</p>
            <div class="scroll">
                \begin{align}
                w_6 + w_5 + w_4 + w_2 = 0 \\
                w_5 + w_4 + w_3 + w_1 = 0 \\
                w_6 + w_5 + w_3 + w_0 = 0
                \end{align}
            </div>
            <p>となる。この係数行列は</p>
            <div class="scroll">
                \begin{align}
                \mathbf{H} = \begin{bmatrix}
                1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 \\
                0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 \\
                1 & 1 & 0 & 1 & 0 & 0 & 0 & 1
                \end{bmatrix}
                \end{align}
            </div>
            <p>となって、\(H\)は(7,4)ハミング符号の検査行列になる。</p>
        <h3>演習</h3>
            <div class="example">
                <p><b>問題</b></p>
                <p>確率変数 \( X \) が値 \( A, B, C \) を取る確率 \( P(X) \) が表 1、確率変数 \( X \) で条件を付けた確率変数 \( Y \) の条件付き確率 \( P(Y|X) \) が表 2 のように定義されているとき、以下の問いに答えよ。</p>
                <ol>
                    <li>
                        \( X \) のエントロピー \( H(X) \) [ビット] を求めよ。また \( H(X) \) が大きいことの意味を簡潔に述べよ。
                    </li>
                    <li>
                        \( X \) と \( Y \) の相互情報量 \( I(X;Y) \) [ビット] を求めよ。また \( I(X;Y) \) が大きいことの意味を簡潔に述べよ。
                    </li>
                    <li>
                        \( X \) の値ごとに、符号アルファベットが \(\{0,1\}\) の二元符号化をした場合、平均符号長が最短となる可逆符号を1つ示し、その平均符号長を求めよ。
                    </li>
                    <li>
                        \( (X, Y) \) の値ごとに、符号アルファベットが \(\{0,1\}\) の二元符号化をした場合、平均符号長が最短となる可逆符号を1つ示し、その平均符号長を求めよ。
                    </li>
                    <li>
                        任意の入力 \( X \) に対し、表 2 で与えられた条件付き確率 \( P(Y|X) \) で、\( Y \) を出力する無記憶定常通信路の通信路容量を求めよ。
                    </li>
                </ol>
                <p>表 1: \( P(X) \)</p>
                <table>
                    <tr>
                        <th>\( X \)</th>
                        <th>\( A \)</th>
                        <th>\( B \)</th>
                        <th>\( C \)</th>
                    </tr>
                    <tr>
                        <td>\( P(X) \)</td>
                        <td>\( \frac{1}{4} \)</td>
                        <td>\( \frac{1}{4} \)</td>
                        <td>\( \frac{1}{2} \)</td>
                    </tr>
                </table>
                <p>表 2: \( P(Y|X) \)</p>
                <table>
                    <tr>
                        <td rowspan="2" colspan="2"></td>
                        <th colspan="3">X</th>
                    </tr>
                    <tr>
                        <th>A</th>
                        <th>B</th>
                        <th>C</th>
                    </tr>
                    <tr>
                        <th rowspan="3">Y</th>
                        <td>A</td>
                        <td>\(\frac{1}{2}\)</td>
                        <td>\(0\)</td>
                        <td>\(\frac{1}{4}\)</td>
                    </tr>
                    <tr>
                        <td>B</td>
                        <td>\(0\)</td>
                        <td>\(\frac{1}{2}\)</td>
                        <td>\(\frac{1}{4}\)</td>
                    </tr>
                    <tr>
                        <td>C</td>
                        <td>\(\frac{1}{2}\)</td>
                        <td>\(\frac{1}{2}\)</td>
                        <td>\(\frac{1}{2}\)</td>
                    </tr>
                </table>
                <p><b>(1)の解答</b></p>
                <div class="scroll">
                    \begin{align}
                    H(X) &= - \frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{2} \log_2 \frac{1}{2} \\
                    &= \frac{1}{2} + \frac{1}{2} + \frac{1}{2} \\
                    &= 1.5
                    \end{align}
                </div>
                <p>\(H(X)\)が大きい：\(X\)が各値を取る確率の偏りが小さく、不確実性が大きい。</p>
                <p><b>(2)の解答</b></p>
                <p>\(I(X;Y) = H(Y) - H(Y|X)\)である。</p>
                <p>\(H(Y|X)\)は以下の式によって求められる。</p>
                <div class="scroll">
                    \begin{align}
                    H(Y|X) = - \sum P(X) \sum P(Y|X) \log_2 P(Y|X)
                    \end{align}
                </div>
                <p>よって、</p>
                <div class="scroll">
                    \begin{align}
                    H(Y|X) &= \frac{1}{4} \times (- \frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{2} \log_2 \frac{1}{2}) \\ 
                    &+ \frac{1}{4} \times (- \frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{2} \log_2 \frac{1}{2}) \\
                    &+ \frac{1}{2} \times (- \frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{2} \log_2 \frac{1}{2}) \\
                    &= 1.25
                    \end{align}
                </div>
                <p>次に、\(H(Y)\)を求めるために\(P(Y)\)を求める。</p>
                <p>\(P(Y)\)は以下のように求めることができる。</p>
                <div class="scroll">
                    \begin{align}
                    P(Y=A) &= P(X=A) \times \frac{1}{2} + P(X=B) \times 0 + P(X=C) \times \frac{1}{4} \\
                    &= \frac{1}{4} \times \frac{1}{2} + \frac{1}{4} \times 0 + \frac{1}{2} \times \frac{1}{4} \\
                    &= \frac{1}{4} \\
                    P(Y=B) &= P(X=A) \times 0 + P(X=B) \times \frac{1}{2} + P(X=C) \times \frac{1}{4} \\
                    &= \frac{1}{4} \times 0 + \frac{1}{4} \times \frac{1}{2} + \frac{1}{2} \times \frac{1}{4} \\
                    &= \frac{1}{4} \\
                    P(Y=C) &= P(X=A) \times \frac{1}{2} + P(X=B) \times \frac{1}{2} + P(X=C) \times \frac{1}{2} \\
                    &= \frac{1}{4} \times \frac{1}{2} + \frac{1}{4} \times \frac{1}{2} + \frac{1}{2} \times \frac{1}{2} \\
                    &= \frac{1}{2} 
                    \end{align}
                </div>
                <p>表2に\(P(X), P(Y)\)を加えると、以下の表が得られる。</p>
                <table>
                    <tr>
                        <td rowspan="2" colspan="2"></td>
                        <th colspan="3">\(X\)</th>
                        <th rowspan="2">\(P(Y)\)</th>
                    </tr>
                    <tr>
                        <th>\(A\)</th>
                        <th>\(B\)</th>
                        <th>\(C\)</th>
                    </tr>
                    <tr>
                        <th rowspan="3">\(Y\)</th>
                        <td>\(A\)</td>
                        <td>\(\frac{1}{2}\)</td>
                        <td>\(0\)</td>
                        <td>\(\frac{1}{4}\)</td>
                        <td>\(\frac{1}{4}\)</td>
                    </tr>
                    <tr>
                        <td>\(B\)</td>
                        <td>\(0\)</td>
                        <td>\(\frac{1}{2}\)</td>
                        <td>\(\frac{1}{4}\)</td>
                        <td>\(\frac{1}{4}\)</td>
                    </tr>
                    <tr>
                        <td>\(C\)</td>
                        <td>\(\frac{1}{2}\)</td>
                        <td>\(\frac{1}{2}\)</td>
                        <td>\(\frac{1}{2}\)</td>
                        <td>\(\frac{1}{2}\)</td>
                    </tr>
                    <tr>
                        <td colspan="2">\(P(X)\)</td>
                        <td>\(\frac{1}{4}\)</td>
                        <td>\(\frac{1}{4}\)</td>
                        <td>\(\frac{1}{2}\)</td>
                    </tr>
                </table>
                <p>したがって、\(H(Y) = H(X) = 1.5\)であり、</p>
                <div class="scroll">
                    \begin{align}
                    I(X;Y) = H(Y) - H(Y|X) = 1.5 - 1.25 = 0.25 \quad \text{[ビット]}
                    \end{align}
                </div>
                <p>となる。</p>
                <p>\(I(X;Y)\)が大きい：\(X\)と\(Y\)の片方が分かったときに他方について得られる情報が多く、不確実性が大きく減少する。</p>
                <p><b>(3)の解答</b></p>
                <p>まず、ハフマン木を描くことによって、以下のように符号語を割り当てることができる。</p>
                <ul>
                    <li>A：11（確率\(\frac{1}{4}\)）</li>
                    <li>B：10（確率\(\frac{1}{4}\)）</li>
                    <li>A：0（確率\(\frac{1}{2}\)）</li>
                </ul>
                <p>このとき、平均符号長は</p>
                <div class="scroll">
                    \begin{align}
                    \frac{1}{4} \times 2 + \frac{1}{4} \times 2 + \frac{1}{2} \times 1 = 1.5
                    \end{align}
                </div>
                <p>となる。</p>
                <p><b>(4)の解答</b></p>
                <p>結合確率\(P(X, Y)\)は以下の通りである。</p>
                <div class="scroll">
                    \begin{align}
                    P(A, A) = P(A) \times P(A | A) = \frac{1}{4} \times \frac{1}{2} = \frac{1}{8} \\
                    P(A, B) = P(A) \times P(B | A) = \frac{1}{4} \times 0 = 0 \\
                    P(A, C) = P(A) \times P(C | A) = \frac{1}{4} \times \frac{1}{2} = \frac{1}{8} \\
                    P(B, A) = P(B) \times P(A | B) = \frac{1}{4} \times 0 = 0 \\
                    P(B, B) = P(B) \times P(B | B) = \frac{1}{4} \times \frac{1}{2} = \frac{1}{8} \\
                    P(B, C) = P(B) \times P(C | B) = \frac{1}{4} \times \frac{1}{2} = \frac{1}{8} \\
                    P(C, A) = P(C) \times P(A | C) = \frac{1}{2} \times \frac{1}{4} = \frac{1}{8} \\
                    P(C, B) = P(C) \times P(B | C) = \frac{1}{2} \times \frac{1}{4} = \frac{1}{8} \\
                    P(C, C) = P(C) \times P(C | C) = \frac{1}{2} \times \frac{1}{2} = \frac{1}{4} \\
                    \end{align}
                </div>
                <p>これに基づいてハフマン木を描くと、以下のように符号化できる。</p>
                <ul>
                    <li>AA：111（確率\(\frac{1}{8}\)）</li>
                    <li>AC：110（確率\(\frac{1}{8}\)）</li>
                    <li>BB：101（確率\(\frac{1}{8}\)）</li>
                    <li>BC：100（確率\(\frac{1}{8}\)）</li>
                    <li>CA：011（確率\(\frac{1}{8}\)）</li>
                    <li>CB：010（確率\(\frac{1}{8}\)）</li>
                    <li>CC：00（確率\(\frac{1}{4}\)）</li>
                </ul>
                <p>このとき、平均符号長は</p>
                <div class="scroll">
                    \begin{align}
                    (\frac{1}{8} \times 3) \times 6 + \frac{1}{4} \times 2 = \frac{11}{4} = 2.75
                    \end{align}
                </div>
                <p>となる。</p>
                <p><b>(5)の解答</b></p>
                <p>入力は任意であるから、\(P(X)\)を以下のように再定義する。</p>
                <table>
                    <tr>
                        <th>\( X \)</th>
                        <th>\( A \)</th>
                        <th>\( B \)</th>
                        <th>\( C \)</th>
                    </tr>
                    <tr>
                        <td>\( P(X) \)</td>
                        <td>\( p_a \)</td>
                        <td>\( p_b \)</td>
                        <td>\( p_c \)</td>
                    </tr>
                </table>
                <p>ただし、\(p_a + p_b + p_c = 1\)である。</p>
                <p>無記憶定常通信路の通信路容量\(C\)は、</p>
                <div class="scroll">
                    \begin{align}
                    C = \max_{P_X} I(X;Y)
                    \end{align}
                </div>
                <p>によって計算できる。\(I(X;Y) = H(Y) - H(Y|X)\)より、まずは\(H(Y)\)と\(H(Y|X)\)を求める。</p>
                <p>次に、\(H(Y)\)を求めるために\(P(Y)\)を求める。</p>
                <p>\(P(Y)\)は以下のように求めることができる。</p>
                <div class="scroll">
                    \begin{align}
                    P(Y=A) &= P(X=A) \times \frac{1}{2} + P(X=B) \times 0 + P(X=C) \times \frac{1}{4} \\
                    &= p_a \times \frac{1}{2} + p_b \times 0 + p_c \times \frac{1}{4} \\
                    &= \frac{p_a}{2} + \frac{p_c}{4} \\
                    P(Y=B) &= P(X=A) \times 0 + P(X=B) \times \frac{1}{2} + P(X=C) \times \frac{1}{4} \\
                    &= p_a \times 0 + p_b \times \frac{1}{2} + p_c \times \frac{1}{4} \\
                    &= \frac{p_b}{2} + \frac{p_c}{4} \\
                    P(Y=C) &= P(X=A) \times \frac{1}{2} + P(X=B) \times \frac{1}{2} + P(X=C) \times \frac{1}{2} \\
                    &= p_a \times \frac{1}{2} + p_b \times \frac{1}{2} + p_c \times \frac{1}{2} \\
                    &= \frac{p_a}{2} + \frac{p_b}{2} + \frac{p_c}{2} 
                    \end{align}
                </div>
                <p>したがって、\(H(Y)\)は</p>
                <div class="scroll">
                    \begin{align}
                    H(Y) = - (\frac{p_a}{2} + \frac{p_c}{4}) \log_2 (\frac{p_a}{2} + \frac{p_c}{4}) - (\frac{p_b}{2} + \frac{p_c}{4}) \log_2 (\frac{p_b}{2} + \frac{p_c}{4}) - (\frac{p_a}{2} + \frac{p_b}{2} + \frac{p_c}{2}) \log_2 (\frac{p_a}{2} + \frac{p_b}{2} + \frac{p_c}{2})
                    \end{align}
                </div>
                <p>となる。</p>
                <p>さらに、\(H(Y|X)\)は</p>
                <div class="scroll">
                    \begin{align}
                    H(Y|X) &= p_a \times (- \frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{2} \log_2 \frac{1}{2}) \\ 
                    &+ p_b \times (- \frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{2} \log_2 \frac{1}{2}) \\
                    &+ p_c \times (- \frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{2} \log_2 \frac{1}{2}) \\
                    &= p_a + p_b + \frac{3}{2} p_c
                    \end{align}
                </div>
                <p>となる。ここで、\(H(Y), H(Y|X)\)に対して\(p_a + p_b + p_c = 1\)を用いると、以下のようになる。</p>
                <div class="scroll">
                    \begin{align}
                    H(Y) = - \frac{p_a - p_b + 1}{4} \log_2 \frac{p_a - p_b + 1}{4} - \frac{p_b - p_a + 1}{4} \log_2 \frac{p_b - p_a + 1}{4} + \frac{1}{2}
                    \end{align}
                </div>
                <div class="scroll">
                    \begin{align}
                    H(Y|X) &= p_a + p_b + \frac{3}{2} p_c \\
                    &= \frac{3}{2} - \frac{p_a + p_b}{2}
                    \end{align}
                </div>
                <p>したがって、\(I(X;Y)\)は以下のようになる。</p>
                <div class="scroll">
                    \begin{align}
                    I(X;Y) &= H(Y) - H(Y|X) \\
                    &= - \frac{p_a - p_b + 1}{4} \log_2 \frac{p_a - p_b + 1}{4} - \frac{p_b - p_a + 1}{4} \log_2 \frac{p_b - p_a + 1}{4} + \frac{p_a + p_b}{2} - 1 \\
                    &= \frac{p_a + p_b}{2}
                    \end{align}
                </div>
                <p>以上より、\(p_c = 0\)のとき、\(p_a + p_b = 1\)となって、\(I(X;Y)\)は最大値0.5となる。</p>
                <p>通信路容量\(C\)は0.5である。</p>
            </div>
            <div class="example">
                <p><b>問題</b></p>
                <p>確率変数 \( X \) と \( Y \) の結合確率分布 \( P(X, Y) \) が次の表で与えられるとき、以下の問いに答えよ。 ただし、\( \log_2 3 = 1.585 \) とし、小数点第3位を四捨五入して答えよ。</p>
                <table>
                    <tr>
                        <th rowspan="2" colspan="2">\( P(X, Y) \)</th>
                        <th colspan="2">\( Y \)</th>
                    </tr>
                    <tr>
                        <th>0</th>
                        <th>1</th>
                    <tr>
                        <td rowspan="2">\( X \)</td>
                        <td>0</td>
                        <td>\( \frac{1}{2} \)</td>
                        <td>\( \frac{1}{4} \)</td>
                    </tr>
                    <tr>
                        <td>1</td>
                        <td>\( \frac{1}{4} \)</td>
                        <td>0</td>
                    </tr>
                </table>   
                <ol>
                    <li>
                        \( X \) と \( Y \) の結合エントロピー \( H(X, Y) \) は何ビットか求めよ。
                    </li>
                    <li>
                        \( X \) のエントロピー \( H(X) \) は何ビットか求めよ。
                    </li>
                    <li>
                        \( X \) で条件づけた \( Y \) の条件付きエントロピー \( H(Y|X) \) は何ビットか求めよ。
                    </li>
                    <li>
                        \( X \) と \( Y \) の相互情報量 \( I(X; Y) \) は何ビットか求めよ。
                    </li>
                </ol>
                <p><b>(1)の解答</b></p>
                <div class="scroll">
                    \begin{align}
                    H(X, Y) = - \frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{4} \log_2 \frac{1}{4} = 1.5
                    \end{align}
                </div>
                <p><b>(2)の解答</b></p>
                <div class="scroll">
                    \begin{align}
                    H(X) = - \frac{3}{4} \log_2 \frac{3}{4} - \frac{1}{4} \log_2 \frac{1}{4} = 0.81125 \simeq 0.81
                    \end{align}
                </div>
                <p><b>(3)の解答</b></p>
                <div class="scroll">
                    \begin{align}
                    H(Y|X) = H(X, Y) - H(X) = 1.5 - 0.81125 = 0.68875 \simeq 0.69
                    \end{align}
                </div>
                <p><b>(4)の解答</b></p>
                <p>\(H(Y) = H(X) = 0.81125\)、\(I(X;Y) = H(Y) - H(Y|X)\)をより、</p>
                <div class="scroll">
                    \begin{align}
                    I(X;Y) = 0.81125 - 0.68875 = 0.1225 \simeq 0.12
                    \end{align}
                </div>
            </div>
            <div class="example">
                <p><b>問題</b></p>
                <ol>
                    <li>
                        \(a\), \(b\), \(c\) の3種類の情報源記号を、それぞれ確率 \(0.1\), \(0.3\), \(0.6\) で発生する記憶のない定常情報源 \(S\) を考える。この情報源 \(S\) から発生する情報源系列について、2情報源記号ごと（\(aa\) や \(bc\) など）にアルファベット \(\{0,1\}\) を用いて2元符号化するブロックハフマン符号を求め、1情報源記号あたりの平均符号語長 \(L\) を求めよ。
                    </li>
                    <li>
                        符号語 \(w = (x_1, x_2, x_3, x_4, x_1 + x_4, x_1 + x_2 + x_3, x_2 + x_3 + x_4)\) となる \((7,4)\) ハミング符号において、系列 1011011 を受信した。単一誤りまでを想定した場合、送信された情報源記号は何であったと推定されるか答えよ。+は2元体上の加算を表す。
                    </li>
                    <li>
                        2元体上の生成多項式が \(G(x) = x^4 + x^2 + 1\) である符号長7の巡回符号において、情報ビット 110 を符号化した後の2元系列を答えよ。
                    </li>
                </ol>
                <p><b>解答</b></p>
            </div>
        </section>
    </main>
</body>
</html>
