<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【院試対策|北海道大学 情報科学院】走る作曲家のAIカフェ～情報理論編～</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
          <p>「北海道大学大学院情報科学院修士課程入学試験」（令和6年8月実施）の「<b>情報理論</b>」対策ページです。</p>
        </section>
        <section>
          <h2>分野別対策</h2>
          <ul>
            <li><a href="gradexam.html">院試対策 ～はじめに～</a></li>
            <li><a href="gradexam_basicmath.html">院試対策 ～基礎数学編～</a></li>
            <li><a href="gradexam_infomath.html">院試対策 ～情報数学編～</a></li>
            <li><a href="gradexam_statistics.html">院試対策 ～確率・統計編～</a></li>
            <li><a href="gradexam_infotheory.html">院試対策 ～情報理論編～</a></li>
            <li><a href="gradexam_algo.html">院試対策 ～アルゴリズムとデータ構造編～</a></li>
            <li><a href="gradexam_ai.html">院試対策 ～人工知能編～</a></li>
            <li><a href="gradexam_cs.html">院試対策 ～コンピュータシステム編～</a></li>
          </ul>
        </section>
        <section id="algorithms-data-structures">
        <h2>情報理論</h2>
        <p>参考にしたサイトは以下のとおり。</p>
        <p><a href="https://www-alg.ist.hokudai.ac.jp/~atsu/info_theory.html">情報理論 (Information Theory).html</a></p>
        <h3>情報理論とは</h3>
            <p><b>情報理論が取り組む4つの課題</b></p>
            <ul>
                <li>できるだけよい情報源符号化法を見出すこと</li>
                <li>情報源符号化の限界を知ること</li>
                <li>できるだけよい通信路符号化法を見出すこと</li>
                <li>通信路符号化の限界を知ること</li>
            </ul>
        <h3>条件付き確率</h3>
            <p><b>条件付き確率</b></p>
            <p>事象\(B\)の条件のもとで事象\(A\)が起こる条件付き確率は</p>
            <div class="scroll">
                \begin{align}
                P(A|B) = \frac{P(A \cap B}{P(B)}
                \end{align}
            </div>
            で定義される。
            <p><b>乗法定理</b></p>
            <div class="scroll">
                \begin{align}
                P(A \cap B) = P(A|B) P(B)
                \end{align}
            </div>
        <h3>情報量とエントロピー(1)</h3>
            <p><b>情報量</b></p>
            <p>確率\(p\)の事象の生起を知った時に得られる<b>情報量</b>を\(I(p)\)とする。</p>
            <p>\(I(p)\)は次のような性質を満たすべき。</p>
            <ul>
                <li>\(I(p)\)は\( 0 < p \leq 1 \)で単調現象な関数である</li>
                <li>確率\(p_1, p_2\)で起こる二つの互いに独立な事象が同時に起こる確率\(p_1, p_2\)について\(I(p_1 p_2) = I(p_1) + I(p_2)\)</li>
                <li>\(I(p)\)は\( 0 < p \leq 1 \)で連続な関数である</li>
            </ul>
            <p>これらを満たす関数\(I(p)\)は\(I(p) = - \log_a p\)という形しかありえない（\(a > 1\)）</p>
            <p>確率\(p\)で生起する事象が起きたことを知ったときに得られる情報量\(I(p)\)を<b>自己情報量</b>と呼び、\(I(p) = - \log_a p\)と定義する。ただし、\(a > 1\)である。</p>
            <p>\(a = 2\)のとき、単位は<b>ビット</b>という。つまり、確率\(\frac{1}{2}\)で生じる結果を知った時の情報量は1 [bit]である。</p>
            <p><b>平均情報量</b></p>
            <p>\(M\)個の互いに排反な事象\(a_1, a_2, \cdots, a_M\)が起こる確率を\(p_1, p_2, \cdots, p_M\)とする（ただし、\(p_1 + p_2 + \cdots + p_M = 1\)）。</p>
            <p>このうち１つの事象が起こったことを知ったときに得る情報量は\(- \log_2 p_i\)であるから、これを平均した期待値\(\overline{I}\)は、</p>
            <div class="scroll">
                \begin{align}
                \overline{I} &= p_1 (- \log_2 p_1) + p_2 (- \log_2 p_12) + \cdots + p_M (- \log_2 p_M) \\
                &= - \sum_{i = 1}^{M} p_i \log_2 p_i
                \end{align}
            </div>
            <p>となる。これを<b>平均情報量</b>（単位はビット）という。</p>
            <p><b>エントロピー</b></p>
            <p>確率変数\(X\)が取り得る値が\(x_1, x_2, \cdots, p_M\)（ただし、\(p_1 + p_2 + \cdots + p_M = 1\)）であるとき、確率変数\(X\)の<b>エントロピー</b>を</p>
            <div class="scroll">
                \begin{align}
                H(X) = - \sum_{i = 1}^{M} p_i \log_2 p_i
                \end{align}
            </div>
            <p>ビットと定義する。</p>
            <p>一般に、確率分布の偏りが非常に大きいときは平均情報量が小さく、確率分布が均一の時に平均情報量が最大になる。</p>
            <p>エントロピーと平均情報量は、完全に同じ形をしている。</p>
            <p>情報を受け取ると曖昧さが減るので、受け取った量だけエントロピーは減って、0に近づく（<b>受け取った情報量 = エントロピーの変化</b>）</p>
            <p><b>エントロピーの性質</b></p>
            <p>\(M\)個の値を取る確率変数\(X\)のエントロピー\(H(X)\)は次の性質を満たす。</p>
            <ol>
                <li>\(0 \leq H(X) \leq \log_2 M\)</li>
                <li>\(H(X)\)が最小値0となるのは、ある値を取る確率が1で、他のM-1個の値を取る確率がすべて0の時に限る。すなわち、\(X\)の取る値がはじめから確定している場合のみである。</li>
                <li>\(H(X)\)が最大値\(\log_2 M\)となるのは、\(M\)個の値がすべて\(\frac{1}{M}\)で等しい場合に限る。</li>
            </ol>
            <p>エントロピーは「答えのあてにくさ」を表す数値ともいえる。</p>
            <p><b>2つ以上の確率変数を扱う場合</b></p>
            <P>２つの確率変数\(X, Y\)を考える。\(X\)は\(x_1, x_2, \cdots, x_{M_X}\)の値をとり、\(Y\)は\(y_1, y_2, \cdots, y_{M_Y}\)の値を取るものとする。</P>
            <p>確率変数の組\((X, Y)\)の値が\((x, y)\)となる<b>結合確率分布</b>を\(P(x, y)\)と書く。</p>
            <p>組\((X, Y)\)をまとめて考えると、４つの値を取る確率変数\(Z\)のエントロピー\(H(Z)\)として考えることができる。</p>
            <p>確率変数\(X\)と\(Y\)の結合エントロピー\(H(X, Y)\)は、</p>
            <div class="scroll">
                \begin{align}
                H(X, Y) = - \sum_{i = 1}^{M_X} \sum_{j = 1}^{M_Y} P(x_i, y_j) \log_2 P(x_i, y_j)
                \end{align}
            </div>
            <p>により定義される。これを<b>結合エントロピー</b>と呼ぶ。ただし、\({x_1, x_2, \cdots, x_{M_X}}\)および\({y_1, y_2, \cdots, y_{M_Y}}\)は、それぞれ\(X\)と\(Y\)が取り得る値の集合とする。</p>
            <p><b>結合エントロピーの性質</b></p>
            <p>確率変数\(X\)と\(Y\)の結合エントロピー\(H(X, Y)\)に対し、</p>
            <div class="scroll">
                \begin{align}
                0 \leq H(X, Y) \leq H(X) + H(Y)
                \end{align}
            </div>
            <p>が成り立つ。また、\(H(X, Y) = H(X) + H(Y)\)となるのは、\(X\)と\(Y\)が<b>独立のときのみ</b>である。</p>
            <p><b>ベイズの定理</b></p>
            <div class="scroll">
                \begin{align}
                P(B|A) = \frac{P(B)P(A|B)}{P(B)P(A|B)+P(\overline{B})P(A|\overline{B})}
                \end{align}
            </div>
        <h3>情報量とエントロピー(2)</h3>
            <p><b>条件付きエントロピー</b></p>
            <p>確率変数\(Y\)で条件を付けた\(X\)の<b>条件付きエントロピー</b>\(H(X|Y)\)は、</p>
            <div class="scroll">
                \begin{align}
                H(X|Y) = - \sum_{j = 1}^{M_Y} P(y_j) \sum_{i = 1}^{M_X} P(x_i|y_j) \log_2 P(x_i|y_j)
                \end{align}
            </div>
            <p>により定義される。ただし、\({x_1, x_2, \cdots, x_{M_X}}\)および\({y_1, y_2, \cdots, y_{M_Y}}\)は、それぞれ\(X\)と\(Y\)が取り得る値の集合とする。</p>
            <p><b>条件付きエントロピーの性質</b></p>
            <p>\({x_1, x_2, \cdots, x_{M_X}}\)および\({y_1, y_2, \cdots, y_{M_Y}}\)を取り得る値の集合とする確率変数\(X\)と\(Y\)に関し、以下が成り立つ。</p>
            <ol>
                <li>
                    <div class="scroll">
                        \begin{align}
                        H(X|Y) = - \sum_{j = 1}^{M_Y} \sum_{i = 1}^{M_X} P(x_i, y_j) \log_2 P(x_i|y_j)
                        \end{align}
                    </div>
                </li>
                <li>
                    <div class="scroll">
                        \begin{align}
                        H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
                        \end{align}
                    </div>
                （この式は、\(H(X)\)と\(H(Y)\)に関するベン図を描くと分かりやすい。）</li>
                <li>\(0 \leq H(X|Y) \leq H(X)\)（\(H(X|Y) = H(X)\)は、\(X\)と\(Y\)が独立の時のみ成立。）</li>
                <li>\(0 \leq H(Y|X) \leq H(Y)\)（\(H(Y|X) = H(Y)\)は、\(X\)と\(Y\)が独立の時のみ成立。）</li>
            </ol>
            <p><b>相互情報量と、その性質</b></p>
            <p>確率変数\(X\)と\(Y\)の<b>相互情報量</b>\(I(X;Y)\)は、</p>
            <div class="scroll">
                \begin{align}
                I(X;Y) = H(X) - H(X|Y)
                \end{align}
            </div>
            <p>によって定義される。</p>
            <p>確率変数\(X\)と\(Y\)と相互情報量\(I(X;Y)\)に関して以下が成り立つ。</p>
            <ol>
                <li>
                    <div class="scroll">
                        \begin{align}
                        I(X;Y) &= H(X) - H(X|Y) \\
                        &= H(Y) - H(Y|X) \\
                        &= H(X) + H(Y) - H(X, Y)
                        \end{align}
                    </div>
                </li>
                <li>\(0 \leq I(X;Y) \leq \min {H(X), H(Y)}\)（\I(X;Y)=0\)は、\(X\)と\(Y\)が独立の時のみ成立。）</li>
            </ol>
            <p><b>シャノンの補助定理</b></p>
            <p>\(p_1, p_2, \cdots, p_M\)および\(q_1, q_2, \cdots, q_M\)を</p>
            <div class="scroll">
                \begin{align}
                p_1 + p_2 + \cdots + p_M = 1 \\
                q_1 + q_2 + \cdots + q_M \leq 1
                \end{align}
            </div>
            <p>を満たす任意の非負の数とする（ただし、\(p_i \neq 0\)のときは\(q_i \neq 0\)とする）。このとき、</p>
            <div class="scroll">
                \begin{align}
                - \sum_{i = 1}^{M} p_i \log_2 q_i \geq - \sum_{i = 1}^{M} p_i \log_2 p_i
                \end{align}
            </div>
            <p>が成立する。等号は\(q_i = p_i (i = 1, 2, \cdots, M)\)のとき、またそのときに限って成立する。</p>
            <p>つまり、確率分布\(P = {p_i}_{i = 1}^{M}\)とちょっと違う分布\(q_i\)（ただし総和が1以下）を持ってきて、\(\log_2\)の内側の\(p_i\)と置き換えると、<b>元より少し大きくなる</b>。</p>
        <h3>情報源のモデル(1)</h3>
            <p><b>離散的\(M\)元情報源</b></p>
            <p>\(M\)個の元からなる記号の有限集合\(A = {a_1, a_2, \cdots, a_M}\)を考える。</p>
            <p>\(A\)を<b>情報源アルファベット</b>、各元\(a_i \in A\)を<b>情報源記号</b>と呼ぶ。</p>
            <p>時点0より毎時点（時点は整数値）で、\(A\)上の情報源記号を、ある確率に従って１個ずつ出力する。</p>
            <p>情報源から出力されるデータ = 情報源記号が並んだ記号列（<b>情報源系列</b>と呼ぶ。）</p>
            <p><b>情報源系列の確率分布</b></p>
            <p>時点0から時点\(n-1\)まで（長さ\(n\)）の情報源系列について、</p>
            <ul>
                <li>時点\(i\)での情報源の出力を、確率変数\(X_i\)で表す。つまり、\(X_i\)は\(a_1, a_2, \cdots, a_M\)のいずれか</li>
                <li>情報源系列は\(X_0 X_1 \cdots X_{n-1}\)</li>
            </ul>
            <p>\(X_0, X_1, \cdots, X_{n-1}\)の<b>結合確率分布</b>:</p>
            <div class="scroll">
                \begin{align}
                P_{X_0 X_1 \cdots X_{n-1}} (x_0, x_1, \cdots, x_{n-1}) = [X_0 = x_0, X_1 = x_1, \cdots, X_{n-1} = x_{n-1} \text{となる確率}]
                \end{align}
            </div>
            <p>\(p(x_0, x_1, \cdots, x_{n-1})\)とも書く。</p>
            <P>結合確率分布から、統計的性質は完全に定まる。</P>
            <p><b>情報源の各種モデル</b></p>
            <p>どんなに大きい\(n\)についても、\(X_0, X_1, \cdots, X_{n-1}\)の結合確率分布を与えることができれば、この情報源の統計的性質は完全に記述できる。しかし、一般には困難。</p>
            <p>議論を進めるためには、情報源に何らかの<b>扱いやすい性質</b>を仮定する必要がある。</p>
            <p><b>記憶のない情報源</b></p>
            <p>各時点における情報源記号の発生が、<b>他の時点と独立</b>である情報源</p>
            <p><b>記憶のない定常情報源</b></p>
            <p>記憶のない情報源において、各時点における情報源記号の発生が<b>同一の確率分布</b>\(P_X (x)\)に従う情報源</p>
            <p>記憶のない定常情報源における長さ\(n\)の系列の結合確率分布は、以下の式で表される。</p>
            <div class="scroll">
                \begin{align}
                P_{X_0 X_1 \cdots X_{n-1}} (x_0, x_1, \cdots, x_{n-1}) = \prod_{i = 0}^{n-1} P_X (x_i)
                \end{align}
            </div>
            <p><b>定常情報源の定義</b></p>
            <p>時間をずらしても、統計的性質の変わらない情報源</p>
            <p>定常情報源の出力は、各時点において同一の確率分布に従う。この確率分布を<b>定常分布</b>と呼ぶ。</p>
            <p><b>エルゴード情報源</b></p>
            <p>十分長い任意の出力系列に、その情報源の統計的性質が完全に現れている定常情報源。</p>
            <p>エルゴード的であれば、十分に長い任意の系列は必ず一定の統計的性質を満たす。</p>
            <p><b>情報源の各種モデル</b></p>
            <p>記憶のない定常情報源は必ずエルゴード情報源になる。</p>
            <p>記憶のある定常情報源でエルゴード的なものはある（例：正規マルコフ情報源）</p>
            <p>定常情報源であってもエルゴード的でないものもある。</p>
            <p>例：確率\(\frac{1}{2}\)で0だけからなる系列か、1だけからなる系列を発生する情報源</p>
            <p>情報理論では、ほとんどの場合、エルゴード情報源を扱う。</p>
            <p><b>マルコフ情報源</b></p>
            <p>マルコフ情報源：記憶のある情報源の基本的なモデル。</p>
            <p><b>\(m\)重マルコフ情報源</b></p>
            <p>系列中の<b>過去\(m\)文字の並びにしたがって、次の文字の生起確率が決まるモデル。</b></p>
            <p><b>\(m\)重マルコフ情報源の状態遷移図</b></p>
            <p>離散的\(q\)元情報源を考える。これが\(m\)重マルコフ情報源だとする。</p>
            <p>\(q^m\)個の<b>状態</b>を持ち、各状態において記号の出力確率が定まっていると見なせる。</p>
            <p>出力を一つ発生する度に\(q^m\)個の状態の中を<b>遷移</b>していく。</p>
            <p><b>一般化されたマルコフ情報源</b></p>
            <p>単に、マルコフ情報源と呼ぶ。</p>
            <p>「直前の\(m\)個の出力に対応する」という条件をなくし、より抽象的に（自由な状態図で）情報源を定義できる。</p>
            <p>マルコフ連鎖モデルともいう。</p>
            <p><b>状態の分類</b></p>
            <p><b>過渡状態</b>：十分時間が経過すれば抜け出てしまい、戻ることのない状態。</p>
            <p><b>閉じた状態集合</b>：任意の状態から任意の状態へ遷移可能な（矢印でたどっていくことのできる）状態の集合。</p>
            <p><b>既約マルコフ情報源</b>：任意の状態から任意の状態へ遷移可能なマルコフ情報源。</p>
            <p><b>非周期的状態集合</b>：閉じた状態集合で、ある時間が経過した後の任意の時点において、どの状態にある確率も0でないようなもの。</p>
            <p><b>周期的状態集合</b>：閉じた状態集合がいくつかの部分集合に分割され、その各々が周期的な時点においてのみ現れ得るもの。</p>
            <p><b>正規マルコフ情報源</b>：非周期的状態集合のモデルで定義されるマルコフ情報源。十分な時間が経過した後に、定常情報源として扱えて有益。</p>
        <h3>情報源のモデル(2)</h3>
            <p><b>遷移確率行列</b></p>
            <p>\(N\)個の状態\(s_0, s_1, \cdots, s_{N-1}\)を持つ<b>正規マルコフ情報源</b>を考える。</p>
            <p>状態遷移の仕方は、状態\(s_i\)にあるとき、次の時点で状態\(s_j\)に遷移する確率\(p_{i,j}=P(s_j|s_i)\)により決まる。これを<b>遷移確率</b>という。</p>
            <p>遷移確率\(p_{i, j}\)を\((i, j)\)要素とする\(N \times N\)行列を<b>遷移確率行列</b>と呼ぶ。</p>
            <p>行が\(i\)側（現在状態）で、列が\(j\)側（次状態）である。</p>
            <p>行ごとに総和が1である。</p>
            <p><b>正規マルコフ情報源の極限分布</b></p>
            <p>正規マルコフ情報源の定義より、\(t > t_0\)となる任意の\(t\)について</p>
            <div class="scroll">
                \begin{align}
                p_{i, j}^{(t)} > 0 \quad (i, j = 0, 1, \cdots, N-1)
                \end{align}
            </div>
            <p>が成り立つようなある正定数\(t_0\)が存在する。</p>
            <p>正規マルコフ情報源では、\(t \rightarrow \infty\)とするとき、\(p_{i, j}^{(t)}\)は\(i\)には無関係な値に収束する。すなわち、</p>
            <div class="scroll">
                \begin{align}
                \lim_{t \rightarrow \infty} p_{i, j}^{(t)} = u_j \quad (j = 0, 1, \cdots, N-1)
                \end{align}
            </div>
            <p>となる\(u_j\)が存在する。</p>
            <p>遷移行列を用いて上式を書き直すと、</p>
            <div class="scroll">
                \begin{align}
                \lim_{t \rightarrow \infty} \Pi = U \quad
                \end{align}
            </div>
            <p>となる。ここで、\(U\)はすべての行が\(\mathbf{u} = (u_0, u_1, \cdots, u_{N-1})\)となる\(N \times N\)行列である。</p>
            <p>時点\(t\)において状態\(s_j\)にいる確率\(P(s^{(t)} = s_j)\)を\(w_{j}^{(t)}\)で表し、</p>
            <div class="scroll">
                \begin{align}
                \mathbf{w}_t = (w_{0}^{(t)}, w_{1}^{(t)}, \cdots, w_{N-1}^{(t)})
                \end{align}
            </div>
            <p>という\(N\)次元ベクトルを定義する。</p>
            <p>時点\(t-1\)で状態\(s_i\)にいる確率が\(w_{i}^{t-1}\)で、状態\(s_i\)にいるときに次の時点で状態\(s_j\)へと遷移する確率が\(p_{i, j}\)であるから、</p>
            <div class="scroll">
                \begin{align}
                w_{j}^{(t)} &= \sum_{i = 0}^{N-1} w_{i}^{t-1} p_{i, j} \\
                \mathbf{w}_t &= \mathbf{w}_{t-1} \Pi
                \end{align}
            </div>
            <p>これを繰り返せば\(\mathbf{w}_{t} = \mathbf{w}_{0} \Pi^{t}\)を得る。ここで、\(\mathbf{w}_0\)は時点0における状態分布（<b>初期分布</b>）である。</p>
            <p>\(mathbf{w}_t\)の\(t \rightarrow \infty\)とした極限を<b>極限分布</b>と呼ぶ。</p>
            <p>正規マルコフ情報源では、次式が成り立つ。</p>
            <div class="scroll">
                \begin{align}
                \lim_{t \rightarrow \infty} \mathbf{w}_t = \mathbf{w}_0 \lim_{t \rightarrow \infty} \Pi^t = \mathbf{w}_0 U = \mathbf{u}
                \end{align}
            </div>
            <p>十分時間が経過すれば、初期分布がどうであれ、状態分布は定常的な確率分布（定常分布）に落ち着く</p>
            <p>正規マルコフ情報源が落ち着く定常分布を</p>
            <div class="scroll">
                \begin{align}
                \mathbf{w} = (w_0, w_1, \cdots, w_{N-1})とする。
                \end{align}
            </div>
            <p>とする。\(w_i\)は確率なので、当然ながら</p>
            <div class="scroll">
                \begin{align}
                w_0 + w_1 + \cdots + w_{N-1} = 1
                \end{align}
            </div>
            <p>が成り立つ。</p>
            <p>ある時点の状態分布が定常的で\(\mathbf{w}\)であるとすれば、次の時点の状態分布も\(\mathbf{w}\)でなければならないので、\(\mathbf{w}\)は</p>
            <div class="scroll">
                \begin{align}
                \mathbf{w} \Pi = \mathbf{w}
                \end{align}
            </div>
            <p>を満たさなければならない。</p>
            <p>正規マルコフ情報源の遷移確率行列\(\Pi\)に対しては、この式を満たす\(\mathbf{w}\)が唯一存在し、極限分布と一致する。</p>
            <p><b>情報源の1次エントロピー</b></p>
            <p>次のような\(M\)元定常情報源\(S\)を考える。</p>
            <ul>
                <li>情報源アルファベット\(A = {a_1, a_2, \cdots, a_M}\)</li>
                <li>記号\(a_k\)の生起確率\(p_k\)</li>
            </ul>
            <p>このとき、</p>
            <div class="scroll">
                \begin{align}
                H_1 (S) = - \sum_{k = 1}^{M} p_k \log_2 p_k
                \end{align}
            </div>
            <p>を情報源\(S\)の<b>1次エントロピー</b>と呼ぶ。</p>
            <p>記憶のない定常情報源の場合、その1次エントロピーは、情報源からの出力を1個受け取ったときに得られる平均的な情報量と考えることができる。</p>
            <p><b>情報源の\(n\)次エントロピー</b></p>
            <p>長さ\(n\)の系列を考え、その系列全体のエントロピーから1記号あたりのエントロピーを算出することを考える。</p>
            <p>\(M\)元情報源の\(n\)個の出力をまとめて1個の記号とみなすと、\(M^n\)元の情報源とみなせる。</p>
            <p>これを<b>\(n\)次拡大情報源</b>といい、\(S^n\)で表す。</p>
            <p>このとき、</p>
            <div class="scroll">
                \begin{align}
                H_n (S) = \frac{H_1 (S^n)}{n}
                \end{align}
            </div>
            <p>を情報源\(S\)の<b>\(n\)次エントロピー</b>と呼ぶ。</p>
            <p>記憶のない定常情報源の場合、その\(n\)次エントロピーは、1次エントロピーと一致する。</p>
            <p><b>情報源のエントロピー</b></p>
            <p>情報源\(S\)の\(n\)次エントロピーの極限</p>
            <div class="scroll">
                \begin{align}
                H(s) = \lim_{n \rightarrow \infty} H_n (s) = \lim_{n \rightarrow \infty} \frac{H_1 (S^n)}{n}
                \end{align}
            </div>
            <p>を情報源\(S\)の<b>エントロピー</b>と呼ぶ。</p>
            <p>これは、十分長い時間をかけて系列を観測した時の1記号当たりの平均情報量と考えることができる。</p>
            <p>記憶のない定常情報源の場合、そのエントロピーは1次エントロピーと一致する。</p>
            <p><b>正規マルコフ情報源のエントロピー</b></p>
            <p>次のような正規マルコフ情報源\(S\)を考える。</p>
            <ul>
                <li>情報源アルファベット \(a_1, a_2, \cdots, a_M\)</li>
                <li>\(N\)個の状態\(s_0, s_1, \cdots, s_{N-1}\)</li>
                <li>定常分布\((w_0, w_1, \cdots, w_{N-1})\)</li>
                <li>状態\(s_i\)にあるときに記号\(a_k\)を発生する確率\(P(a_k | s_i)\)</li>
            </ul>
            <p>この情報源\(S\)に対するエントロピー\(H(S)\)は</p>
            <div class="scroll">
                \begin{align}
                H(s) = - \sum_{i = 0}^{N-1} w_i (\sum_{k = 1}^{M} P(a_k | s_i) \log_2 P(a_k | s_i))
                \end{align}
            </div>
            <p>である。</p>
            <p>結局、正規マルコフ情報源\(S\)のエントロピー\(H(S)\)の求め方は以下の通りである。</p>
            <ol>
                <li>定常分布を求める</li>
                <li>各状態\(s_i\)において、記憶のない情報源とみなし、その場合のエントロピー\(H_{s_i} (S)\)を求める</li>
                <li>上で求めたエントロピー\(H_{s_i} (S)\)を、定常分布にしたがった割合で合算する</li>
            </ol>
        <h3>情報源符号化とその限界</h3>
            <p><b>平均符号長とは</b></p>
            <p>情報源符号化の目的の1つは、できるだけ「良い」情報源符号化法を設計することである。</p>
            <p>われわれの目的は、通信路をできるだけ効率よく圧縮することであるから、同じ情報系列を平均として短い符号列系に符号化できれば、「良い」符号化となる。</p>
            <p>そこで、言い換えると、1情報源あたり符号系列の長さの平均値をできるだけ小さくしたい。これを<b>平均符号長</b>と呼ぶ。</p>
            <p><b>平均符号長の定義</b></p>
            <p>情報源アルファベットが\(\Sigma = {a_1, a_2, \cdots, a_M}\)で、発生確率が\(P(a_i) = p_i (i = 1, 2, \cdots, M)\)で与えられる記憶のない情報源\(S\)を考える。</p>
            <p>符号\(C\)によって、情報源記号\(a_1, a_2, \cdots, a_M\)のそれぞれに対応付けられた符号語の長さを\(l_1, l_2, \cdots, l_M\)とする。このとき、1情報源記号当たりの<b>平均符号長</b>は、</p>
            <div class="scroll">
                \begin{align}
                L &= p_1 l_1 + \cdots + p_M l_M \\
                &= \sum_{i = 1}^{M} p_i l_i
                \end{align}
            </div>
            <p>で与えられる。</p>
            <p><b>瞬時符号とは</b></p>
            <p>符号化は、一意に復号できても、前から読んで戻せるとは限らない。そこで、符号化は、一意復号可能かどうかだけでなく、前から読んだときに、そのまま後戻りせずに復号可能だと便利である。</p>
            <p>符号\(C\)が<b>瞬時符号</b>であるとは、符号化を前から読んでいったときに、ある符号語のパターンが現れたら、それを直ちに復号できるときをいう。そうでない符号を<b>非瞬時符号</b>という。</p>
            <p><b>瞬時符号の条件</b></p>
            <p>ある符号語\(x\)が別の符号語\(y\)の頭の部分のパターンと一致するとき、\(x\)は\(y\)の<b>語頭</b>という。</p>
            <p>瞬時符号であるためには、どの符号語も他の符号語の語頭であってはならない。これを<b>語頭条件</b>と呼ぶ。</p>
            <p><b>符号の木と瞬時符号の関係</b></p>
            <p>瞬時符号は、符号語がすべて葉に対応付けられている。非瞬時符号は、葉以外の節点にも対応付けられている。</p>
            <p><b>クラフトの不等式</b></p>
            <p>長さが\(l_1, l_2, \cdots, l_M\)となる\(M\)個の符号語を持つ\(q\)元符号で瞬時符号となるものが存在するための必要十分条件は、</p>
            <div class="scroll">
                \begin{align}
                q^{-l_1} + q^{-l_2} + \cdots + q^{-l_M} \leq 1 
                \end{align}
            </div>
            <p>が満たされることである。</p>
            <p>※「存在する」としか言っていないことに注意。「この不等式を満たすから瞬時符号」とは言えない。</p>
            <p>※実は、一意復号可能である必要十分条件も、この式を満たすことであり、<b>マクラミンの不等式</b>と呼ぶ。</p>
            <p><b>平均符号長\(L\)の限界に関する定理その１</b></p>
            <p>定常分布を持つ情報源\(S\)の各情報源記号を一意復号可能な\(r\)元符号に符号化したとき、その平均符号長\(L\)は</p>
            <div class="scroll">
                \begin{align}
                \frac{H_1 (S)}{\log_2 r} \leq L
                \end{align}
            </div>
            <p>を満たす。また、平均符号長\(L\)が</p>
            <div class="scroll">
                \begin{align}
                L < \frac{H_1 (S)}{\log_2 r} + 1
                \end{align}
            </div>
            <p>となる\(r\)元瞬時符号を作ることができる。</p>
            <p><b>平均符号長\(L\)の限界に関する定理その２</b></p>
            <p>情報源\(S\)は、任意の一意復号可能な\(r\)元符号で符号化する場合、その平均符号長\(L\)は、</p>
            <div class="scroll">
                \begin{align}
                \frac{H (S)}{\log_2 r} \leq L
                \end{align}
            </div>
            <p>を満たす。また、任意の正数\(\epsilon > 0\)について平均符号長\(L\)が</p>
            <div class="scroll">
                \begin{align}
                L < \frac{H (S)}{\log_2 r} + \epsilon
                \end{align}
            </div>
            <p>となる\(r\)元瞬時符号を作ることができる。</p>
            <p>つまり、どんなに工夫しても、平均符号長\(L\)はエントロピー\(H(S)\)までしか改善できない。（2元符号の場合、\(\log_2 r = 1\)）</p>
        <h3>情報源符号化法(1)</h3>
            <p><b>ハフマン符号はなぜ大事か？</b></p>
            <p>ハフマン符号は<b>コンパクト符号</b>である。</p>
            <p>コンパクト符号とは、1記号ずつ符号化する際、その平均符号長を最小とする効率の良い符号のこと。</p>
            <p><b>ハフマン符号の作り方</b></p>
            <p><b>基本アイディア</b>：最も確率が低い2つの情報源記号を繰り返し選んでまとめながら、下から順に符号木を作る。</p>
            <p>具体的な手順は以下のとおり。</p>
            <ol>
                <li>まず、確率の高い順に記号を並び替える。</li>
                <li>各記号に対応する符号木の葉を作る。葉には確率を添えてかいておく。</li>
                <li>最も確率が小さい葉を2つ選び、それを集約するためのノードを新たに作って枝で結ぶ。そのノードを新しい葉として扱い、元の二つの葉の確率を足し合わせたものを添える。</li>
                <li>3. を繰り返して符号木を作る。</li>
                <li>各ノードから葉へ向かう方向の2本の枝に、0と1のラベルを割り当てる（割り当て方は任意で良い）。</li>
            </ol>
        </section>
    </main>
</body>
</html>
