<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【RSNA 2024 | Kaggle】走る作曲家のAIカフェ</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1050827580219099"
     crossorigin="anonymous"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
            <h2>RSNA 2024 Lumbar Spine Degenerative Classification</h2>
            <p><a href="https://www.kaggle.com/competitions/rsna-2024-lumbar-spine-degenerative-classification">RSNA 2024 Lumbar Spine Degenerative Classification</a></p>
        </section>
        <section>
          <h2>概要</h2>
          <p>このコンペティションの目的は、腰椎のMRI画像を用いて、変性疾患を診断・分類するAIモデルを開発することである。
              参加者は、放射線科医が行う診断と同様の精度で、腰椎の変性疾患を検出・分類できるモデルを作成する。</p>
          <h3>病態</h3>
          <p>腰痛は世界中で最も一般的な障害の原因であり、世界保健機関（WHO）によると、2020年には6億1900万人が腰痛に苦しんでいた。
              腰痛は加齢と共に発生頻度が高まり、多くの人が一度は経験する。
              腰痛は、椎間板の変性や脊椎管の狭窄など、神経の圧迫や炎症を引き起こす変性疾患によって引き起こされることがある。
              MRIは、腰椎や椎間板、神経の詳細な画像を提供し、これらの変性疾患の有無や重症度を評価することができる。
              適切な診断と分類は、治療や手術の指針となり、患者の痛みや生活の質の向上に貢献する。</p>
          <h3>コンペティションの焦点</h3>
          <p>このコンペティションでは、以下の5つの腰椎変性疾患を分類することを目指す：</p>
            <ul>
                <li>左神経孔狭窄</li>
                <li>右神経孔狭窄</li>
                <li>左関節包狭窄</li>
                <li>右関節包狭窄</li>
                <li>脊椎管狭窄</li>
            </ul>
            <p>それぞれの椎間板レベル（L1/L2、L2/L3、L3/L4、L4/L5、L5/S1）において、各疾患の重症度（正常/軽度、中等度、重度）がラベル付けされたデータが提供される。</p>
            <h3>評価方法</h3>
            <p>評価は、サンプル重み付きロス関数の平均と、重度の脊椎疾患を検出する予測モデルを組み合わせたカスタムメトリックで行われる。</p>
            <p>サンプルの重みは以下の通り：</p>
            <ul>
                <li>正常/軽度：1</li>
                <li>中等度：2</li>
                <li>重度：4</li>
            </ul>
        </section>
      <section>
        <h2>データ</h2>
        <h3>データセットの目的</h3>
        <p>このデータセットは、診断ラベル付きの画像とメタデータで構成されている。
          主な画像フォーマットはJPEGで、関連するCSVファイルには、バイナリの診断ラベル（ターゲット）や、年齢や性別、解剖学的な位置などの入力変数が含まれている。</p>
        <b>予測タスク</b>
        <p>このチャレンジでは、悪性の皮膚病変と良性の病変を識別する。
          各画像（isic_id）に対して、悪性の可能性（ターゲット）を0から1の範囲で割り当てる。</p>
        <b>SLICE-3Dデータセット</b>
        <p>3D全身写真（TBP）から抽出された皮膚病変画像を使用して、非ダーモスコピー画像に類似したデータセットが提供される。
          Canfield ScientificのVectra WB360という3D TBPシステムが全身の皮膚表面をマクロ品質の解像度で撮影し、その画像からAIソフトウェアが個々の病変を特定する。
          このデータセットには、2015年から2024年にかけて9つの機関で撮影された数千人の患者のすべての病変が含まれている。</p>
        <b>提供ファイル</b>
        <ul>
          <li>train-image/: トレーニングセットの画像ファイル</li>
          <li>train-image.hdf5: HDF5形式のトレーニング画像データ</li>
          <li>train-metadata.csv: トレーニングセットのメタデータ</li>
          <li>test-image.hdf5: テスト画像データ</li>
          <li>test-metadata.csv: テストサブセットのメタデータ</li>
          <li>sample_submission.csv: 提出ファイルのサンプル</li>
        </ul>
        <b>メタデータのフィールド</b>
        <ul>
          <li>target: バイナリクラス（0: 良性、1: 悪性）</li>
          <li>isic_id: 画像のユニークID</li>
          <li>patient_id: 患者のユニークID</li>
          <li>age_approx: 撮影時の推定年齢</li>
          <li>sex: 患者の性別</li>
          <li>anatom_site_general: 病変の位置</li>
          <li>その他、病変の面積や境界の不規則性、色の対比などを表すフィールドも含まれている。</li>
        </ul>
      </section>
      <section>
        <h2>解法</h2>
        <h3>自分の解法</h3>
        <b>大まかな流れ</b>
        <ol>
          <li>ImageNetで事前学習されたEfficientNetをISICデータセットでファインチューニング</li>
          <li>テスト画像をEfficientNetに入力して、メラノーマである確率を算出し、その結果を特徴量としてデータフレームに追加</li>
          <li>LightGBM, CatBoost, XGBoostのアンサンブルモデルで予測</li>
        </ol>
        <b>工夫</b>
        <ul>
          <li>不均衡データへの対処： RandomOverSampler と RandomUnderSampler を使ってクラスバランスを調整した</li>
          <li>アンサンブルモデルの構築： VotingClassifier を使用して Soft Votingした</li>
          <li>ノイズの追加：最終的な予測結果にノイズを追加することでモデルの過信を防いだ</li>
        </ul>
        <h3><a href="https://www.kaggle.com/competitions/isic-2024-challenge/discussion/532704">2位(+23)の解法</a></h3>
        <b>全体的なアプローチ</b>
        <ol>
          <li>画像モデルの特徴量を表形式データに組み込む</li>
          <li>複数のGBDT（Gradient Boosting Decision Trees）を使用して推論を行う</li>
        </ol>
        <b>GBDTモデル</b>
        <p>アルゴリズムとアンサンブル戦略</p>
        <ul>
            <li>使用したアルゴリズム: LightGBM、XGBoost、CatBoost</li>
            <li>モデル数: 各アルゴリズムで18種類のバリエーションを作成し、合計54モデルを構築</li>
            <li>シード平均化: データ全体で訓練したモデルを用いてシード平均化（n=5）を実施</li>
        </ul>
        <b>特徴量エンジニアリング</b>
        <p>基本的な特徴量として、以下の公開ノートブックからの手法を採用</p>
        <ul>
            <li><a href="https://www.kaggle.com/code/snnclsr/lgbm-baseline-with-new-features/notebook">LGBM Baseline with New Features</a></li>
            <li><a href="https://www.kaggle.com/code/vyacheslavbolotin/lightgbm-catboost-with-new-features/notebook#Feature-Engineering">LightGBM + Catboost with new features</a></li>
            <li><a href="https://www.kaggle.com/code/richolson/isic-2024-lgbm-imagenet-v5a">ISIC 2024 LGBM+ImageNet v5a</a></li>
        </ul>
        <p>さらに、データの異なる側面を捉えるために、患者関連の特徴量をいくつか作成</p>
        <ul>
            <li>患者ごとの標準化</li>
            <li>患者とtbp_lv_locationによる標準化</li>
            <li>患者とtbp_lv_location_simpleによる標準化</li>
            <li>患者とanatom_site_generalによる標準化</li>
            <li><a href="https://www.kaggle.com/competitions/isic-2024-challenge/discussion/530183">Tabular Ugly Ducklingsテクニック</a>の実装</li>
        </ul>
        <p>多様性を持たせるため、一部のモデルではこれらの特徴量の一部のみを使用</p>
        <b>画像特徴量の統合</b>
        <ul>
            <li>使用した画像特徴量の数: モデルごとに0〜3個の画像特徴量をメタ特徴量として使用</li>
            <li>多様性の向上: 特徴量の使用数を変えることでモデルの多様性を向上</li>
        </ul>
        <b>ハイパーパラメータのチューニング</b>
        <ul>
            <li>ブーストラウンド数: 200〜300に設定</li>
            <li>チューニングの組み合わせ:</li>
            <ul>
                <li>使用する画像特徴量の数</li>
                <li>使用する患者特徴量の数</li>
            </ul>
        </ul>
        <b>モデルの多様性</b>
        <ul>
            <li>特徴量の微調整: モデルごとに使用する表形式の特徴量を微妙に変更</li>
            <li>画像特徴量のバリエーション: 画像特徴量の数を変えることで、多様性のあるアンサンブルを実現</li>
        </ul>
        <b>画像モデル</b>
        <b>概要</b>
        <p>5つの異なるトレーニング設定で合計9つの画像モデルを作成。
            具体的には、補助損失を導入して表形式データの予測を行い、自己教師あり学習を実装して精度を向上。
            また、フォールド間の分散が低いモデルを選択することで、安定したパフォーマンスを目指した。</p>
        <b>トレーニング設定</b>
        <ul>
            <li>標準的なトレーニング: 基本的な設定でモデルを訓練</li>
            <li>Mixup拡張: データ拡張としてMixupを追加</li>
            <li>表形式データの補助損失: 複数のモダリティからの学習を促進</li>
            <li>iddx_fullクラスタの補助損失: tf-idfでベクトル化し、k-meansでクラスタリング。各データポイントからクラスタ重心までの距離を補助損失として予測</li>
            <li>自己教師あり事前学習（表形式データ使用）: <a href="https://arxiv.org/abs/2407.07582">最新のマルチモーダル学習の論文</a>に基づき、自己教師あり事前学習を実施</li>
        </ul>
        <b>画像モデルの詳細とCVスコア</b>
        <table>
          <thead>
            <tr>
              <th>モデル</th>
              <th>トレーニング設定</th>
              <th>CVスコア</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>eva02_small</td>
              <td>標準的なトレーニング</td>
              <td>0.1537</td>
            </tr>
            <tr>
              <td>deit3_small</td>
              <td>標準的なトレーニング</td>
              <td>0.1534</td>
            </tr>
            <tr>
              <td>beitv2_base</td>
              <td>Mixup拡張</td>
              <td>0.1594</td>
            </tr>
            <tr>
              <td>convnextv2_tiny</td>
              <td>表形式データの補助損失</td>
              <td>0.1548</td>
            </tr>
            <tr>
              <td>swinv2_small</td>
              <td>iddx_fullクラスターの補助損失</td>
              <td>0.1612</td>
            </tr>
            <tr>
              <td>eva02_small</td>
              <td>iddx_fullクラスターの補助損失</td>
              <td>0.1580</td>
            </tr>
            <tr>
              <td>resnet50</td>
              <td>iddx_fullクラスターの補助損失</td>
              <td>0.1515</td>
            </tr>
            <tr>
              <td>convnextv2_nano</td>
              <td>自己教師あり事前学習 (表形式データ使用)</td>
              <td>0.1607</td>
            </tr>
            <tr>
              <td>swin_tiny</td>
              <td>自己教師あり事前学習 (表形式データ使用)</td>
              <td>0.1596</td>
            </tr>
          </tbody>
        </table>
        <b>共通のトレーニング設定</b>
        <ul>
            <li>アンダーサンプリング: 各エポックで1:3または1:5の比率で実施</li>
            <li>エポック数: 50〜200エポックで訓練し、早期停止は使用せず</li>
            <li>データ拡張: <a href="https://www.kaggle.com/competitions/siim-isic-melanoma-classification/discussion/175412">ISIC 2020のトップソリューション</a>を参考に調整。モデルに応じて強度を変更</li>
            <li>オプティマイザ: AdamWを使用し、バックボーンの学習率を1e-5〜8e-6、ヘッドの学習率を1e-3に設定。ウォームアップとコサインスケジューラを使用</li>
        </ul>
        <b>推論</b>
        <ul>
            <li>全データでの訓練: モデルはデータ全体で訓練し、推論に使用</li>
            <li>高速化: Automatic Mixed Precision (AMP)を有効化</li>
            <ul>
                <li>AMP：通常float32で計算するところをfloat16で計算して高速化する技術</li>
            </ul>
        </ul>
        <b>クロスバリデーション戦略</b>
        <p>モデルの評価と選択のためにこのCV戦略の結果に大きく依存した。
            今回のコンペでは、以前のKaggleコンペで使用されたアプローチに触発された「Triple Stratified Leak-Free KFold CV」戦略を実装した。
            この方法は、データリークを防ぎつつ、堅牢なモデル検証を可能にする。</p>
        <b>戦略の主なポイント</b>
        <ul>
            <li>患者の分離: 単一の患者からのすべての画像は同じフォールドに保持し、クロスバリデーション中のリークを防止</li>
            <li>悪性画像のバランス: 層化は各患者の悪性画像の割合を考慮</li>
            <li>患者の画像数の分布: 患者は画像数に基づいてビン分けされ、層化に使用</li>
        </ul>
        <p>これらすべてを同時に実装するために、5フォールドのStratified Group KFoldクロスバリデーションを使用した。</p>
        <p>元のアイディア：<a href="https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/165526">riple Stratified Leak-Free KFold CV</a></p>
        <b>うまくいかなかったこと</b>
        <p>過去のコンペティションのデータを組み込もうと試みた。
            トーンを合わせるためにヒストグラムマッチングなどの手法を適用したが、精度の向上は見られなかった。
            検証のため、ISIC2018のデータと現在のデータを混合し、過去データと現在データを区別する画像モデルを構築した。
            その結果、モデルは容易にAUC 0.99を達成した。
            これらの結果から、視覚的には識別できない明確な違いが存在すると判断し、過去のデータの使用を断念した。</p>
        <b>コード：</b>
          <p><a href="https://github.com/uchiyama33/isic-2024-2nd-place">https://github.com/uchiyama33/isic-2024-2nd-place</a></p>
      </section>
    </main>
</body>
</html>
