<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【自然言語処理 ～言語処理100本ノック～】走る作曲家のAIカフェ</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1050827580219099"
     crossorigin="anonymous"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
      <section>
        <h2>言語処理100本ノック</h2>
        <p><a href="https://nlp100.github.io/ja/">言語処理100本ノック</a>に挑戦します。</p>
        <p>使用言語はPythonです。</p>
        <p><a href="https://github.com/abigworld1/abigworld1.github.io/tree/main/nlp_100">コード一覧（Jupyter Notebook）</a></p>
      </section>
      <section>
            <h2>目次</h2>
            <ul>
                <li><a href="#01">第1章：準備運動</a></li>
                <li><a href="#02">第2章：UNIXコマンド</a></li>
                <li><a href="#03">第3章：正規表現</a></li>
                <li><a href="#04">第4章：形態素解析</a></li>
                <li><a href="#05">第5章：係り受け解析</a></li>
                <li><a href="#06">第6章：機械学習</a></li>
                <li><a href="#07">第7章：単語ベクトル</a></li>
                <li><a href="#08">第8章：ニューラルネット</a></li>
                <li><a href="#09">第9章：RNNとCNN</a></li>
                <li><a href="#10">第10章：機械翻訳</a></li>
            </ul>
        </section>
      <section id="01">
        <h2>第1章：準備運動</h2>
        <h3>00.文字列の逆順</h3>
        <pre><code>
# 文字列の定義
text = "stressed"

# 文字列を逆順に並べ替える
reversed_text = text[::-1]

# 結果を出力
print(reversed_text)
        </code></pre>
        <p><code2>text[start:end:step]</code2>というスライスの一般的な形式で、<code2>start</code2>と<code2>end</code2>を省略し、<code2>step</code2>に<code2>-1</code2>を指定することで、逆順に文字を取得できる。</p>
        <h3>01.「パタトクカシーー」</h3>
        <pre><code>
# 文字列の定義
text = "パタトクカシーー"

# 1, 3, 5, 7文字目を取り出して連結
result = text[0] + text[2] + text[4] + text[6]

# 結果を出力
print(result)
        </code></pre>
        <h3>02.「パトカー」＋「タクシー」＝「パタトクカシーー」</h3>
        <pre><code>
# 文字列の定義
text1 = &quot;パトカー&quot;
text2 = &quot;タクシー&quot;

# 交互に文字を取り出して連結
result = &#039;&#039;.join([a + b for a, b in zip(text1, text2)])

# 結果を出力
print(result)
        </code></pre>
          <p>Pythonの<code2>zip</code2>関数を使うことで、2つの文字列を並列に処理できる。</p>
          <p><code2>zip(text1, text2)</code2>は、<code2>text1</code2>と<code2>text2</code2>のそれぞれ対応する位置の文字をペアにする。</p>
          <p>リスト内包表記を用いて、<code2>a + b</code2>という形で交互に文字を連結する。</p>
          <p><code2>''.join([...])</code2>で、リスト内の連結された文字を1つの文字列にまとめる。</p>
          <h3>03.円周率</h3>
        <pre><code>
# 与えられた文章
sentence = &quot;Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.&quot;

# 単語に分解（句読点を除去）
words = sentence.replace(&quot;,&quot;, &quot;&quot;).replace(&quot;.&quot;, &quot;&quot;).split()

# 各単語の文字数を計算
word_lengths = [len(word) for word in words]

# 結果を出力
print(word_lengths)
        </code></pre>
          <p><code2>.replace(",", "")</code2>と<code2>.replace(".", "")</code2>を使って、カンマやピリオドなどの句読点を取り除く。これにより、単語だけを扱えるようにする。</p>
          <p><code2>.split()</code2>を使うことで、文章を単語ごとに分割し、リスト<code2>words</code2>に格納する。</p>
          <p>リスト内包表記を使って、<code2>[len(word) for word in words]</code2>で各単語の文字数を計算し、その結果を<code2>word_lengths</code2>に格納する。</p>
          <h3>04.元素記号</h3>
        <pre><code>
# 与えられた文章
sentence = &quot;Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.&quot;

# 句読点を除去して単語に分解
words = sentence.replace(&quot;.&quot;, &quot;&quot;).split()

# 1文字だけを取り出す単語のインデックス（1から始まる）
one_char_indices = [1, 5, 6, 7, 8, 9, 15, 16, 19]

# 結果を格納する辞書
word_map = {}

# 各単語を処理して辞書に格納
for i, word in enumerate(words, 1):  # 単語の位置が1から始まるようにenumerateを使う
    if i in one_char_indices:
        word_map[word[:1]] = i  # 1文字だけを取り出す
    else:
        word_map[word[:2]] = i  # 2文字を取り出す

# 結果を出力
print(word_map)
        </code></pre>
        <h3>05.n-gram</h3>
        <pre><code>
# n-gramを作成する関数
def n_gram(sequence, n):
    return [sequence[i:i+n] for i in range(len(sequence) - n + 1)]

# 与えられた文章
sentence = &quot;I am an NLPer&quot;

# 単語bi-gram
words = sentence.split()  # 文を単語に分解
word_bi_gram = n_gram(words, 2)  # 単語でのbi-gramを生成

# 文字bi-gram
char_bi_gram = n_gram(sentence.replace(&quot; &quot;, &quot;&quot;), 2)  # 空白を除去して文字でのbi-gramを生成

# 結果を出力
print(&quot;単語bi-gram:&quot;, word_bi_gram)
print(&quot;文字bi-gram:&quot;, char_bi_gram)
        </code></pre>
          <h3>06.集合</h3>
        <pre><code>
# n-gramを作成する関数
def n_gram(sequence, n):
    return [sequence[i:i+n] for i in range(len(sequence) - n + 1)]

# 文字列からbi-gramを作成
str1 = &quot;paraparaparadise&quot;
str2 = &quot;paragraph&quot;

# それぞれの文字bi-gram集合を求める
X = set(n_gram(str1, 2))
Y = set(n_gram(str2, 2))

# 和集合、積集合、差集合を求める
union = X | Y
intersection = X &amp; Y
difference = X - Y

# &#039;se&#039;がXとYに含まれるかどうかを調べる
is_se_in_X = &#039;se&#039; in X
is_se_in_Y = &#039;se&#039; in Y

# 結果を出力
print(&quot;X:&quot;, X)
print(&quot;Y:&quot;, Y)
print(&quot;和集合:&quot;, union)
print(&quot;積集合:&quot;, intersection)
print(&quot;差集合:&quot;, difference)
print(&quot;&#039;se&#039;がXに含まれるか:&quot;, is_se_in_X)
print(&quot;&#039;se&#039;がYに含まれるか:&quot;, is_se_in_Y)
        </code></pre>
          <h3>07.テンプレートによる文生成</h3>
        <pre><code>
# 関数の定義
def create_sentence(x, y, z):
    return f&quot;{x}時の{y}は{z}&quot;

# 関数の実行
x = 12
y = &quot;気温&quot;
z = 22.4
result = create_sentence(x, y, z)

# 結果を出力
print(result)
        </code></pre>
          <h3>08.暗号文</h3>
        <pre><code>
# cipher関数の定義
def cipher(text):
    # 各文字を処理
    result = &#039;&#039;.join([chr(219 - ord(c)) if c.islower() else c for c in text])
    return result

# 暗号化するメッセージ
message = &quot;I am a NLPer.&quot;

# 暗号化
encrypted_message = cipher(message)
print(&quot;暗号化:&quot;, encrypted_message)

# 復号化 (同じ関数を使うことで復号も可能)
decrypted_message = cipher(encrypted_message)
print(&quot;復号化:&quot;, decrypted_message)
        </code></pre>
          <h3>09.Typoglycemia</h3>
        <pre><code>
import random

# 単語をシャッフルする関数
def shuffle_middle(word):
    if len(word) &lt;= 4:  # 長さが4以下の単語は並び替えない
        return word
    middle = list(word[1:-1])  # 先頭と末尾以外の文字を取り出す
    random.shuffle(middle)  # 先頭と末尾以外の文字をシャッフル
    return word[0] + &#039;&#039;.join(middle) + word[-1]  # 先頭、シャッフルした文字、末尾を結合して返す

# 文章全体を処理する関数
def process_sentence(sentence):
    words = sentence.split()  # スペースで分割して単語ごとにリスト化
    shuffled_words = [shuffle_middle(word) for word in words]  # 各単語をシャッフル処理
    return &#039; &#039;.join(shuffled_words)  # 単語を再びスペースで結合して返す

# 適当な英語の文
sentence = &quot;I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .&quot;

# 実行
result = process_sentence(sentence)
print(result)
        </code></pre>
      </section>
        <section id="02">
            <h2>第2章：UNIXコマンド</h2>
            <h3>10. 行数のカウント</h3>
            <p>Pythonプログラム</p>
            <pre><code>
# popular-names.txtの行数をカウントするプログラム
def count_lines(file_path):
    with open(file_path, &#039;r&#039;) as file:
        lines = file.readlines()  # ファイルをすべて読み込んで行ごとにリスト化
        return len(lines)  # 行数を返す

# popular-names.txtのパス
file_path = &#039;popular-names.txt&#039;

# 行数をカウントして表示
line_count = count_lines(file_path)
print(f&quot;行数: {line_count}&quot;)
            </code></pre>
            <p>UNIXコマンド</p>
            <pre><code>
wc -l popular-names.txt
            </code></pre>
            <h3>11. タブをスペースに置換</h3>
            <p>Pythonプログラム</p>
            <pre><code>
# タブ文字をスペースに置換するプログラム
def replace_tabs_with_spaces(file_path, output_file_path):
    with open(file_path, &#039;r&#039;) as file:
        content = file.read()  # ファイル全体を読み込む
        modified_content = content.replace(&#039;\t&#039;, &#039; &#039;)  # タブをスペースに置換
    
    with open(output_file_path, &#039;w&#039;) as output_file:
        output_file.write(modified_content)  # 置換後の内容を書き込む

# ファイルパスの指定
input_file_path = &#039;popular-names.txt&#039;
output_file_path = &#039;popular-names-space.txt&#039;

# 置換処理の実行
replace_tabs_with_spaces(input_file_path, output_file_path)
print(f&quot;&#039;{input_file_path}&#039;のタブをスペースに置換し、&#039;{output_file_path}&#039;に保存しました。&quot;)
            </code></pre>
            <p>UNIXコマンド</p>
            <ul>
                <li>sed</li>
                <pre><code>
sed &#039;s/\t/ /g&#039; popular-names.txt &gt; popular-names-space.txt
                </code></pre>
                <li>tr</li>
                <pre><code>
tr &#039;\t&#039; &#039; &#039; &lt; popular-names.txt &gt; popular-names-space.txt
                </code></pre>
                <li>expand</li>
                <pre><code>
expand -t 1 popular-names.txt &gt; popular-names-space.txt
                </code></pre>
            </ul>
            <h3>12. 1列目をcol1.txtに，2列目をcol2.txtに保存</h3>
            <p>Pythonプログラム</p>
            <pre><code>
# 1列目と2列目を別ファイルに保存するプログラム
def extract_columns(input_file, col1_file, col2_file):
    with open(input_file, &#039;r&#039;) as file, open(col1_file, &#039;w&#039;) as col1, open(col2_file, &#039;w&#039;) as col2:
        for line in file:
            columns = line.split(&#039;\t&#039;)  # タブ区切りで列を分割
            if len(columns) &gt;= 2:
                col1.write(columns[0] + &#039;\n&#039;)  # 1列目を書き込み
                col2.write(columns[1] + &#039;\n&#039;)  # 2列目を書き込み

# 入力ファイルと出力ファイルのパス
input_file_path = &#039;popular-names.txt&#039;
col1_output_path = &#039;col1.txt&#039;
col2_output_path = &#039;col2.txt&#039;

# 列を抜き出してファイルに保存
extract_columns(input_file_path, col1_output_path, col2_output_path)
print(f&quot;1列目を &#039;{col1_output_path}&#039; に、2列目を &#039;{col2_output_path}&#039; に保存しました。&quot;)
            </code></pre>
            <p>UNIXコマンド</p>
            <pre><code>
# 1列目を col1.txt に保存
cut -f 1 popular-names.txt &gt; col1.txt

# 2列目を col2.txt に保存
cut -f 2 popular-names.txt &gt; col2.txt
            </code></pre>
            <h3>13. col1.txtとcol2.txtをマージ</h3>
            <p>Pythonプログラム</p>
            <pre><code>
# col1.txtとcol2.txtを結合して、タブ区切りで新しいファイルに書き込むプログラム
def merge_columns(col1_file, col2_file, output_file):
    with open(col1_file, &#039;r&#039;) as col1, open(col2_file, &#039;r&#039;) as col2, open(output_file, &#039;w&#039;) as output:
        for col1_line, col2_line in zip(col1, col2):
            output.write(f&quot;{col1_line.strip()}\t{col2_line.strip()}\n&quot;)  # タブで結合して書き込む

# ファイルパスの指定
col1_file_path = &#039;col1.txt&#039;
col2_file_path = &#039;col2.txt&#039;
output_file_path = &#039;merged.txt&#039;

# 結合処理の実行
merge_columns(col1_file_path, col2_file_path, output_file_path)
print(f&quot;&#039;{col1_file_path}&#039; と &#039;{col2_file_path}&#039; を結合し、&#039;{output_file_path}&#039; に保存しました。&quot;)
            </code></pre>
            <p>UNIXコマンド</p>
            <pre><code>
paste col1.txt col2.txt &gt; merged.txt
            </code></pre>
            <h3>14. 先頭からN行を出力</h3>
            <p>Pythonプログラム</p>
            <pre><code>
# 以下の14.pyファイルを作成し、python 14.py popular-names.txt 5で実行

import sys

# 先頭N行を表示するプログラム
def head(file_path, N):
    with open(file_path, &#039;r&#039;) as file:
        for i, line in enumerate(file):
            if i &gt;= N:
                break
            print(line.strip())

# コマンドライン引数の処理
if __name__ == &quot;__main__&quot;:
    if len(sys.argv) != 3:
        print(&quot;Usage: python script.py &lt;file_path&gt; &lt;N&gt;&quot;)
    else:
        file_path = sys.argv[1]  # ファイルのパス
        N = int(sys.argv[2])     # 表示する行数
        head(file_path, N)
            </code></pre>
            <p>UNIXコマンド</p>
            <pre><code>
head -n 5 popular-names.txt
            </code></pre>
            <h3>15. 末尾のN行を出力</h3>
            <p>Pythonプログラム</p>
            <pre><code>
# 以下の15.pyファイルを作成し、python 15.py popular-names.txt 5で実行

import sys

# 末尾N行を表示するプログラム
def tail(file_path, N):
    with open(file_path, &#039;r&#039;) as file:
        lines = file.readlines()  # 全ての行をリストとして読み込む
        for line in lines[-N:]:   # 末尾からN行だけスライスして表示
            print(line.strip())

# コマンドライン引数の処理
if __name__ == &quot;__main__&quot;:
    if len(sys.argv) != 3:
        print(&quot;Usage: python script.py &lt;file_path&gt; &lt;N&gt;&quot;)
    else:
        file_path = sys.argv[1]  # ファイルのパス
        N = int(sys.argv[2])     # 表示する行数
        tail(file_path, N)
            </code></pre>
            <p>UNIXコマンド</p>
            <pre><code>
tail -n 5 popular-names.txt
            </code></pre>
            <h3>16. ファイルをN分割する</h3>
            <p>Pythonプログラム</p>
            <pre><code>
# 以下の16.pyファイルを作成し、python 16.py popular-names.txt 5で実行

import sys
import math

# ファイルを行単位でN分割するプログラム
def split_file(file_path, N):
    with open(file_path, &#039;r&#039;) as file:
        lines = file.readlines()  # ファイルの全行をリストに読み込む
        total_lines = len(lines)
        lines_per_file = math.ceil(total_lines / N)  # 各ファイルに含まれる行数を計算

        # ファイルを分割して書き込み
        for i in range(N):
            start = i * lines_per_file
            end = min(start + lines_per_file, total_lines)
            with open(f&quot;split_{i+1}.txt&quot;, &#039;w&#039;) as output_file:
                output_file.writelines(lines[start:end])
            print(f&quot;split_{i+1}.txtに行数 {start+1} から {end} を書き込みました。&quot;)

# コマンドライン引数の処理
if __name__ == &quot;__main__&quot;:
    if len(sys.argv) != 3:
        print(&quot;Usage: python script.py &lt;file_path&gt; &lt;N&gt;&quot;)
    else:
        file_path = sys.argv[1]  # ファイルのパス
        N = int(sys.argv[2])     # 分割数
        split_file(file_path, N)
            </code></pre>
            <p>UNIXコマンド</p>
            <pre><code>
split -l $(( $(wc -l &lt; popular-names.txt) / 5 )) popular-names.txt split_
            </code></pre>
            <h3>17. １列目の文字列の異なり</h3>
            <p>Pythonプログラム</p>
            <pre><code>
def unique_first_column(file_path):
    with open(file_path, &#039;r&#039;) as file:
        first_column = {line.split(&#039;\t&#039;)[0] for line in file}  # 1列目をセットに格納
    return first_column

# ファイルパスの指定
file_path = &#039;popular-names.txt&#039;

# 1列目の異なる文字列を取得
unique_names = unique_first_column(file_path)

# 結果を出力
for name in sorted(unique_names):
    print(name)
            </code></pre>
            <p>セット<code2>{}</code2>は重複を自動的に除去するため、異なる一列目の文字列のみが取得される。</p>
            <p>UNIXコマンド</p>
            <pre><code>
cut -f 1 popular-names.txt | sort | uniq
            </code></pre>
            <h3>18. 各行を3コラム目の数値の降順にソート</h3>
            <p>Pythonプログラム</p>
            <pre><code>
def sort_by_third_column(file_path, output_file):
    with open(file_path, &#039;r&#039;) as file:
        lines = file.readlines()

    # 3列目の数値を基準に行をソート（逆順）
    sorted_lines = sorted(lines, key=lambda line: int(line.split(&#039;\t&#039;)[2]), reverse=True)

    # 結果を出力
    with open(output_file, &#039;w&#039;) as output:
        output.writelines(sorted_lines)

# ファイルパスの指定
input_file_path = &#039;popular-names.txt&#039;
output_file_path = &#039;sorted_by_third_column.txt&#039;

# 3列目の数値でソートして新しいファイルに保存
sort_by_third_column(input_file_path, output_file_path)
print(f&quot;3列目を基準に逆順でソートし、&#039;{output_file_path}&#039; に保存しました。&quot;)
            </code></pre>
            <p>UNIXコマンド</p>
            <pre><code>
sort -k 3,3 -n -r popular-names.txt &gt; sorted_by_third_column.txt
            </code></pre>
            <h3>19. 各行の1コラム目の文字列の出現頻度を求め，出現頻度の高い順に並べる</h3>
            <p>Pythonプログラム</p>
            <pre><code>
from collections import Counter

# 1列目の出現頻度を求め、頻度順にソートするプログラム
def count_first_column(file_path):
    with open(file_path, &#039;r&#039;) as file:
        first_column = [line.split(&#039;\t&#039;)[0] for line in file]  # 1列目の文字列をリストに格納

    # 出現頻度をカウント
    counts = Counter(first_column)

    # 出現頻度の高い順にソートして表示
    sorted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)

    for name, count in sorted_counts:
        print(f&quot;{name}: {count}&quot;)

# ファイルパスの指定
file_path = &#039;popular-names.txt&#039;

# 出現頻度をカウントして表示
count_first_column(file_path)
            </code></pre>
            <p>UNIXコマンド</p>
            <pre><code>
cut -f 1 popular-names.txt | sort | uniq -c | sort -nr
            </code></pre>
        </section>
        <section id="03">
        <h2>第3章：正規表現</h2>
        <h3>20. JSONデータの読み込み</h3>
        <pre><code>
import json
import re
import gzip

# 圧縮されたJSONファイルから、Wikipedia記事のリストを読み込む
text_list = []
with gzip.open(&#039;jawiki-country.json.gz&#039;, &#039;rt&#039;, encoding=&#039;utf-8&#039;) as f:
    lines = f.readlines()  # すべての行を読み込む
    for line in lines:
        # 各行がJSON形式なので、json.loadsでPythonの辞書形式に変換
        text_list.append(json.loads(line))

# 「イギリス」の記事本文を探す
UK_text = None  # 「イギリス」の記事を格納するための変数
for article in text_list:
    if article[&#039;title&#039;] == &quot;イギリス&quot;:
        UK_text = article[&#039;text&#039;]  # 記事本文のみを抽出
        break  # 一度見つかったらループを終了

# 「イギリス」の記事本文を表示（デバッグ用）
if UK_text:
    print(UK_text)
else:
    print(&quot;記事が見つかりませんでした。&quot;)
        </code></pre>
        <p><code2>rt</code2>は、ファイルをテキストモードで開くということ。</p>
        <p><code2>json.loads()</code2>を使用して辞書型に変換している。</p>
        <p><code2>article["title"] == "イギリス"</code2>で、タイトルが「イギリス」の記事を見つけ、該当する記事の本文<code2>article["text"]</code2>を<code2>UK_textに格納。</code2></p>
        <h3>21. カテゴリ名を含む行を抽出</h3>
        <pre><code>
import re

# 正規表現パターンを定義
# \[\[Category:.*?\]\] は、[[Category:カテゴリ名]] の形式にマッチするパターン
pattern = r&quot;\[\[Category:.*?\]\]&quot;  # 生文字列として正規表現を記述

# UK_textに含まれるすべてのカテゴリ行を正規表現で抽出
result = re.findall(pattern, UK_text)

# 結果を表示
for line in result:
    print(line)
        </code></pre>
        <p>正規表現のパターンをraw文字列（先頭に<code2>r</code2>をつける）として定義。これにより、Pythonのエスケープ処理を避け、正規表現のパターンをそのまま記述できる。</p>
        <p><code2>re.findall()</code2>を使って、<code2>UK_text</code2>内の<code2>[[Category:...]]</code2>形式にマッチする全ての部分を抽出する。<code2>findall</code2>は一致する部分全てをリストとして返す。</p>
        <h3>22. カテゴリ名の抽出</h3>
        <pre><code>
import re

# カテゴリ名のみを抽出する関数
def extract_category_names(article_text):
    # 正規表現パターン: [[Category:カテゴリ名]] もしくは [[Category:カテゴリ名|...]] にマッチ
    pattern = r&#039;\[\[Category:(.*?)(\|.*)?\]\]&#039;
    
    # findallでマッチしたカテゴリ名をリストで抽出
    category_names = re.findall(pattern, article_text)
    
    # マッチした結果はタプル (カテゴリ名, オプション部分) なので、カテゴリ名だけを取り出す
    category_names = [match[0] for match in category_names]
    
    return category_names

# UK_textに含まれるカテゴリ名を抽出
category_names = extract_category_names(UK_text)

# 結果を表示
for name in category_names:
    print(name)
        </code></pre>
        <h3>23. セクション構造</h3>
        <pre><code>
import re

# セクション名とそのレベルを抽出する関数
def extract_sections(article_text):
    # 正規表現パターン: セクション名とそのレベルにマッチ
    pattern = r&#039;(={2,})\s*(.+?)\s*\1&#039;
    
    # findallでマッチしたセクション名とレベルをリストで抽出
    sections = re.findall(pattern, article_text)
    
    # セクション名とそのレベルを表示
    for section in sections:
        level = len(section[0]) - 1  # &#039;=&#039; の数でセクションレベルを決定
        section_name = section[1].strip()  # セクション名の前後の空白を除去
        print(f&quot;{section_name} : {level}&quot;)

# 「イギリス」の記事本文からセクション構造を抽出して表示
extract_sections(UK_text)
        </code></pre>
        <h3>24. ファイル参照の抽出</h3>
        <pre><code>
import re

# ファイル参照を抽出する関数
def extract_media_files(article_text):
    # 正規表現パターン: [[File:ファイル名]] または [[ファイル:ファイル名]] にマッチ
    pattern = r&#039;\[\[File:(.+?)(\|.*)?\]\]|\[\[ファイル:(.+?)(\|.*)?\]\]&#039;
    
    # findallでマッチしたファイル名をリストで抽出
    media_files = re.findall(pattern, article_text)
    
    # マッチした結果のうち、ファイル名だけを取り出す
    media_files = [match[0] or match[2] for match in media_files]
    
    return media_files

# 「イギリス」の記事本文からメディアファイルを抽出
media_files = extract_media_files(UK_text)

# 結果を表示
for file in media_files:
    print(file)
        </code></pre>
        <h3>25. テンプレートの抽出</h3>
        <pre><code>
import re

# 基礎情報テンプレートのフィールド名と値を辞書形式で抽出する関数
def extract_basic_info(article_text):
    # 基礎情報テンプレート全体を抽出する正規表現パターン
    basic_info_pattern = r&#039;{{基礎情報.*?\n(.*?)\n}}&#039;
    
    # 基礎情報テンプレートを本文から抽出
    basic_info_match = re.search(basic_info_pattern, article_text, re.DOTALL)
    
    # 基礎情報テンプレートが存在しない場合は空の辞書を返す
    if not basic_info_match:
        return {}

    # 基礎情報の内容部分（フィールド名と値のペア）を取得
    basic_info_content = basic_info_match.group(1)

    # フィールド名と値を抽出するための正規表現パターン
    field_pattern = r&#039;\|(?P&lt;field_name&gt;[^=]+)\s*=\s*(?P&lt;value&gt;.+?)(?=\n\||\n$)&#039;

    # フィールド名と値を辞書に格納
    fields = re.findall(field_pattern, basic_info_content, re.DOTALL)
    
    # 辞書にフィールド名と値を整形して格納
    basic_info_dict = {field_name.strip(): value.strip() for field_name, value in fields}
    
    return basic_info_dict

# 「イギリス」の記事本文から基礎情報テンプレートを抽出して辞書形式で取得
basic_info = extract_basic_info(UK_text)

# 結果を表示
for field, value in basic_info.items():
    print(f&quot;{field}: {value}&quot;)
        </code></pre>
        <p><code2>re.DOTALL</code2>フラグを指定して、改行を含むすべての文字にマッチ。</p>
        <p>フィールド名と値の余分な空白を削除するため、<code2>strip()</code2>を使用する。</p>
        <h3>26. 強調マークアップの除去</h3>
        <pre><code>
import re

# 基礎情報テンプレートのフィールド名と値を辞書形式で抽出し、強調マークアップを削除
def extract_basic_info(article_text):
    # 基礎情報テンプレート全体を抽出する正規表現パターン
    basic_info_pattern = r&#039;{{基礎情報.*?\n(.*?)\n}}&#039;
    
    # 基礎情報テンプレートを本文から抽出
    basic_info_match = re.search(basic_info_pattern, article_text, re.DOTALL)
    
    # 基礎情報テンプレートが存在しない場合は空の辞書を返す
    if not basic_info_match:
        return {}

    # 基礎情報の内容部分（フィールド名と値のペア）を取得
    basic_info_content = basic_info_match.group(1)

    # フィールド名と値を抽出するための正規表現パターン
    field_pattern = r&#039;\|(?P&lt;field_name&gt;[^=]+)\s*=\s*(?P&lt;value&gt;.+?)(?=\n\||\n$)&#039;

    # フィールド名と値を辞書に格納
    fields = re.findall(field_pattern, basic_info_content, re.DOTALL)
    
    # 辞書にフィールド名と値を整形して格納
    basic_info_dict = {}
    for field_name, value in fields:
        # 強調マークアップを削除
        clean_value = remove_emphasis_markup(value.strip())
        basic_info_dict[field_name.strip()] = clean_value
    
    return basic_info_dict

# 強調マークアップ（&#039;&#039;や&#039;&#039;&#039;）を除去する関数
def remove_emphasis_markup(text):
    # 弱い強調（&#039;&#039;）、強調（&#039;&#039;&#039;）、強い強調（&#039;&#039;&#039;&#039;&#039;）を削除する正規表現
    clean_text = re.sub(r&quot;&#039;&#039;&#039;&#039;&#039;(.*?)&#039;&#039;&#039;&#039;&#039;&quot;, r&#039;\1&#039;, text)  # 5つの&#039;の強い強調
    clean_text = re.sub(r&quot;&#039;&#039;&#039;(.*?)&#039;&#039;&#039;&quot;, r&#039;\1&#039;, clean_text)  # 3つの&#039;の強調
    clean_text = re.sub(r&quot;&#039;&#039;(.*?)&#039;&#039;&quot;, r&#039;\1&#039;, clean_text)  # 2つの&#039;の弱い強調
    return clean_text

# 「イギリス」の記事本文から基礎情報テンプレートを抽出して辞書形式で取得
basic_info = extract_basic_info(UK_text)

# 結果を表示
for field, value in basic_info.items():
    print(f&quot;{field}: {value}&quot;)
        </code></pre>
        <p><code2>re.sub()</code2>を使用して、指定したパターンにマッチする文字列を削除し、テキストに変換する。</p>
        <h3>27. 内部リンクの除去</h3>
        <pre><code>
import re

# 基礎情報テンプレートのフィールド名と値を辞書形式で抽出し、強調マークアップと内部リンクを削除
def extract_basic_info(article_text):
    # 基礎情報テンプレート全体を抽出する正規表現パターン
    basic_info_pattern = r'{{基礎情報.*?\n(.*?)\n}}'
    
    # 基礎情報テンプレートを本文から抽出
    basic_info_match = re.search(basic_info_pattern, article_text, re.DOTALL)
    
    # 基礎情報テンプレートが存在しない場合は空の辞書を返す
    if not basic_info_match:
        return {}

    # 基礎情報の内容部分（フィールド名と値のペア）を取得
    basic_info_content = basic_info_match.group(1)

    # フィールド名と値を抽出するための正規表現パターン
    field_pattern = r'\|(?P<field_name>[^=]+)\s*=\s*(?P<value>.+?)(?=\n\||\n$)'

    # フィールド名と値を辞書に格納
    fields = re.findall(field_pattern, basic_info_content, re.DOTALL)
    
    # 辞書にフィールド名と値を整形して格納
    basic_info_dict = {}
    for field_name, value in fields:
        # 強調マークアップと内部リンクを削除
        clean_value = remove_internal_links(remove_emphasis_markup(value.strip()))
        basic_info_dict[field_name.strip()] = clean_value
    
    return basic_info_dict

# 強調マークアップ（''や'''）を除去する関数
def remove_emphasis_markup(text):
    # 弱い強調（''）、強調（'''）、強い強調（'''''）を削除する正規表現
    clean_text = re.sub(r"'''''(.*?)'''''", r'\1', text)  # 5つの'の強い強調
    clean_text = re.sub(r"'''(.*?)'''", r'\1', clean_text)  # 3つの'の強調
    clean_text = re.sub(r"''(.*?)''", r'\1', clean_text)  # 2つの'の弱い強調
    return clean_text

# 内部リンクマークアップ ([[記事名]] や [[記事名|表示名]]) を削除する関数
def remove_internal_links(text):
    # [[記事名|表示名]] の場合は表示名を取得し、[[記事名]] の場合は記事名だけを取得
    clean_text = re.sub(r'\[\[([^|\]]+?\|)?(.+?)\]\]', r'\2', text)
    return clean_text

# 「イギリス」の記事本文から基礎情報テンプレートを抽出して辞書形式で取得
basic_info = extract_basic_info(UK_text)

# 結果を表示
for field, value in basic_info.items():
    print(f"{field}: {value}")
        </code></pre>
        <h3>28. MediaWikiマークアップの除去</h3>
        <pre><code>
import re

# 基礎情報テンプレートのフィールド名と値を辞書形式で抽出し、MediaWikiマークアップを削除
def extract_basic_info(article_text):
    # 基礎情報テンプレート全体を抽出する正規表現パターン
    basic_info_pattern = r&#039;{{基礎情報.*?\n(.*?)\n}}&#039;
    
    # 基礎情報テンプレートを本文から抽出
    basic_info_match = re.search(basic_info_pattern, article_text, re.DOTALL)
    
    # 基礎情報テンプレートが存在しない場合は空の辞書を返す
    if not basic_info_match:
        return {}

    # 基礎情報の内容部分（フィールド名と値のペア）を取得
    basic_info_content = basic_info_match.group(1)

    # フィールド名と値を抽出するための正規表現パターン
    field_pattern = r&#039;\|(?P&lt;field_name&gt;[^=]+)\s*=\s*(?P&lt;value&gt;.+?)(?=\n\||\n$)&#039;

    # フィールド名と値を辞書に格納
    fields = re.findall(field_pattern, basic_info_content, re.DOTALL)
    
    # 辞書にフィールド名と値を整形して格納
    basic_info_dict = {}
    for field_name, value in fields:
        # 強調マークアップ、内部リンク、HTMLタグ、MediaWikiタグを削除
        clean_value = remove_mediawiki_markup(value.strip())
        basic_info_dict[field_name.strip()] = clean_value
    
    return basic_info_dict

# MediaWikiマークアップ（強調、リンク、言語テンプレート、HTMLタグなど）を削除する関数
def remove_mediawiki_markup(text):
    # 1. 強調マークアップの削除
    text = remove_emphasis_markup(text)
    
    # 2. 内部リンクの削除
    text = remove_internal_links(text)
    
    # 3. 外部リンクの削除（[http://example.com 表示テキスト] → 表示テキスト）
    text = re.sub(r&#039;\[http[^\s]*\s?([^\]]*)\]&#039;, r&#039;\1&#039;, text)
    
    # 4. 言語テンプレートの削除（{{lang|en|text}} → text）
    text = re.sub(r&#039;{{lang\|[^\|]+\|([^}]+)}}&#039;, r&#039;\1&#039;, text)
    
    # 5. &lt;ref&gt;タグやHTMLコメントを削除
    text = re.sub(r&#039;&lt;ref[^&gt;]*&gt;.*?&lt;/ref&gt;&#039;, &#039;&#039;, text)  # &lt;ref&gt;タグとその内容を削除
    text = re.sub(r&#039;&lt;!--.*?--&gt;&#039;, &#039;&#039;, text)  # HTMLコメントを削除
    
    # 6. &lt;br&gt;タグや他のHTMLタグの削除
    text = re.sub(r&#039;&lt;br\s*/?&gt;&#039;, &#039;&#039;, text)  # &lt;br&gt;タグの削除
    text = re.sub(r&#039;&lt;[\/\!]*?[^&lt;&gt;]*?&gt;&#039;, &#039;&#039;, text)  # その他のHTMLタグを削除
    
    # 7. ファイル参照の削除（[[ファイル:ファイル名|説明]] → 説明）
    text = re.sub(r&#039;\[\[ファイル:[^\|]+\|([^\]]+)\]\]&#039;, r&#039;\1&#039;, text)
    
    return text

# 強調マークアップ（&#039;&#039;や&#039;&#039;&#039;）を除去する関数
def remove_emphasis_markup(text):
    # 弱い強調（&#039;&#039;）、強調（&#039;&#039;&#039;）、強い強調（&#039;&#039;&#039;&#039;&#039;）を削除する正規表現
    clean_text = re.sub(r&quot;&#039;&#039;&#039;&#039;&#039;(.*?)&#039;&#039;&#039;&#039;&#039;&quot;, r&#039;\1&#039;, text)  # 5つの&#039;の強い強調
    clean_text = re.sub(r&quot;&#039;&#039;&#039;(.*?)&#039;&#039;&#039;&quot;, r&#039;\1&#039;, clean_text)  # 3つの&#039;の強調
    clean_text = re.sub(r&quot;&#039;&#039;(.*?)&#039;&#039;&quot;, r&#039;\1&#039;, clean_text)  # 2つの&#039;の弱い強調
    return clean_text

# 内部リンクマークアップ ([[記事名]] や [[記事名|表示名]]) を除去する関数
def remove_internal_links(text):
    # [[記事名|表示名]] の場合は表示名を取得し、[[記事名]] の場合は記事名だけを取得
    clean_text = re.sub(r&#039;\[\[([^|\]]+?\|)?(.+?)\]\]&#039;, r&#039;\2&#039;, text)
    return clean_text

# 「イギリス」の記事本文から基礎情報テンプレートを抽出して辞書形式で取得
basic_info = extract_basic_info(UK_text)

# 結果を表示
for field, value in basic_info.items():
    print(f&quot;{field}: {value}&quot;)
        </code></pre>
        <h3>29. 国旗画像のURLを取得する</h3>
        <pre><code>
import requests
import re

# 基礎情報テンプレートのフィールド名と値を辞書形式で抽出し、MediaWikiマークアップを削除
def extract_basic_info(article_text):
    # 基礎情報テンプレート全体を抽出する正規表現パターン
    basic_info_pattern = r&#039;{{基礎情報.*?\n(.*?)\n}}&#039;
    
    # 基礎情報テンプレートを本文から抽出
    basic_info_match = re.search(basic_info_pattern, article_text, re.DOTALL)
    
    # 基礎情報テンプレートが存在しない場合は空の辞書を返す
    if not basic_info_match:
        return {}

    # 基礎情報の内容部分（フィールド名と値のペア）を取得
    basic_info_content = basic_info_match.group(1)

    # フィールド名と値を抽出するための正規表現パターン
    field_pattern = r&#039;\|(?P&lt;field_name&gt;[^=]+)\s*=\s*(?P&lt;value&gt;.+?)(?=\n\||\n$)&#039;

    # フィールド名と値を辞書に格納
    fields = re.findall(field_pattern, basic_info_content, re.DOTALL)
    
    # 辞書にフィールド名と値を整形して格納
    basic_info_dict = {}
    for field_name, value in fields:
        # 強調マークアップ、内部リンク、HTMLタグ、MediaWikiタグを削除
        clean_value = remove_mediawiki_markup(value.strip())
        basic_info_dict[field_name.strip()] = clean_value
    
    return basic_info_dict

# MediaWikiマークアップ（強調、リンク、言語テンプレート、HTMLタグなど）を削除する関数
def remove_mediawiki_markup(text):
    # 1. 強調マークアップの削除
    text = remove_emphasis_markup(text)
    
    # 2. 内部リンクの削除
    text = remove_internal_links(text)
    
    # 3. 外部リンクの削除（[http://example.com 表示テキスト] → 表示テキスト）
    text = re.sub(r&#039;\[http[^\s]*\s?([^\]]*)\]&#039;, r&#039;\1&#039;, text)
    
    # 4. 言語テンプレートの削除（{{lang|en|text}} → text）
    text = re.sub(r&#039;{{lang\|[^\|]+\|([^}]+)}}&#039;, r&#039;\1&#039;, text)
    
    # 5. &lt;ref&gt;タグやHTMLコメントを削除
    text = re.sub(r&#039;&lt;ref[^&gt;]*&gt;.*?&lt;/ref&gt;&#039;, &#039;&#039;, text)  # &lt;ref&gt;タグとその内容を削除
    text = re.sub(r&#039;&lt;!--.*?--&gt;&#039;, &#039;&#039;, text)  # HTMLコメントを削除
    
    # 6. &lt;br&gt;タグや他のHTMLタグの削除
    text = re.sub(r&#039;&lt;br\s*/?&gt;&#039;, &#039;&#039;, text)  # &lt;br&gt;タグの削除
    text = re.sub(r&#039;&lt;[\/\!]*?[^&lt;&gt;]*?&gt;&#039;, &#039;&#039;, text)  # その他のHTMLタグを削除
    
    # 7. ファイル参照の削除（[[ファイル:ファイル名|説明]] → 説明）
    text = re.sub(r&#039;\[\[ファイル:[^\|]+\|([^\]]+)\]\]&#039;, r&#039;\1&#039;, text)
    
    return text

# 強調マークアップ（&#039;&#039;や&#039;&#039;&#039;）を除去する関数
def remove_emphasis_markup(text):
    # 弱い強調（&#039;&#039;）、強調（&#039;&#039;&#039;）、強い強調（&#039;&#039;&#039;&#039;&#039;）を削除する正規表現
    clean_text = re.sub(r&quot;&#039;&#039;&#039;&#039;&#039;(.*?)&#039;&#039;&#039;&#039;&#039;&quot;, r&#039;\1&#039;, text)  # 5つの&#039;の強い強調
    clean_text = re.sub(r&quot;&#039;&#039;&#039;(.*?)&#039;&#039;&#039;&quot;, r&#039;\1&#039;, clean_text)  # 3つの&#039;の強調
    clean_text = re.sub(r&quot;&#039;&#039;(.*?)&#039;&#039;&quot;, r&#039;\1&#039;, clean_text)  # 2つの&#039;の弱い強調
    return clean_text

# 内部リンクマークアップ ([[記事名]] や [[記事名|表示名]]) を除去する関数
def remove_internal_links(text):
    # [[記事名|表示名]] の場合は表示名を取得し、[[記事名]] の場合は記事名だけを取得
    clean_text = re.sub(r&#039;\[\[([^|\]]+?\|)?(.+?)\]\]&#039;, r&#039;\2&#039;, text)
    return clean_text

# Wikipedia APIを使って国旗画像のURLを取得する関数
def get_flag_image_url(flag_image_name):
    # ファイル名の前に &quot;File:&quot; を追加
    file_name = f&quot;File:{flag_image_name}&quot;
    
    # WikipediaのAPIエンドポイント
    endpoint = &quot;https://en.wikipedia.org/w/api.php&quot;
    
    # APIリクエストのパラメータ
    params = {
        &quot;action&quot;: &quot;query&quot;,
        &quot;titles&quot;: file_name,
        &quot;prop&quot;: &quot;imageinfo&quot;,
        &quot;iiprop&quot;: &quot;url&quot;,
        &quot;format&quot;: &quot;json&quot;
    }
    
    # APIリクエストを送信
    response = requests.get(endpoint, params=params)
    
    # レスポンスのJSONデータを取得
    data = response.json()
    
    # 画像URLを抽出
    pages = data[&#039;query&#039;][&#039;pages&#039;]
    for page_id in pages:
        if &#039;imageinfo&#039; in pages[page_id]:
            return pages[page_id][&#039;imageinfo&#039;][0][&#039;url&#039;]
    
    return None

# 「イギリス」の記事本文から基礎情報テンプレートを抽出
basic_info = extract_basic_info(UK_text)

# 国旗画像ファイル名を取得
flag_image_name = basic_info.get(&#039;国旗画像&#039;, None)

# 国旗画像ファイル名が存在する場合、URLを取得
if flag_image_name:
    flag_image_url = get_flag_image_url(flag_image_name)
    if flag_image_url:
        print(f&quot;国旗画像のURL: {flag_image_url}&quot;)
    else:
        print(&quot;国旗画像のURLが取得できませんでした。&quot;)
else:
    print(&quot;国旗画像のファイル名が見つかりませんでした。&quot;)
        </code></pre>
        </section>
        <section id="04">
        <h2>第4章：形態素解析</h2>
        <h3>準備</h3>
        <p>ターミナルで以下のコマンドを打つ。</p>
        <pre><code>
wget https://nlp100.github.io/data/neko.txt
apt install mecab libmecab-dev mecab-ipadic-utf8
mecab -o ./neko.txt.mecab ./neko.txt
wc -l ./neko.txt.mecab # 行数を確認する
head -15 ./neko.txt.mecab # 先頭15行を確認する
        </code></pre>    
        <h3>30. 形態素解析結果の読み込み</h3>
        <pre><code>
filename = &#039;./neko.txt.mecab&#039;  # 形態素解析結果が保存されているファイル

sentences = []  # 文全体の形態素リストを格納するリスト
morphs = []  # 1文ごとの形態素リストを格納するリスト

# ファイルを読み込み、1行ずつ解析結果を処理
with open(filename, mode=&#039;r&#039;) as f:
    for line in f:
        if line != &#039;EOS\n&#039;:  # 文末を示す &#039;EOS&#039; でない場合
            fields = line.split(&#039;\t&#039;)  # 行をタブで分割（表層形とその他の情報に分ける）
            
            # 行が正しく分割されない場合や空行の場合はスキップ
            if len(fields) != 2 or fields[0] == &#039;&#039;:
                continue  # 次の行へスキップ
            
            # 形態素の情報（品詞など）をカンマで分割
            attr = fields[1].split(&#039;,&#039;)
            
            # 形態素情報を辞書型に格納
            morph = {
                &#039;surface&#039;: fields[0],  # 表層形（単語そのもの）
                &#039;base&#039;: attr[6],       # 基本形（辞書形）
                &#039;pos&#039;: attr[0],        # 品詞
                &#039;pos1&#039;: attr[1]        # 品詞細分類1
            }
            morphs.append(morph)  # 形態素リストに追加
        else:  # &#039;EOS&#039; に到達した場合（文の終わりを示す）
            sentences.append(morphs)  # これまでの形態素リストを文リストに追加
            morphs = []  # 新しい文に備えてリストをリセット

# 確認として、3番目の文の形態素を表示
for morph in sentences[2]:
    print(morph)
        </code></pre>
        <p>各形態素（単語）は次の形式で辞書に格納される：</p>
        <ul>
            <li>surface: 表層形（単語そのもの）</li>
            <li>base: 基本形（辞書形、存在しない場合は'*'）</li>
            <li>pos: 品詞</li>
            <li>pos1: 品詞細分類1</li>
        </ul>
        <p>文の終わりはEOSで示される。</p>
        <p>'\t'で行を分割し、形態素の詳細情報をカンマで分割する。基本形が存在しない場合には、'*'を使ってエラーを防止する。</p>
        <h3>31. 動詞</h3>
        <pre><code>
ans = set()  # 動詞の表層形を格納するためのset（重複を排除）

# すべての文に対して処理を行う
for sentence in sentences:
  # 各文に含まれるすべての形態素をチェック
  for morph in sentence:
    # 品詞が「動詞」である場合、その表層形をsetに追加
    if morph[&#039;pos&#039;] == &#039;動詞&#039;:
      ans.add(morph[&#039;surface&#039;])  # setに追加することで自動的に重複を排除

# 動詞の表層形の種類を確認
print(f&#039;動詞の表層形の種類: {len(ans)}\n&#039;)  # 動詞の種類の数を表示

# 抽出された動詞の表層形の最初の10個を表示
for v in list(ans)[:10]:
  print(v)
        </code></pre>
        <h3>32. 動詞の基本形</h3>
        <pre><code>
ans = set()  # 動詞の基本形を格納するためのset（重複を排除）

# すべての文に対して処理を行う
for sentence in sentences:
  # 各文に含まれるすべての形態素をチェック
  for morph in sentence:
    # 品詞が「動詞」である場合、その基本形をsetに追加
    if morph[&#039;pos&#039;] == &#039;動詞&#039;:
      ans.add(morph[&#039;base&#039;])  # 基本形をsetに追加することで自動的に重複を排除

# 動詞の基本形の種類を確認
print(f&#039;動詞の基本形の種類: {len(ans)}\n&#039;)  # 動詞の基本形の種類を表示

# 抽出された動詞の基本形の最初の10個を表示
for v in list(ans)[:10]:
  print(v)
        </code></pre>
        <h3>33. 「AのB」</h3>
        <pre><code>
ans = set()  # 動詞の基本形を格納するためのset（重複を排除）

# すべての文に対して処理を行う
for sentence in sentences:
  # 各文に含まれるすべての形態素をチェック
  for morph in sentence:
    # 品詞が「動詞」である場合、その基本形をsetに追加
    if morph[&#039;pos&#039;] == &#039;動詞&#039;:
      ans.add(morph[&#039;base&#039;])  # 基本形をsetに追加することで自動的に重複を排除

# 動詞の基本形の種類を確認
print(f&#039;動詞の基本形の種類: {len(ans)}\n&#039;)  # 動詞の基本形の種類を表示

# 抽出された動詞の基本形の最初の10個を表示
for v in list(ans)[:10]:
  print(v)
        </code></pre>
        <h3>34. 名詞の連接</h3>
        <pre><code>
ans = set()  # 連接名詞を格納するためのset（重複を排除）

# 文ごとに形態素を処理
for sentence in sentences:
  nouns = &#039;&#039;  # 名詞の連続部分を一時的に格納
  num = 0     # 名詞が連続している個数をカウント
  
  for morph in sentence:
    if morph[&#039;pos&#039;] == &#039;名詞&#039;:  # 名詞の場合
      nouns = &#039;&#039;.join([nouns, morph[&#039;surface&#039;]])  # 名詞を連結
      num += 1  # 名詞のカウントを増やす
    elif num &gt;= 2:  # 名詞以外が出現し、2つ以上の名詞が連続していた場合
      ans.add(nouns)  # 連接名詞をsetに追加（重複排除）
      nouns = &#039;&#039;  # nounsをリセット
      num = 0     # カウントをリセット
    else:  # 名詞が1つ以下で連接が途切れた場合
      nouns = &#039;&#039;  # リセット
      num = 0     # リセット

  # 文の最後に名詞が2つ以上連続していた場合
  if num &gt;= 2:
    ans.add(nouns)

# 連接名詞の種類といくつかの例を表示
print(f&#039;連接名詞の種類: {len(ans)}\n&#039;)

# 最初の10個の連接名詞を表示
for n in list(ans)[:10]:
  print(n)
        </code></pre>
        <h3>35. 単語の出現頻度</h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        </section>
        <section id="05">
        <h2>第5章：係り受け解析</h2>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        </section>
        <section id="06">
        <h2>第6章：機械学習</h2>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        </section>
        <section id="07">
        <h2>第7章：単語ベクトル</h2>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        </section>
        <section id="03">
        <h2>第8章：ニューラルネット</h2>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        </section>
        <section id="09">
        <h2>第9章：RNNとCNN</h2>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        </section>
        <section id="10">
        <h2>第10章：機械翻訳</h2>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        <h3></h3>
        <pre><code>

        </code></pre>
        </section>
    </main>
</body>
</html>
