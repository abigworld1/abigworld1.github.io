<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【自然言語処理 ～言語処理100本ノック～】走る作曲家のAIカフェ</title>
    <link rel="stylesheet" href="style.css">
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1050827580219099"
     crossorigin="anonymous"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
      <section>
        <h2>言語処理100本ノック</h2>
        <p><a href="https://nlp100.github.io/ja/">言語処理100本ノック</a>に挑戦します。</p>
        <p>使用言語はPythonです。</p>
      </section>
      <section>
            <h2>目次</h2>
            <ul>
                <li><a href="#01">第1章：準備運動</a></li>
                <li><a href="#02">第2章：UNIXコマンド</a></li>
                <li><a href="#03">第3章：正規表現</a></li>
                <li><a href="#04">第4章：形態素解析</a></li>
                <li><a href="#05">第5章：係り受け解析</a></li>
                <li><a href="#06">第6章：機械学習</a></li>
                <li><a href="#07">第7章：単語ベクトル</a></li>
                <li><a href="#08">第8章：ニューラルネット</a></li>
                <li><a href="#09">第9章：RNNとCNN</a></li>
                <li><a href="#10">第10章：機械翻訳</a></li>
            </ul>
        </section>
      <section id="01">
        <h2>第1章：準備運動</h2>
        <h3>00.文字列の逆順</h3>
        <p><b>文字列”stressed”の文字を逆に（末尾から先頭に向かって）並べた文字列を得よ．</b></p>
        <pre><code>
# 文字列の定義
text = "stressed"

# 文字列を逆順に並べ替える
reversed_text = text[::-1]

# 結果を出力
print(reversed_text)
        </code></pre>
        <p><code2>text[start:end:step]</code2>というスライスの一般的な形式で、<code2>start</code2>と<code2>end</code2>を省略し、<code2>step</code2>に<code2>-1</code2>を指定することで、逆順に文字を取得できる。</p>
        <h3>01.「パタトクカシーー」</h3>
        <p><b>「パタトクカシーー」という文字列の1,3,5,7文字目を取り出して連結した文字列を得よ．</b></p>
        <pre><code>
# 文字列の定義
text = "パタトクカシーー"

# 1, 3, 5, 7文字目を取り出して連結
result = text[0] + text[2] + text[4] + text[6]

# 結果を出力
print(result)
        </code></pre>
        <h3>02.「パトカー」＋「タクシー」＝「パタトクカシーー」</h3>
        <p><b>「パトカー」＋「タクシー」の文字を先頭から交互に連結して文字列「パタトクカシーー」を得よ．</b></p>
        <pre><code>
# 文字列の定義
text1 = &quot;パトカー&quot;
text2 = &quot;タクシー&quot;

# 交互に文字を取り出して連結
result = &#039;&#039;.join([a + b for a, b in zip(text1, text2)])

# 結果を出力
print(result)
        </code></pre>
          <p>Pythonの<code2>zip</code2>関数を使うことで、2つの文字列を並列に処理できる。</p>
          <p><code2>zip(text1, text2)</code2>は、<code2>text1</code2>と<code2>text2</code2>のそれぞれ対応する位置の文字をペアにする。</p>
          <p>リスト内包表記を用いて、<code2>a + b</code2>という形で交互に文字を連結する。</p>
          <p><code2>''.join([...])</code2>で、リスト内の連結された文字を1つの文字列にまとめる。</p>
          <h3>03.円周率</h3>
          <p><b>“Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.”という文を単語に分解し，各単語の（アルファベットの）文字数を先頭から出現順に並べたリストを作成せよ．</b></p>
        <pre><code>
# 与えられた文章
sentence = &quot;Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.&quot;

# 単語に分解（句読点を除去）
words = sentence.replace(&quot;,&quot;, &quot;&quot;).replace(&quot;.&quot;, &quot;&quot;).split()

# 各単語の文字数を計算
word_lengths = [len(word) for word in words]

# 結果を出力
print(word_lengths)
        </code></pre>
          <p><code2>.replace(",", "")</code2>と<code2>.replace(".", "")</code2>を使って、カンマやピリオドなどの句読点を取り除く。これにより、単語だけを扱えるようにする。</p>
          <p><code2>.split()</code2>を使うことで、文章を単語ごとに分割し、リスト<code2>words</code2>に格納する。</p>
          <p>リスト内包表記を使って、<code2>[len(word) for word in words]</code2>で各単語の文字数を計算し、その結果を<code2>word_lengths</code2>に格納する。</p>
          <h3>04.元素記号</h3>
          <p><b>“Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.”という文を単語に分解し，1, 5, 6, 7, 8, 9, 15, 16, 19番目の単語は先頭の1文字，それ以外の単語は先頭の2文字を取り出し，取り出した文字列から単語の位置（先頭から何番目の単語か）への連想配列（辞書型もしくはマップ型）を作成せよ．</b></p>
        <pre><code>
# 与えられた文章
sentence = &quot;Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.&quot;

# 句読点を除去して単語に分解
words = sentence.replace(&quot;.&quot;, &quot;&quot;).split()

# 1文字だけを取り出す単語のインデックス（1から始まる）
one_char_indices = [1, 5, 6, 7, 8, 9, 15, 16, 19]

# 結果を格納する辞書
word_map = {}

# 各単語を処理して辞書に格納
for i, word in enumerate(words, 1):  # 単語の位置が1から始まるようにenumerateを使う
    if i in one_char_indices:
        word_map[word[:1]] = i  # 1文字だけを取り出す
    else:
        word_map[word[:2]] = i  # 2文字を取り出す

# 結果を出力
print(word_map)
        </code></pre>
        <h3>05.n-gram</h3>
          <p><b>与えられたシーケンス（文字列やリストなど）からn-gramを作る関数を作成せよ．この関数を用い，”I am an NLPer”という文から単語bi-gram，文字bi-gramを得よ．</b></p>
        <pre><code>
# n-gramを作成する関数
def n_gram(sequence, n):
    return [sequence[i:i+n] for i in range(len(sequence) - n + 1)]

# 与えられた文章
sentence = &quot;I am an NLPer&quot;

# 単語bi-gram
words = sentence.split()  # 文を単語に分解
word_bi_gram = n_gram(words, 2)  # 単語でのbi-gramを生成

# 文字bi-gram
char_bi_gram = n_gram(sentence.replace(&quot; &quot;, &quot;&quot;), 2)  # 空白を除去して文字でのbi-gramを生成

# 結果を出力
print(&quot;単語bi-gram:&quot;, word_bi_gram)
print(&quot;文字bi-gram:&quot;, char_bi_gram)
        </code></pre>
          <h3>06.集合</h3>
          <p><b>“paraparaparadise”と”paragraph”に含まれる文字bi-gramの集合を，それぞれ, XとYとして求め，XとYの和集合，積集合，差集合を求めよ．さらに，’se’というbi-gramがXおよびYに含まれるかどうかを調べよ．</b></p>
        <pre><code>
# n-gramを作成する関数
def n_gram(sequence, n):
    return [sequence[i:i+n] for i in range(len(sequence) - n + 1)]

# 文字列からbi-gramを作成
str1 = &quot;paraparaparadise&quot;
str2 = &quot;paragraph&quot;

# それぞれの文字bi-gram集合を求める
X = set(n_gram(str1, 2))
Y = set(n_gram(str2, 2))

# 和集合、積集合、差集合を求める
union = X | Y
intersection = X &amp; Y
difference = X - Y

# &#039;se&#039;がXとYに含まれるかどうかを調べる
is_se_in_X = &#039;se&#039; in X
is_se_in_Y = &#039;se&#039; in Y

# 結果を出力
print(&quot;X:&quot;, X)
print(&quot;Y:&quot;, Y)
print(&quot;和集合:&quot;, union)
print(&quot;積集合:&quot;, intersection)
print(&quot;差集合:&quot;, difference)
print(&quot;&#039;se&#039;がXに含まれるか:&quot;, is_se_in_X)
print(&quot;&#039;se&#039;がYに含まれるか:&quot;, is_se_in_Y)
        </code></pre>
          <h3>07.テンプレートによる文生成</h3>
          <p><b>引数x, y, zを受け取り「x時のyはz」という文字列を返す関数を実装せよ．さらに，x=12, y=”気温”, z=22.4として，実行結果を確認せよ．</b></p>
        <pre><code>
# 関数の定義
def create_sentence(x, y, z):
    return f&quot;{x}時の{y}は{z}&quot;

# 関数の実行
x = 12
y = &quot;気温&quot;
z = 22.4
result = create_sentence(x, y, z)

# 結果を出力
print(result)
        </code></pre>
          <h3>08.暗号文</h3>
          <p><b>与えられた文字列の各文字を，以下の仕様で変換する関数cipherを実装せよ．
              <ul>
                  <li>英小文字ならば(219 - 文字コード)の文字に置換</li>
                  <li>その他の文字はそのまま出力</li>
              </ul>
              この関数を用い，英語のメッセージを暗号化・復号化せよ．</b></p>
        <pre><code>
# cipher関数の定義
def cipher(text):
    # 各文字を処理
    result = &#039;&#039;.join([chr(219 - ord(c)) if c.islower() else c for c in text])
    return result

# 暗号化するメッセージ
message = &quot;I am a NLPer.&quot;

# 暗号化
encrypted_message = cipher(message)
print(&quot;暗号化:&quot;, encrypted_message)

# 復号化 (同じ関数を使うことで復号も可能)
decrypted_message = cipher(encrypted_message)
print(&quot;復号化:&quot;, decrypted_message)
        </code></pre>
          <h3>09.Typoglycemia</h3>
          <p><b>スペースで区切られた単語列に対して，各単語の先頭と末尾の文字は残し，それ以外の文字の順序をランダムに並び替えるプログラムを作成せよ．ただし，長さが４以下の単語は並び替えないこととする．適当な英語の文（例えば”I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .”）を与え，その実行結果を確認せよ．</b></p>
        <pre><code>
import random

# 単語をシャッフルする関数
def shuffle_middle(word):
    if len(word) &lt;= 4:  # 長さが4以下の単語は並び替えない
        return word
    middle = list(word[1:-1])  # 先頭と末尾以外の文字を取り出す
    random.shuffle(middle)  # 先頭と末尾以外の文字をシャッフル
    return word[0] + &#039;&#039;.join(middle) + word[-1]  # 先頭、シャッフルした文字、末尾を結合して返す

# 文章全体を処理する関数
def process_sentence(sentence):
    words = sentence.split()  # スペースで分割して単語ごとにリスト化
    shuffled_words = [shuffle_middle(word) for word in words]  # 各単語をシャッフル処理
    return &#039; &#039;.join(shuffled_words)  # 単語を再びスペースで結合して返す

# 適当な英語の文
sentence = &quot;I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .&quot;

# 実行
result = process_sentence(sentence)
print(result)
        </code></pre>
      </section>
        <section id="02">
            <h2>第2章：UNIXコマンド</h2>
            <h3>10. 行数のカウント</h3>
            <p><b>行数をカウントせよ．確認にはwcコマンドを用いよ．</b></p>
            <p>Pythonプログラム</p>
            <pre><code>
# popular-names.txtの行数をカウントするプログラム
def count_lines(file_path):
    with open(file_path, &#039;r&#039;) as file:
        lines = file.readlines()  # ファイルをすべて読み込んで行ごとにリスト化
        return len(lines)  # 行数を返す

# popular-names.txtのパス
file_path = &#039;popular-names.txt&#039;

# 行数をカウントして表示
line_count = count_lines(file_path)
print(f&quot;行数: {line_count}&quot;)
            </code></pre>
            <p>UNIXコマンド</p>
            <pre><code>
wc -l popular-names.txt
            </code></pre>
            <h3>11. タブをスペースに置換</h3>
            <p><b>タブ1文字につきスペース1文字に置換せよ．確認にはsedコマンド，trコマンド，もしくはexpandコマンドを用いよ．</b></p>
            <p>Pythonプログラム</p>
            <pre><code>
# タブ文字をスペースに置換するプログラム
def replace_tabs_with_spaces(file_path, output_file_path):
    with open(file_path, &#039;r&#039;) as file:
        content = file.read()  # ファイル全体を読み込む
        modified_content = content.replace(&#039;\t&#039;, &#039; &#039;)  # タブをスペースに置換
    
    with open(output_file_path, &#039;w&#039;) as output_file:
        output_file.write(modified_content)  # 置換後の内容を書き込む

# ファイルパスの指定
input_file_path = &#039;popular-names.txt&#039;
output_file_path = &#039;popular-names-space.txt&#039;

# 置換処理の実行
replace_tabs_with_spaces(input_file_path, output_file_path)
print(f&quot;&#039;{input_file_path}&#039;のタブをスペースに置換し、&#039;{output_file_path}&#039;に保存しました。&quot;)
            </code></pre>
            <p>UNIXコマンド</p>
            <ul>
                <li>sed</li>
                <pre><code>
sed &#039;s/\t/ /g&#039; popular-names.txt &gt; popular-names-space.txt
                </code></pre>
                <li>tr</li>
                <pre><code>
tr &#039;\t&#039; &#039; &#039; &lt; popular-names.txt &gt; popular-names-space.txt
                </code></pre>
                <li>expand</li>
                <pre><code>
expand -t 1 popular-names.txt &gt; popular-names-space.txt
                </code></pre>
            </ul>
            <h3>12. 1列目をcol1.txtに，2列目をcol2.txtに保存</h3>
            <p><b>各行の1列目だけを抜き出したものをcol1.txtに，2列目だけを抜き出したものをcol2.txtとしてファイルに保存せよ．確認にはcutコマンドを用いよ．</b></p>
            <p>Pythonプログラム</p>
            <pre><code>
# 1列目と2列目を別ファイルに保存するプログラム
def extract_columns(input_file, col1_file, col2_file):
    with open(input_file, &#039;r&#039;) as file, open(col1_file, &#039;w&#039;) as col1, open(col2_file, &#039;w&#039;) as col2:
        for line in file:
            columns = line.split(&#039;\t&#039;)  # タブ区切りで列を分割
            if len(columns) &gt;= 2:
                col1.write(columns[0] + &#039;\n&#039;)  # 1列目を書き込み
                col2.write(columns[1] + &#039;\n&#039;)  # 2列目を書き込み

# 入力ファイルと出力ファイルのパス
input_file_path = &#039;popular-names.txt&#039;
col1_output_path = &#039;col1.txt&#039;
col2_output_path = &#039;col2.txt&#039;

# 列を抜き出してファイルに保存
extract_columns(input_file_path, col1_output_path, col2_output_path)
print(f&quot;1列目を &#039;{col1_output_path}&#039; に、2列目を &#039;{col2_output_path}&#039; に保存しました。&quot;)
            </code></pre>
            <p>UNIXコマンド</p>
            <pre><code>
# 1列目を col1.txt に保存
cut -f 1 popular-names.txt &gt; col1.txt

# 2列目を col2.txt に保存
cut -f 2 popular-names.txt &gt; col2.txt
            </code></pre>
            <h3>13. col1.txtとcol2.txtをマージ</h3>
            <p><b>12で作ったcol1.txtとcol2.txtを結合し，元のファイルの1列目と2列目をタブ区切りで並べたテキストファイルを作成せよ．確認にはpasteコマンドを用いよ．</b></p>
            <p>Pythonプログラム</p>
            <pre><code>
# col1.txtとcol2.txtを結合して、タブ区切りで新しいファイルに書き込むプログラム
def merge_columns(col1_file, col2_file, output_file):
    with open(col1_file, &#039;r&#039;) as col1, open(col2_file, &#039;r&#039;) as col2, open(output_file, &#039;w&#039;) as output:
        for col1_line, col2_line in zip(col1, col2):
            output.write(f&quot;{col1_line.strip()}\t{col2_line.strip()}\n&quot;)  # タブで結合して書き込む

# ファイルパスの指定
col1_file_path = &#039;col1.txt&#039;
col2_file_path = &#039;col2.txt&#039;
output_file_path = &#039;merged.txt&#039;

# 結合処理の実行
merge_columns(col1_file_path, col2_file_path, output_file_path)
print(f&quot;&#039;{col1_file_path}&#039; と &#039;{col2_file_path}&#039; を結合し、&#039;{output_file_path}&#039; に保存しました。&quot;)
            </code></pre>
            <p>UNIXコマンド</p>
            <pre><code>
paste col1.txt col2.txt &gt; merged.txt
            </code></pre>
            <h3>14. 先頭からN行を出力</h3>
            <p><b>自然数Nをコマンドライン引数などの手段で受け取り，入力のうち先頭のN行だけを表示せよ．確認にはheadコマンドを用いよ．</b></p>
            <p>Pythonプログラム</p>
            <pre><code>
# 以下の14.pyファイルを作成し、python 14.py popular-names.txt 5で実行

import sys

# 先頭N行を表示するプログラム
def head(file_path, N):
    with open(file_path, &#039;r&#039;) as file:
        for i, line in enumerate(file):
            if i &gt;= N:
                break
            print(line.strip())

# コマンドライン引数の処理
if __name__ == &quot;__main__&quot;:
    if len(sys.argv) != 3:
        print(&quot;Usage: python script.py &lt;file_path&gt; &lt;N&gt;&quot;)
    else:
        file_path = sys.argv[1]  # ファイルのパス
        N = int(sys.argv[2])     # 表示する行数
        head(file_path, N)
            </code></pre>
            <p>UNIXコマンド</p>
            <pre><code>
head -n 5 popular-names.txt
            </code></pre>
            <h3>15. 末尾のN行を出力</h3>
            <p><b>自然数Nをコマンドライン引数などの手段で受け取り，入力のうち末尾のN行だけを表示せよ．確認にはtailコマンドを用いよ．</b></p>
            <p>Pythonプログラム</p>
            <pre><code>
# 以下の15.pyファイルを作成し、python 15.py popular-names.txt 5で実行

import sys

# 末尾N行を表示するプログラム
def tail(file_path, N):
    with open(file_path, &#039;r&#039;) as file:
        lines = file.readlines()  # 全ての行をリストとして読み込む
        for line in lines[-N:]:   # 末尾からN行だけスライスして表示
            print(line.strip())

# コマンドライン引数の処理
if __name__ == &quot;__main__&quot;:
    if len(sys.argv) != 3:
        print(&quot;Usage: python script.py &lt;file_path&gt; &lt;N&gt;&quot;)
    else:
        file_path = sys.argv[1]  # ファイルのパス
        N = int(sys.argv[2])     # 表示する行数
        tail(file_path, N)
            </code></pre>
            <p>UNIXコマンド</p>
            <pre><code>
tail -n 5 popular-names.txt
            </code></pre>
            <h3>16. ファイルをN分割する</h3>
            <p><b>自然数Nをコマンドライン引数などの手段で受け取り，入力のファイルを行単位でN分割せよ．同様の処理をsplitコマンドで実現せよ．</b></p>
            <p>Pythonプログラム</p>
            <pre><code>
# 以下の16.pyファイルを作成し、python 16.py popular-names.txt 5で実行

import sys
import math

# ファイルを行単位でN分割するプログラム
def split_file(file_path, N):
    with open(file_path, &#039;r&#039;) as file:
        lines = file.readlines()  # ファイルの全行をリストに読み込む
        total_lines = len(lines)
        lines_per_file = math.ceil(total_lines / N)  # 各ファイルに含まれる行数を計算

        # ファイルを分割して書き込み
        for i in range(N):
            start = i * lines_per_file
            end = min(start + lines_per_file, total_lines)
            with open(f&quot;split_{i+1}.txt&quot;, &#039;w&#039;) as output_file:
                output_file.writelines(lines[start:end])
            print(f&quot;split_{i+1}.txtに行数 {start+1} から {end} を書き込みました。&quot;)

# コマンドライン引数の処理
if __name__ == &quot;__main__&quot;:
    if len(sys.argv) != 3:
        print(&quot;Usage: python script.py &lt;file_path&gt; &lt;N&gt;&quot;)
    else:
        file_path = sys.argv[1]  # ファイルのパス
        N = int(sys.argv[2])     # 分割数
        split_file(file_path, N)
            </code></pre>
            <p>UNIXコマンド</p>
            <pre><code>
split -l $(( $(wc -l &lt; popular-names.txt) / 5 )) popular-names.txt split_
            </code></pre>
            <h3>17. １列目の文字列の異なり</h3>
            <p><b>1列目の文字列の種類（異なる文字列の集合）を求めよ．確認にはcut, sort, uniqコマンドを用いよ．</b></p>
            <p>Pythonプログラム</p>
            <pre><code>
def unique_first_column(file_path):
    with open(file_path, &#039;r&#039;) as file:
        first_column = {line.split(&#039;\t&#039;)[0] for line in file}  # 1列目をセットに格納
    return first_column

# ファイルパスの指定
file_path = &#039;popular-names.txt&#039;

# 1列目の異なる文字列を取得
unique_names = unique_first_column(file_path)

# 結果を出力
for name in sorted(unique_names):
    print(name)
            </code></pre>
            <p>セット<code2>{}</code2>は重複を自動的に除去するため、異なる一列目の文字列のみが取得される。</p>
            <p>UNIXコマンド</p>
            <pre><code>
cut -f 1 popular-names.txt | sort | uniq
            </code></pre>
            <h3>18. 各行を3コラム目の数値の降順にソート</h3>
            <p><b>各行を3コラム目の数値の逆順で整列せよ（注意: 各行の内容は変更せずに並び替えよ）．確認にはsortコマンドを用いよ（この問題はコマンドで実行した時の結果と合わなくてもよい）．</b></p>
            <p>Pythonプログラム</p>
            <pre><code>
def sort_by_third_column(file_path, output_file):
    with open(file_path, &#039;r&#039;) as file:
        lines = file.readlines()

    # 3列目の数値を基準に行をソート（逆順）
    sorted_lines = sorted(lines, key=lambda line: int(line.split(&#039;\t&#039;)[2]), reverse=True)

    # 結果を出力
    with open(output_file, &#039;w&#039;) as output:
        output.writelines(sorted_lines)

# ファイルパスの指定
input_file_path = &#039;popular-names.txt&#039;
output_file_path = &#039;sorted_by_third_column.txt&#039;

# 3列目の数値でソートして新しいファイルに保存
sort_by_third_column(input_file_path, output_file_path)
print(f&quot;3列目を基準に逆順でソートし、&#039;{output_file_path}&#039; に保存しました。&quot;)
            </code></pre>
            <p>UNIXコマンド</p>
            <pre><code>
sort -k 3,3 -n -r popular-names.txt &gt; sorted_by_third_column.txt
            </code></pre>
            <h3>19. 各行の1コラム目の文字列の出現頻度を求め，出現頻度の高い順に並べる</h3>
            <p><b>各行の1列目の文字列の出現頻度を求め，その高い順に並べて表示せよ．確認にはcut, uniq, sortコマンドを用いよ．</b></p>
            <p>Pythonプログラム</p>
            <pre><code>
from collections import Counter

# 1列目の出現頻度を求め、頻度順にソートするプログラム
def count_first_column(file_path):
    with open(file_path, &#039;r&#039;) as file:
        first_column = [line.split(&#039;\t&#039;)[0] for line in file]  # 1列目の文字列をリストに格納

    # 出現頻度をカウント
    counts = Counter(first_column)

    # 出現頻度の高い順にソートして表示
    sorted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)

    for name, count in sorted_counts:
        print(f&quot;{name}: {count}&quot;)

# ファイルパスの指定
file_path = &#039;popular-names.txt&#039;

# 出現頻度をカウントして表示
count_first_column(file_path)
            </code></pre>
            <p>UNIXコマンド</p>
            <pre><code>
cut -f 1 popular-names.txt | sort | uniq -c | sort -nr
            </code></pre>
        </section>
        <section id="03">
        <h2>第3章：正規表現</h2>
        <h3>20. JSONデータの読み込み</h3>
        <p><b>Wikipedia記事のJSONファイルを読み込み，「イギリス」に関する記事本文を表示せよ．問題21-29では，ここで抽出した記事本文に対して実行せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>21. カテゴリ名を含む行を抽出</h3>
        <p><b>記事中でカテゴリ名を宣言している行を抽出せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>22. カテゴリ名の抽出</h3>
        <p><b>記事のカテゴリ名を（行単位ではなく名前で）抽出せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>23. セクション構造</h3>
        <p><b>記事中に含まれるセクション名とそのレベル（例えば”== セクション名 ==”なら1）を表示せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>24. ファイル参照の抽出</h3>
        <p><b>記事から参照されているメディアファイルをすべて抜き出せ．</b></p>
        <pre><code>

        </code></pre>
        <h3>25. テンプレートの抽出</h3>
        <p><b>記事中に含まれる「基礎情報」テンプレートのフィールド名と値を抽出し，辞書オブジェクトとして格納せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>26. 強調マークアップの除去</h3>
        <p><b>25の処理時に，テンプレートの値からMediaWikiの強調マークアップ（弱い強調，強調，強い強調のすべて）を除去してテキストに変換せよ（参考: <a href="https://ja.wikipedia.org/wiki/Help:%E6%97%A9%E8%A6%8B%E8%A1%A8">マークアップ早見表</a>）．</b></p>
        <pre><code>

        </code></pre>
        <h3>27. 内部リンクの除去</h3>
        <p><b>26の処理に加えて，テンプレートの値からMediaWikiの内部リンクマークアップを除去し，テキストに変換せよ</b></p>
        <pre><code>

        </code></pre>
        <h3>28. MediaWikiマークアップの除去</h3>
        <p><b>27の処理に加えて，テンプレートの値からMediaWikiマークアップを可能な限り除去し，国の基本情報を整形せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>29. 国旗画像のURLを取得する</h3>
        <p><b>テンプレートの内容を利用し，国旗画像のURLを取得せよ．（ヒント: <a href="https://www.mediawiki.org/wiki/API:Main_page/ja">MediaWiki API</a>の<a href="https://www.mediawiki.org/wiki/API:Imageinfo">imageinfo</a>を呼び出して，ファイル参照をURLに変換すればよい）</b></p>
        <pre><code>

        </code></pre>
        </section>
        <section id="04">
        <h2>第4章：形態素解析</h2>
        <h3>30. 形態素解析結果の読み込み</h3>
        <p><b>形態素解析結果（neko.txt.mecab）を読み込むプログラムを実装せよ．ただし，各形態素は表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をキーとするマッピング型に格納し，1文を形態素（マッピング型）のリストとして表現せよ．第4章の残りの問題では，ここで作ったプログラムを活用せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>31. 動詞</h3>
        <p><b>動詞の表層形をすべて抽出せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>32. 動詞の基本形</h3>
        <p><b>動詞の基本形をすべて抽出せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>33. 「AのB」</h3>
        <p><b>2つの名詞が「の」で連結されている名詞句を抽出せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>34. 名詞の連接</h3>
        <p><b>名詞の連接（連続して出現する名詞）を最長一致で抽出せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>35. 単語の出現頻度Permalink</h3>
        <p><b>文章中に出現する単語とその出現頻度を求め，出現頻度の高い順に並べよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>36. 頻度上位10語</h3>
        <p><b>出現頻度が高い10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>37. 「猫」と共起頻度の高い上位10語Permalink</h3>
        <p><b>「猫」とよく共起する（共起頻度が高い）10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>38. ヒストグラムPermalink</h3>
        <p><b>単語の出現頻度のヒストグラムを描け．ただし，横軸は出現頻度を表し，1から単語の出現頻度の最大値までの線形目盛とする．縦軸はx軸で示される出現頻度となった単語の異なり数（種類数）である．</b></p>
        <pre><code>

        </code></pre>
        <h3>39. Zipfの法則</h3>
        <p><b>単語の出現頻度順位を横軸，その出現頻度を縦軸として，両対数グラフをプロットせよ．</b></p>
        <pre><code>

        </code></pre>
        </section>
        <section id="05">
        <h2>第5章：係り受け解析</h2>
        <h3>40. 係り受け解析結果の読み込み（形態素）</h3>
        <p><b>形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，係り受け解析の結果（ai.ja.txt.parsed）を読み込み，各文をMorphオブジェクトのリストとして表現し，冒頭の説明文の形態素列を表示せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>41. 係り受け解析結果の読み込み（文節・係り受け）</h3>
        <p><b>40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストの係り受け解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，冒頭の説明文の文節の文字列と係り先を表示せよ．本章の残りの問題では，ここで作ったプログラムを活用せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>42. 係り元と係り先の文節の表示</h3>
        <p><b>係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>43. 名詞を含む文節が動詞を含む文節に係るものを抽出</h3>
        <p><b>名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>44. 係り受け木の可視化</h3>
        <p><b>与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，Graphviz等を用いるとよい．</b></p>
        <pre><code>

        </code></pre>
        <h3>45. 動詞の格パターンの抽出</h3>
        <b>
        <p>今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．</p>
        <ul>
            <li>動詞を含む文節において，最左の動詞の基本形を述語とする</li>
            <li>述語に係る助詞を格とする</li>
            <li>述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる</li>
        </ul>
        <p>「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．</p>
        <code2>作り出す で は を</code2>
        <p>このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．</p>
        <ul>
            <li>コーパス中で頻出する述語と格パターンの組み合わせ</li>
            <li>「行う」「なる」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）</li>
        </ul>
        </b>
        <pre><code>

        </code></pre>
        <h3>46. 動詞の格フレーム情報の抽出</h3>
        <b>
        <p>45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．</p>
        <ul>
            <li>項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）</li>
            <li>述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる</li>
        </ul>
        <p>「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．</p>
        <code2>作り出す で は を 会議で ジョンマッカーシーは 用語を</code2>
        </b>
        <pre><code>

        </code></pre>
        <h3>47. 機能動詞構文のマイニング</h3>
        <b>
        <p>動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．</p>
        <ul>
            <li>「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする</li>
            <li>述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる</li>
            <li>述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる</li>
            <li>述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）</li>
        </ul>
        <p>例えば「また、自らの経験を元に学習を行う強化学習という手法もある。」という文から，以下の出力が得られるはずである．</p>
        <code2>学習を行う    に を  元に 経験を</code2>
        </b>
        <pre><code>

        </code></pre>
        <h3>48. 名詞から根へのパスの抽出</h3>
        <b>
        <p>文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．</p>
        <ul>
            <li>各文節は（表層形の）形態素列で表現する</li>
            <li>パスの開始文節から終了文節に至るまで，各文節の表現を” -> “で連結する</li>
        </ul>
        <p>「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．</p>
        <code2>ジョンマッカーシーは -> 作り出した</code2><br>
        <code2>AIに関する -> 最初の -> 会議で -> 作り出した</code2><br>
        <code2>最初の -> 会議で -> 作り出した</code2><br>
        <code2>会議で -> 作り出した</code2><br>
        <code2>人工知能という -> 用語を -> 作り出した</code2><br>
        <code2>用語を -> 作り出した</code2>
        <p>KNPを係り受け解析に用いた場合，次のような出力が得られると思われる．</p>
        <code2>ジョンマッカーシーは -> 作り出した</code2><br>
        <code2>ＡＩに -> 関する -> 会議で -> 作り出した</code2><br>
        <code2>会議で -> 作り出した</code2><br>
        <code2>人工知能と -> いう -> 用語を -> 作り出した</code2><br>
        <code2>用語を -> 作り出した</code2>
        </b>
        <pre><code>

        </code></pre>
        <h3>49. 名詞間の係り受けパスの抽出</h3>
        <b>
        <p>文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がiとj（i<j）のとき，係り受けパスは以下の仕様を満たすものとする．</p>
        <ul>
            <li>問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する</li>
            <li>文節iとjに含まれる名詞句はそれぞれ，XとYに置換する</li>
        </ul>
        <p>また，係り受けパスの形状は，以下の2通りが考えられる．</p>
        <ul>
            <li>文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示</li>
            <li>上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を” | “で連結して表示</li>
        </ul>
        <p>「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．</p>
        <code2>Xは | Yに関する -> 最初の -> 会議で | 作り出した</code2><br>
        <code2>Xは | Yの -> 会議で | 作り出した</code2><br>
        <code2>Xは | Yで | 作り出した</code2><br>
        <code2>Xは | Yという -> 用語を | 作り出した</code2><br>
        <code2>Xは | Yを | 作り出した</code2><br>
        <code2>Xに関する -> Yの</code2><br>
        <code2>Xに関する -> 最初の -> Yで</code2><br>
        <code2>Xに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した</code2><br>
        <code2>Xに関する -> 最初の -> 会議で | Yを | 作り出した</code2><br>
        <code2>Xの -> Yで</code2><br>
        <code2>Xの -> 会議で | Yという -> 用語を | 作り出した</code2><br>
        <code2>Xの -> 会議で | Yを | 作り出した</code2><br>
        <code2>Xで | Yという -> 用語を | 作り出した</code2><br>
        <code2>Xで | Yを | 作り出した</code2><br>
        <code2>Xという -> Yを</code2>
        <p>KNPを係り受け解析に用いた場合，次のような出力が得られると思われる．</p>
        <code2>Xは | Yに -> 関する -> 会議で | 作り出した。</code2><br>
        <code2>Xは | Yで | 作り出した。</code2><br>
        <code2>Xは | Yと -> いう -> 用語を | 作り出した。</code2><br>
        <code2>Xは | Yを | 作り出した。</code2><br>
        <code2>Xに -> 関する -> Yで</code2><br>
        <code2>Xに -> 関する -> 会議で | Yと -> いう -> 用語を | 作り出した。</code2><br>
        <code2>Xに -> 関する -> 会議で | Yを | 作り出した。</code2><br>
        <code2>Xで | Yと -> いう -> 用語を | 作り出した。</code2><br>
        <code2>Xで | Yを | 作り出した。</code2><br>
        <code2>Xと -> いう -> Yを</code2>
        </b>
        <pre><code>

        </code></pre>
        </section>
        <section id="06">
        <h2>第6章：機械学習</h2>
        <h3>50. データの入手・整形</h3>
        <b>
            <p><a href="https://archive.ics.uci.edu/dataset/359/news+aggregator">News Aggregator Data Set</a>をダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．</p>
            <ol>
                <li>ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．</li>
                <li>情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．</li>
                <li>抽出された事例をランダムに並び替える．</li>
                <li>抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．</li>
            </ol>
            <p>学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．</p>
        </b>
        <pre><code>

        </code></pre>
        <h3>51. 特徴量抽出Permalink</h3>
        <p><b>学習データ，検証データ，評価データから特徴量を抽出し，それぞれtrain.feature.txt，valid.feature.txt，test.feature.txtというファイル名で保存せよ． なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．</b></p>
        <pre><code>

        </code></pre>
        <h3>52. 学習</h3>
        <p><b>51で構築した学習データを用いて，ロジスティック回帰モデルを学習せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>53. 予測</h3>
        <p><b>52で学習したロジスティック回帰モデルを用い，与えられた記事見出しからカテゴリとその予測確率を計算するプログラムを実装せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>54. 正解率の計測</h3>
        <p><b>52で学習したロジスティック回帰モデルの正解率を，学習データおよび評価データ上で計測せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>55. 混同行列の作成</h3>
        <p><b>52で学習したロジスティック回帰モデルの混同行列（confusion matrix）を，学習データおよび評価データ上で作成せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>56. 適合率，再現率，F1スコアの計測</h3>
        <p><b>52で学習したロジスティック回帰モデルの適合率，再現率，F1スコアを，評価データ上で計測せよ．カテゴリごとに適合率，再現率，F1スコアを求め，カテゴリごとの性能をマイクロ平均（micro-average）とマクロ平均（macro-average）で統合せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>57. 特徴量の重みの確認</h3>
        <p><b>52で学習したロジスティック回帰モデルの中で，重みの高い特徴量トップ10と，重みの低い特徴量トップ10を確認せよ．</b></p>
        <pre><code>

        </code></pre>
        <h3>58. 正則化パラメータの変更</h3>
        <p><b>ロジスティック回帰モデルを学習するとき，正則化パラメータを調整することで，学習時の過学習（overfitting）の度合いを制御できる．異なる正則化パラメータでロジスティック回帰モデルを学習し，学習データ，検証データ，および評価データ上の正解率を求めよ．実験の結果は，正則化パラメータを横軸，正解率を縦軸としたグラフにまとめよ</b></p>
        <pre><code>

        </code></pre>
        <h3>59. ハイパーパラメータの探索</h3>
        <p><b>学習アルゴリズムや学習パラメータを変えながら，カテゴリ分類モデルを学習せよ．検証データ上の正解率が最も高くなる学習アルゴリズム・パラメータを求めよ．また，その学習アルゴリズム・パラメータを用いたときの評価データ上の正解率を求めよ．</b></p>
        <pre><code>

        </code></pre>
        </section>
        <section id="07">
        <h2>第7章：単語ベクトル</h2>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        </section>
        <section id="08">
        <h2>第8章：ニューラルネット</h2>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        </section>
        <section id="03">
        <h2>第9章：RNNとCNN</h2>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        </section>
        <section id="03">
        <h2>第10章：機械翻訳</h2>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        <h3></h3>
        <p><b></b></p>
        <pre><code>

        </code></pre>
        </section>
    </main>
</body>
</html>
