<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>【研究ノート】走る作曲家のAIカフェ - 深層強化学習メモ</title>
    <link rel="stylesheet" href="style.css"> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
            <h2>目次</h2>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#source">Source</a></li>
                <li><a href="#mdp-planning">Makov Decision Process and Planning Algorithm</a></li>
            </ul>
        </section>
        <section id="neural-network">
            <h2>Overview</h2>
            深層強化学習とは
            <ul>
                <li>エージェントが環境と相互作用しながら集めたデータを使って高い報酬を得る方法を学習する</li>
                <li>エージェントが試行錯誤的に行動し、環境から与えられる報酬をもとに、期待報酬を最大化するような行動を選択するように行動を修正していく</li>
            </ul>
            という手法です。このページでは、全くの初学者である私が、深層強化学習についてまとめていきます。
        </section>
        <section id="source">
            <h2>Source</h2>
            以下の書籍・講座を参考にしました。
            <ul>
                <li><a href="https://www.amazon.co.jp/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%EF%BC%88%E7%AC%AC2%E7%89%88%EF%BC%89-R-Sutton/dp/4627826621/ref=pd_lpo_sccl_2/357-3139727-5663608?pd_rd_w=NkO82&content-id=amzn1.sym.855d8f70-df76-4181-80b0-56e48ae3bb9b&pf_rd_p=855d8f70-df76-4181-80b0-56e48ae3bb9b&pf_rd_r=PMD3GFN7FFG0MYVRX9KR&pd_rd_wg=tHSVD&pd_rd_r=57b87974-355e-4c18-ae1f-bcdf55adb96d&pd_rd_i=4627826621&psc=1#customerReviews">強化学習（第2版）</a></li>
                <li><a href="https://www.amazon.co.jp/%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8BDeep-Learning-%E2%80%95%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E7%B7%A8-%E6%96%8E%E8%97%A4-%E5%BA%B7%E6%AF%85/dp/4873119758/ref=sr_1_3?adgrpid=120952843404&dib=eyJ2IjoiMSJ9.hH2wBl7SFx37GV25E2wHXN5tkG88yeA4w0_uhCDQeAGXHennOFOA9HtC0IZoe1CJ12MDkBW19CKDw3V8W2epn6z8IWch7fF_7Pwyi6zEDUlrbfxmsgNs9jOCWG4xqL5Ukct-erC8712p4NJMeWAkLnC9f4WVhcuWRdquRpwzfwj4z7us0YWOIBZY56XeEUZOZo4nhQB5e-BXeh85Ks22-elRZBG8YkYU53x3nirZCAH7Yd4T3n8nN0MGoXI2EKoztLgR5El8GbWZOVz6wVt01ZaJENrUy8OFCaJItSusR_E.zYa2-YOpfMzxqHg0sL06bGmED9UNuzELMrLgzQ2FWCQ&dib_tag=se&hvadid=665912024283&hvdev=c&hvqmt=b&hvtargid=kwd-851214456388&hydadcr=27494_14701818&jp-ad-ap=0&keywords=%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8Bdeep-learning&qid=1721996873&sr=8-3">ゼロから作るDeep Learning ❹ ―強化学習編</a></li>
                <li><a href="https://www.amazon.co.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-Python%E3%81%A7%E5%AD%A6%E3%81%B6%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E6%94%B9%E8%A8%82%E7%AC%AC2%E7%89%88-%E5%85%A5%E9%96%80%E3%81%8B%E3%82%89%E5%AE%9F%E8%B7%B5%E3%81%BE%E3%81%A7-%E4%B9%85%E4%BF%9D/dp/4065172519">機械学習スタートアップシリーズ Pythonで学ぶ強化学習 [改訂第2版] 入門から実践まで</a></li>
                <li><a href="https://www.amazon.co.jp/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E6%A3%AE%E6%9D%91-%E5%93%B2%E9%83%8E/dp/4065155916">強化学習 (機械学習プロフェッショナルシリーズ) </a></li>
                <li><a href="https://www.amazon.co.jp/%E3%81%A4%E3%81%8F%E3%82%8A%E3%81%AA%E3%81%8C%E3%82%89%E5%AD%A6%E3%81%B6-%E6%B7%B1%E5%B1%A4%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-PyTorch%E3%81%AB%E3%82%88%E3%82%8B%E5%AE%9F%E8%B7%B5%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-%E6%A0%AA%E5%BC%8F%E4%BC%9A%E7%A4%BE%E9%9B%BB%E9%80%9A%E5%9B%BD%E9%9A%9B%E6%83%85%E5%A0%B1%E3%82%B5%E3%83%BC%E3%83%93%E3%82%B9-%E5%B0%8F%E5%B7%9D%E9%9B%84%E5%A4%AA%E9%83%8E/dp/4839965625">つくりながら学ぶ! 深層強化学習 ~PyTorchによる実践プログラミング~</a></li>
                <li><a href="https://www.amazon.co.jp/%E3%83%9E%E3%83%AB%E3%83%81%E3%82%A8%E3%83%BC%E3%82%B8%E3%82%A7%E3%83%B3%E3%83%88%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%AE%E5%9F%BA%E7%A4%8E%E3%81%A8%E5%BF%9C%E7%94%A8%E2%80%95%E8%A4%87%E9%9B%91%E7%B3%BB%E5%B7%A5%E5%AD%A6%E3%81%AE%E8%A8%88%E7%AE%97%E3%83%91%E3%83%A9%E3%83%80%E3%82%A4%E3%83%A0-%E5%A4%A7%E5%86%85-%E6%9D%B1/dp/4339023884">マルチエ-ジェントシステムの基礎と応用: 複雑系工学の計算パラダイム</a></li>
                <li><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u">Stanford CS234: Reinforcement Learning | Winter 2019</a></li>
                <li><a href="https://weblab.t.u-tokyo.ac.jp/education/deep-reinforcement-learning/">深層強化学習 Deep Learning 応用講座 2024 Summer</a></li>
            </ul>
        </section>
        <section id="mdp-planning">
            <h2>Makov Decision Process and Planning Algorithm</h2>
            <ul>
                <li>深層強化学習≒強化学習＋関数近似</li>
                <li>強化学習≒プランニング＋サンプル近似</li>
            </ul>
            <h3>強化学習で扱う問題</h3>
            <ul>
                <li>単純な分類・回帰問題と違って、意思決定によって状態が遷移する。</li>
                <li>正解ラベルがない。人の真似をしても、人を超えることはできない。</li>
                <li>与えられた状況に対して、長期的に得する意思決定がしたい。</li>
                <li>強化学習では、<b>逐次意思決定問題</b>を扱う。</li>
                <li>逐次意思決定問題では、エージェントが何らかの環境で意思決定を複数回繰り返す。</li>
                <li>最適な意思決定のルールを見つけたい。</li>
                <li>ここでのアルゴリズムは、「最適な意思決定のルールを見つけるための手順」を指す。</li>
                <li>現実の逐次意思決定問題は非常に複雑である。</li>
                <li>そこで、<b>マルコフ決定過程</b>という数理モデルを使って、シンプルに定式化する。</li>
            </ul>
            <h3>マルコフ決定過程</h3>
            <ul>
                <li>マルコフ決定過程は、次の５つで構成される。
                <ul>
                    <li>行動集合 \(A\) :エージェントが選択可能な行動(意思決定) (\(a \in A\))の集合</li>
                    <li>状態集合 \(S\) :環境の全ての状態 (\(s ∈ S\)) の集合</li>
                    <li>状態遷移確率 \(P: S \times A \rightarrow \Delta (S)\) :ある状態行動から次の状態に遷移する確率</li>
                    <li>報酬関数 \(r: S \times A \rightarrow \mathbb{R}\) :ある状態行動に対する評価</li>
                    <li>初期状態 \(s_0 \in S\) :最初の状態</li>
                </ul>
                </li>
                <li>これに加えて、エージェントを以下のようにモデル化する。
                    <ul>
                        <li>方策 \(\pi: S \rightarrow \Delta (A)\) :ある状態でのエージェントの意思決定が従う確率</li>
                    </ul>
                </li>
                <li>マルコフ決定過程上の逐次意思決定問題のうち、特に、
                    <ul>
                        <li>マルコフ決定過程の情報がすべて既知の場合の意思決定問題をプランニング問題</li>
                        <li>マルコフ決定過程の一部が不明で、情報を集めながら意思決定する問題を強化学習問題</li>
                    </ul>
                    という。
                </li>
                <li><b>テーブルマルコフ決定過程</b>とは、行動集合\(A\)と状態集合\(S\)が有限なマルコフ決定過程のことである。</li>
                <li>行動集合と状態集合が有限ならば、遷移確率\(P\)、報酬関数\(r\)、方策\(\pi\)は行列として表現可能である。</li>
            </ul>
            <h3>収益と最適方策</h3>
            <ul>
                <li>「長期的な累積報酬の和が最大になる方策が最適方策」であり、この最適方策を見つけるのが逐次意思決定問題のゴールである。</li>
                <li>ここで、「どれくらい長期的に考えるか」によって定式化が異なる。</li>
                <li>考慮する意思決定の長さが有限の場合は有限マルコフ決定過程と呼び、無限の場合は無限マルコフ決定過程と呼ぶ。</li>
                <li>意思決定の長さが１の強化学習問題のことを、バンディット問題と呼ぶ。</li>
                <li>エピソードとは、意思決定が始まってから終わるまでの１巡のことである。</li>
                <li>ホライゾン\(H \in \mathbb{N}\)とは、エピソード中の意思決定の回数のことである。</li>
                <li>エピソードでの収益は、\(R=\sum_{h=0}{H-1}r_{(h)}\)として定義される。</li>
                <li>期待収益\(\mathbb{E}^{\pi} \in \mathbb{R}\)は、収益Rの期待値である。</li>
                <li>最適方策\(\pi^{*}=arg\max_{\pi} \mathbb{E}^{\pi}[R]\)は、期待収益\(\mathbb{E}^{\pi}\)を最大にする方策のことである。</li>
                <li>無限マルコフ決定過程の場合は、どの場合も期待収益が無限になってしまうので、割引率\(\gamma\)を使って「割引収益」、「期待割引収益」を考える。</li>
                <li>期待割引収益\(\mathbb{E}^{\pi}[R] \leqq \frac{\text{報酬の最大値}}{1-\gamma}\)</li>
            </ul>
            <h3>価値反復法</h3>
            <ul>
                <li>多くの深層強化学習アルゴリズムは価値反復法の拡張である。</li>
                <li>価値反復法は、マルコフ決定の帰納的な性質を利用して最適方策を求めるプランニングアルゴリズムである。</li>
                <li>価値反復法のように、マルコフ決定過程の帰納的な性質を利用して最適方策を求めるアルゴリズムを動的計画法と呼ぶ。</li>
                <li>価値反復法は強化学習アルゴリズムではなく、プランニング問題に対するアルゴリズムである。</li>
                <li>価値反復法は次の価値関数を利用して最適方策を見つける。</li>
                <li>方策\(\pi\)の状態行動価値関数\(Q^{\pi}(s, a) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \mid s_0 = s, a_0 = a \right]\)は、
                状態\(s\)、行動\(a\)から方策\(\pi\)で無限ステップ逐次意思決定したときに期待される割引収益を表す。</li>
                <li>これはQ関数とも呼ばれ、行列で表現可能である。</li>
                <li>最終目標は\(\pi^{*}\)を見つけることである。</li>
                <li>\(\pi^{*}\)の価値関数\(Q^{\pi}^{*}\)が見つかれば、\(\pi^{*}\)も見つかる。</li>
                <li>これは、「\(Q^{\pi}\)を基準とすると、期待収益が必ず向上するように方策\(\pi\)を更新可能である」という原理に基づく。</li>
                <li>\(Q^{\pi}^{*}\)はベルマン方程式\(Q^\pi(s, a) = R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \sum_{a'} \pi(a' \mid s') Q^\pi(s', a')\)を満たすため、このベルマン方程式を満たす\(Q\)を見つければよい。</li>
                <li>したがって、ベルマン誤差\(delta(s, a) = Q^\pi(s, a) - \left[ R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \max_{a'} Q^\pi(s', a') \right]\)が小さくなるようにQ関数を更新すればよい。</li>
                <li>価値反復法はベルマン誤差を小さくするようにQ関数を更新し、収束したQ関数の貪欲方策を出力する。</li>
            </ul>
            <h3>Q学習アルゴリズム</h3>
            <ul>
                <li>価値反復法では、状態遷移確率\(P\)と報酬関数\(r\)が既知のプランニング問題を対象にしていた。</li>
                <li>また、状態空間のサイズが有限だった。</li>
                <li>強化学習では\(r\)や\(P\)の値が未知な場合を扱いい、深層強化学習では状態空間のサイズが無限の場合を扱う。</li>
                <li>価値反復法の更新式\(Q_{k+1}(s, a) = R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \max_{a'} Q_k(s', a')\)における\(\sum_{s'} P(s' \mid s, a)\)がしばしば計算できない。</li>
                <li>そこで、サンプルを使って期待値を近似し、以下のように\(Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R(s, a) + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]\)Q関数を更新する。</li>                
                <li>\(s\), \(a\)が「すべての状態行動\(S \times A\)をカバーする」＆「\alphaが十分小さい」ならば、このサンプル近似による更新は\(Q^{\pi^{*}}\)へ収束する。</li>
            </ul>
            <h3>DQNアルゴリズム</h3>
            <ul>
                <li>ネットワークが行動に対応するQ値を出力する。</li>
                <li>Q学習では、\(S \times A\)マスの行列を用意し、各\((s, a)\)マスに\(Q(s, a)\)の値を格納していた。</li>
                <li>これからは「状態（画像など）を受け取り、行動空間上の値を返すネットワーク」を使ってQ関数を表現する。</li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
        </section>
    </main>
</body>
</html>
