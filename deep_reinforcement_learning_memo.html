<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>走る作曲家のAIカフェ - 深層強化学習（殴り書き版）</title>
    <link rel="stylesheet" href="style.css"> 
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>走る作曲家のAIカフェ</h1>
        <nav>
            <ul>
                <li><a href="index.html">ホーム</a></li>
                <li><a href="study.html">勉強</a></li>
                <li><a href="music.html">音楽</a></li>
                <li><a href="sports.html">スポーツ</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <section>
            <h2>目次</h2>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#source">Source</a></li>
                <li><a href="#mdp-planning">Makov Decision Process and Planning Algorithm</a></li>
                <li><a href="#basic">Basic Deep Reinforcement Learning and The Applications</a></li>
                <li><a href="#cas">Deep Reinforcement Learning in Continuous Action Spaces</a></li>
                <li><a href="#demo">Methods using demonstrations</a></li>
                <li><a href="modelbase">Control as Inference</a></li>
            </ul>
        </section>
        <section id="neural-network">
            <h2>Overview</h2>
            深層強化学習とは
            <ul>
                <li>エージェントが環境との相互作用を通じて試行錯誤し、得られた報酬を最大化するためにディープニューラルネットワークを用いて最適な行動戦略を学習する</li>
            </ul>
            という手法です。このページでは、全くの初学者である私が、深層強化学習についてまとめていきます。
        </section>
        <section id="source">
            <h2>Source</h2>
            以下の講義・書籍を参考にしました。
            <ul>
                <li><a href="https://www.amazon.co.jp/dp/0262039249">Reinforcement Learning, second edition: An Introduction (Adaptive Computation and Machine Learning series)</a></li>
                <ul>
                    <li><a href="http://incompleteideas.net/book/RLbook2020.pdf">Full PDF</a></li>
                </ul>
                <li><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u">Stanford CS234: Reinforcement Learning | Winter 2019</a></li>
                <li><a href="https://www.youtube.com/playlist?list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps">UC Berkeley CS 285: Deep Reinforcement Learning course | Fall 2023</a></li>
                <li><a href="https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNBPJdt8WamRAt4XKc639wF">UC Berkeley CS 287: Advanced Robotics | Fall 2019</a></li>
                <li><a href="https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ">DeepMind x UCL | Introduction to Reinforcement Learning 2015</a></li>
                <li><a href="https://weblab.t.u-tokyo.ac.jp/education/deep-reinforcement-learning/">深層強化学習 Deep Learning 応用講座 2024 Summer</a></li>
                <li><a href="https://www.amazon.co.jp/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%EF%BC%88%E7%AC%AC2%E7%89%88%EF%BC%89-R-Sutton/dp/4627826621/ref=pd_lpo_sccl_2/357-3139727-5663608?pd_rd_w=NkO82&content-id=amzn1.sym.855d8f70-df76-4181-80b0-56e48ae3bb9b&pf_rd_p=855d8f70-df76-4181-80b0-56e48ae3bb9b&pf_rd_r=PMD3GFN7FFG0MYVRX9KR&pd_rd_wg=tHSVD&pd_rd_r=57b87974-355e-4c18-ae1f-bcdf55adb96d&pd_rd_i=4627826621&psc=1#customerReviews">強化学習（第2版）</a></li>
                <li><a href="https://www.amazon.co.jp/%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8BDeep-Learning-%E2%80%95%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E7%B7%A8-%E6%96%8E%E8%97%A4-%E5%BA%B7%E6%AF%85/dp/4873119758/ref=sr_1_3?adgrpid=120952843404&dib=eyJ2IjoiMSJ9.hH2wBl7SFx37GV25E2wHXN5tkG88yeA4w0_uhCDQeAGXHennOFOA9HtC0IZoe1CJ12MDkBW19CKDw3V8W2epn6z8IWch7fF_7Pwyi6zEDUlrbfxmsgNs9jOCWG4xqL5Ukct-erC8712p4NJMeWAkLnC9f4WVhcuWRdquRpwzfwj4z7us0YWOIBZY56XeEUZOZo4nhQB5e-BXeh85Ks22-elRZBG8YkYU53x3nirZCAH7Yd4T3n8nN0MGoXI2EKoztLgR5El8GbWZOVz6wVt01ZaJENrUy8OFCaJItSusR_E.zYa2-YOpfMzxqHg0sL06bGmED9UNuzELMrLgzQ2FWCQ&dib_tag=se&hvadid=665912024283&hvdev=c&hvqmt=b&hvtargid=kwd-851214456388&hydadcr=27494_14701818&jp-ad-ap=0&keywords=%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8Bdeep-learning&qid=1721996873&sr=8-3">ゼロから作るDeep Learning ❹ ―強化学習編</a></li>
                <li><a href="https://www.amazon.co.jp/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E6%A3%AE%E6%9D%91-%E5%93%B2%E9%83%8E/dp/4065155916">強化学習 (機械学習プロフェッショナルシリーズ) </a></li>
                <li><a href="https://www.amazon.co.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-Python%E3%81%A7%E5%AD%A6%E3%81%B6%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E6%94%B9%E8%A8%82%E7%AC%AC2%E7%89%88-%E5%85%A5%E9%96%80%E3%81%8B%E3%82%89%E5%AE%9F%E8%B7%B5%E3%81%BE%E3%81%A7-%E4%B9%85%E4%BF%9D/dp/4065172519">機械学習スタートアップシリーズ Pythonで学ぶ強化学習 [改訂第2版] 入門から実践まで</a></li>
                <li><a href="https://www.amazon.co.jp/%E3%81%A4%E3%81%8F%E3%82%8A%E3%81%AA%E3%81%8C%E3%82%89%E5%AD%A6%E3%81%B6-%E6%B7%B1%E5%B1%A4%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-PyTorch%E3%81%AB%E3%82%88%E3%82%8B%E5%AE%9F%E8%B7%B5%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-%E6%A0%AA%E5%BC%8F%E4%BC%9A%E7%A4%BE%E9%9B%BB%E9%80%9A%E5%9B%BD%E9%9A%9B%E6%83%85%E5%A0%B1%E3%82%B5%E3%83%BC%E3%83%93%E3%82%B9-%E5%B0%8F%E5%B7%9D%E9%9B%84%E5%A4%AA%E9%83%8E/dp/4839965625">つくりながら学ぶ! 深層強化学習 ~PyTorchによる実践プログラミング~</a></li>
            </ul>
        </section>
        <section id="mdp-planning">
            <h2>Makov Decision Process and Planning Algorithm</h2>
            <ul>
                <li>深層強化学習≒強化学習＋関数近似</li>
                <li>強化学習≒プランニング＋サンプル近似</li>
            </ul>
            <h3>強化学習で扱う問題</h3>
            <ul>
                <li>単純な分類・回帰問題と違って、意思決定によって状態が遷移する。</li>
                <li>正解ラベルがない。人の真似をしても、人を超えることはできない。</li>
                <li>与えられた状況に対して、長期的に得する意思決定がしたい。</li>
                <li>強化学習では、<b>逐次意思決定問題</b>を扱う。</li>
                <li>逐次意思決定問題では、エージェントが何らかの環境で意思決定を複数回繰り返す。</li>
                <li>最適な意思決定のルールを見つけたい。</li>
                <li>ここでのアルゴリズムは、「最適な意思決定のルールを見つけるための手順」を指す。</li>
                <li>現実の逐次意思決定問題は非常に複雑である。</li>
                <li>そこで、<b>マルコフ決定過程</b>という数理モデルを使って、シンプルに定式化する。</li>
            </ul>
            <h3>マルコフ決定過程</h3>
            <ul>
                <li>マルコフ決定過程は、次の５つで構成される。
                <ul>
                    <li>行動集合 \(A\) :エージェントが選択可能な行動(意思決定) (\(a \in A\))の集合</li>
                    <li>状態集合 \(S\) :環境の全ての状態 (\(s ∈ S\)) の集合</li>
                    <li>状態遷移確率 \(P: S \times A \rightarrow \Delta (S)\) :ある状態行動から次の状態に遷移する確率</li>
                    <li>報酬関数 \(r: S \times A \rightarrow \mathbb{R}\) :ある状態行動に対する評価</li>
                    <li>初期状態 \(s_0 \in S\) :最初の状態</li>
                </ul>
                </li>
                <li>これに加えて、エージェントを以下のようにモデル化する。
                    <ul>
                        <li>方策 \(\pi: S \rightarrow \Delta (A)\) :ある状態でのエージェントの意思決定が従う確率</li>
                    </ul>
                </li>
                <li>マルコフ決定過程上の逐次意思決定問題のうち、特に、
                    <ul>
                        <li>マルコフ決定過程の情報がすべて既知の場合の意思決定問題を<b>プランニング問題</b></li>
                        <li>マルコフ決定過程の一部が不明で、情報を集めながら意思決定する問題を<b>強化学習問題</b></li>
                    </ul>
                    という。
                </li>
                <li><b>テーブルマルコフ決定過程</b>とは、行動集合\(A\)と状態集合\(S\)が有限なマルコフ決定過程のことである。</li>
                <li>行動集合と状態集合が有限ならば、遷移確率\(P\)、報酬関数\(r\)、方策\(\pi\)は行列として表現可能である。</li>
            </ul>
            <h3>収益と最適方策</h3>
            <ul>
                <li>「長期的な累積報酬の和が最大になる方策が最適方策」であり、この最適方策を見つけるのが逐次意思決定問題のゴールである。</li>
                <li>ここで、「どれくらい長期的に考えるか」によって定式化が異なる。</li>
                <li>考慮する意思決定の長さが有限の場合は<b>有限マルコフ決定過程</b>と呼び、無限の場合は<b>無限マルコフ決定過程</b>と呼ぶ。</li>
                <li>意思決定の長さが１の強化学習問題のことを、バンディット問題と呼ぶ。</li>
                <li>エピソードとは、意思決定が始まってから終わるまでの１巡のことである。</li>
                <li>ホライゾン\(H \in \mathbb{N}\)とは、エピソード中の意思決定の回数のことである。</li>
                <li>エピソードでの収益は、\(R=\sum_{h=0}^{H-1}r_{(h)}\)として定義される。</li>
                <li>期待収益\(\mathbb{E}^{\pi}[R] \in \mathbb{R}\)は、収益\(R\)の期待値である。</li>
                <li>最適方策\(\pi^{*}=arg\max_{\pi} \mathbb{E}^{\pi}[R]\)は、期待収益\(\mathbb{E}^{\pi}[R]\)を最大にする方策のことである。</li>
                <li>無限マルコフ決定過程の場合は、どの場合も期待収益が無限になってしまうので、割引率\(\gamma\)を使って「割引収益」と「期待割引収益」を考える。</li>
                <li>期待割引収益は\(\mathbb{E}^{\pi}[R] \leqq \frac{\text{報酬の最大値}}{1-\gamma}\)となり、有限の値となる。</li>
            </ul>
            <h3>価値反復法</h3>
            <ul>
                <li>多くの深層強化学習アルゴリズムは価値反復法の拡張である。</li>
                <li>価値反復法は、マルコフ決定の帰納的な性質を利用して最適方策を求めるプランニングアルゴリズムである。</li>
                <li>価値反復法のように、マルコフ決定過程の帰納的な性質を利用して最適方策を求めるアルゴリズムを動的計画法と呼ぶ。</li>
                <li>価値反復法は強化学習アルゴリズムではなく、プランニング問題に対するアルゴリズムである。</li>
                <li>価値反復法は次の価値関数を利用して最適方策を見つける。</li>
                <li>方策\(\pi\)の状態行動価値関数
                    <div class="scroll">
                    \begin{align}
                    Q^{\pi}(s, a) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \mid s_0 = s, a_0 = a \right]
                    \end{align}
                    </div>
                    は、状態\(s\)、行動\(a\)から方策\(\pi\)で無限ステップ逐次意思決定したときに期待される割引収益を表す。</li>
                <li>これはQ関数とも呼ばれ、行列で表現可能である。</li>
                <li>最終目標は\(\pi^{*}\)を見つけることである。</li>
                <li>\(\pi^{*}\)の価値関数\(Q^{\pi^{*}}\)が見つかれば、\(\pi^{*}\)も見つかる。</li>
                <li>これは、「\(Q^{\pi}\)を基準とすると、期待収益が必ず向上するように方策\(\pi\)を更新可能である」という原理に基づく。</li>
                <li>\(Q^{\pi^{*}}\)はベルマン方程式
                    <div class="scroll">
                    \begin{align}
                    Q^{\pi^{*}}(s, a) = r(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \max_{a'} Q^{\pi^{*}}(s', a')
                    \end{align}
                    </div>
                    を満たすため、このベルマン方程式を満たす\(Q\)を見つければよい。</li>
                <li>したがって、ベルマン誤差
                    <div class="scroll">
                    \begin{align}
                    \delta(s, a) = Q(s, a) - \{ r(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \max_{a'} Q(s', a') \}
                    \end{align}
                    </div>
                    が小さくなるようにQ関数を更新すればよい。</li>
                <li>価値反復法はベルマン誤差を小さくするようにQ関数を更新し、収束したQ関数の貪欲方策を出力する。</li>
            </ul>
            <h3>Q学習アルゴリズム</h3>
            <ul>
                <li>価値反復法では、状態遷移確率\(P\)と報酬関数\(r\)が既知のプランニング問題を対象にしていた。</li>
                <li>また、状態空間のサイズが有限だった。</li>
                <li>強化学習では\(r\)や\(P\)の値が未知な場合を扱い、深層強化学習では状態空間のサイズが無限の場合を扱う。</li>
                <li>価値反復法の更新式
                    <div class="scroll">
                    \begin{align}
                    Q_{k+1}(s, a) = r(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \max_{a'} Q_k(s', a')
                    \end{align}
                    </div>
                    における\(\sum_{s'} P(s' \mid s, a)\)がしばしば計算できない。</li>
                <li>そこで、サンプルを使って期待値を近似し、
                    <div class="scroll">
                    \begin{align}
                    Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R(s, a) + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
                    \end{align}
                    </div>
                    によってQ関数を更新する。</li>                
                <li>\(s\), \(a\)が「すべての状態行動\(S \times A\)をカバーする」＆「\(\alpha\)が十分小さい」ならば、このサンプル近似による更新は\(Q^{\pi^{*}}\)へ収束する。</li>
            </ul>
            <h3>DQNアルゴリズム</h3>
            <ul>
                <li>ネットワークが行動に対応するQ値を出力する。</li>
                <li>Q学習では、\(S \times A\)マスの行列を用意し、各\((s, a)\)マスに\(Q(s, a)\)の値を格納していた。</li>
                <li>これからは「状態（画像など）を受け取り、行動空間上の値を返すネットワーク」を使ってQ関数を表現する。</li>
                <li><b>DQN(Deep Q-Network)</b>では、ベルマン誤差を小さくするようにネットワーク\(Q_{\theta}\)を更新する。</li>
                <li>ミニバッチ\(D\)を使って、
                    <div class="scroll">
                    \begin{align}
                    \theta \leftarrow arg\min_{\theta} \mathbb{E}_{D}[(Q_{\theta}\text{のベルマン誤差})^2]
                    \end{align}
                    </div>
                と更新すればよい。</li>
                <li>DQNには、性能を上げる工夫がいくつか導入されている。
                <ul>
                    <li>ターゲットネットワークの利用：学習の安定化</li>
                    <li>Experience Replayの利用：過去の経験を蓄積して学習に利用</li>
                </ul></li>
                <li>単純に\(Q_{\theta}\)のベルマン誤差を
                    <div class="scroll">
                    \begin{align}
                    Q_{\theta}(s, a) - (r(s, a) + \gamma \max_{a'} Q_{\theta} (s', a'))
                    \end{align}
                    </div>
                とした場合を考える。</li>
                <li>このとき、DQNの誤差関数は
                    <div class="scroll">
                    \begin{align}
                    \mathbb{E}_{D}[(Q_{\theta}\text{のベルマン誤差})^2] = \mathbb{E}_{D} [Q_{\theta}(s, a) - (r(s, a) + \gamma \max_{a'} Q_{\theta} (s', a'))]
                    \end{align}
                    </div>
                となり、\(r(s, a) + \gamma \max_{a'} Q_{\theta} (s', a')\)が回帰問題における正解ラベルになる。</li>
                <li>この正解ラベルの値は\(\theta\)を更新するたびに変化するため、学習が不安定になってしまう。</li>
                <li>そこで、学習を安定化させる１つ目の工夫として、<b>ターゲットネットワーク</b>がある。</li>
                <li>DQNでは\(Q_{\theta}\)以外にラベル用のネットワーク\(Q_{\overline{\theta}}\)を使って
                    <div class="scroll">
                    \begin{align}
                    \mathbb{E}_{D} [Q_{\theta}(s, a) - (r(s, a) + \gamma \max_{a'} Q_{\overline{\theta}} (s', a'))]
                    \end{align}
                    </div>
                を損失関数にする。</li>
                <li>教師あり学習と同じで、更新ごとに使うデータのミニバッチ\(D\)はなるべく偏らない方が良い。</li>
                <li>更新データに偏りがあると、学習が不安定になってしまう。</li>
                <li>そこで、学習を安定化させる２つ目の工夫として、<b>Experience Replay</b>がある。</li>
                <li>DQNでは過去に経験した\((s, a, r, s')\)のペアを<b>リプレイバッファ</b>と呼ばれるデータセットに蓄積しておき、そこからサンプリングしてミニバッチ学習する。</li>
            </ul>
            <h3>セクションのまとめ</h3>
            <ul>
                <li>深層強化学習は強化学習に関数近似を加えたものであり、強化学習はプランニングとサンプル近似を組み合わせたものである。</li>
                <li>強化学習では、単純な分類・回帰問題と異なり、意思決定によって状態が遷移し、正解ラベルが存在しない中で、長期的に得する意思決定を目指す。</li>
                <li>具体的には逐次意思決定問題を扱い、エージェントが何らかの環境で複数回意思決定を繰り返すことで最適な意思決定のルールを見つけることを目指す。</li>
                <li>現実の逐次意思決定問題は非常に複雑であるため、マルコフ決定過程という数理モデルを用いてシンプルに定式化する。</li>
                <li>マルコフ決定過程は行動集合、状態集合、状態遷移確率、報酬関数、初期状態の5つの要素で構成される。</li>
                <li>さらにエージェントの方策をモデル化し、全ての情報が既知の場合の意思決定問題をプランニング問題、不明な部分がある場合を強化学習問題と呼ぶ。</li>
                <li>テーブルマルコフ決定過程では行動集合と状態集合が有限であり、遷移確率、報酬関数、方策は行列として表現できる。</li>
                <li>長期的な累積報酬の和が最大になる方策が最適方策であり、これを見つけることがゴールである。</li>
                <li>価値反復法はマルコフ決定過程の帰納的性質を利用して最適方策を求めるプランニングアルゴリズムであり、深層強化学習アルゴリズムの基盤となる。</li>
                <li>Q学習アルゴリズムでは、既知の状態遷移確率と報酬関数を利用せず、サンプルを使って期待値を近似し、Q関数を更新する。</li>
                <li>DQNアルゴリズムでは、状態を受け取り行動空間上の値を返すネットワークを用いてQ関数を表現する。</li>
            </ul>
        </section>
        <section id="basic">
            <h2>Basic Deep Reinforcement Learning and The Applications</h2>
            <h3>テーブル形式の強化学習</h3>
            <ul>
                <li>方策学習のため環境内を探索して得た経験{\(s_t, a_t, r_{t+1}, s_{t+1}\)}を使って価値関数を推定する。</li>
                <li>このとき、価値関数は各状態の各行動（ここでは状態行動価値関数を想定）に対して値を設定するため、<b>ルックアップテーブルの関数</b>とも呼ばれる。</li>
                <li>このように推定する価値関数をテーブルで表す手法を<b>テーブル形式の強化学習</b>と呼ぶ。</li>
            </ul>
            <h3>TD学習</h3>
            <ul>
                <li>強化学習で環境内で得た経験データ全てを蓄積して更新に用いるのは効率が悪い。</li>
                <li>ある時間ステップ\(t\)で得た経験{\(s_t, a_t, r_{t+1}, s_{t+1}\)}を用いて価値関数を更新し、方策学習を行なう手法。</li>
                <li>アルゴリズムはTDターゲットの種類で分類される。
                    <ul>
                        <li>Q学習：方策オフ型</li>
                        <li>SARSA：方策オン型</li>
                    </ul>
                </li>
            </ul>
            <h3>テーブル形式学習の限界</h3>
            <ul>
                <li>状態数が多くなりすぎると、すべての状態行動対をテーブル形式で表現するのは難しい。</li>
                <li>考えられる対策：人間が決めたやり方で状態を離散化し、状態空間を小さくする。</li>
                <li>欠点
                    <ul>
                        <li>区切り方は人間が決めるので、重要な状態の情報が落ちる可能性がある。</li>
                        <li>隣り合う状態の類似性が無視されて個別に学習される。</li>
                    </ul>
                </li>
            </ul>
            <h3>関数近似による強化学習</h3>
            <ul>
                <li>状態数が非常に多い課題でテーブル形式のやり方で価値などを管理すると、要素数が多くなりすぎて学習が困難になる。</li>
                <li>入力から特徴抽出して状態の類似性をとらえ、似た状態に対しては似た出力をしてくれるようなパラドックを持つ関数を学習することを考える。</li>
            </ul>
            <h3>深層強化学習</h3>
            <ul>
                <li>現在、画像認識や、機械翻訳の分野で、人を超える性能を発揮している<b>深層ニューラルネットワークによる深層学習</b>を強化学習にも適用する。</li>
                <li>深層ニューラルネットワークの持つ高い関数近似能力によってエージェントの観測から価値関数や方策を実現する。</li>
                <li>他の深層学習手法と同様に、ReLUやシグモイド関数のような活性化関数、損失関数を定義してSGD, Adam, RMSPropなどの最適化、誤差逆伝播による学習を行なう。</li>
                <li>一般的には、<b>DQN(Deep Q-Networks)</b>以降に登場したニューラルネットワークを使った手法全般を指す。</li>
            </ul>
            <h3>Deep Q-Networks (DQN)</h3>
            <ul>
                <li>深層強化学習アルゴリズムの祖といえる手法。</li>
                <li>既に画像認識で使われていた畳み込み処理をニューラルネットのアーキテクチャに採用し、<b>経験再生、ターゲットネットワークの使用、報酬のクリッピング</b>など、様々な工夫により深層ニューラルネットワークを強化学習の関数近似器として使う手法のスタンダードを確立した。</li>
                <li>ゲームの画像をネットワークへの入力とし、Q学習で使っていたQ値（状態行動価値）を適切に出力するようにTD誤差から深層ニューラルネットワークを学習する、Q学習やSARSAと同じ<b>価値ベース</b>の手法。</li>
                <li>ある状態の入力\(s\)を入力したとき、その状態で可能な各行動\(a\)の状態行動価値を出力するような、パラメータ\(\theta\)を持つ深層ニューラルネットワーク\(Q(s, a ; \theta)\)を考える。</li>
                <li>Q学習のQ値の更新式を一般的な深層強化学習で行う<b>パラメータ更新</b>の式に改良し、目的変数（ターゲット）から損失関数を定義して勾配を求め、確率的勾配降下法で学習する。</li>
                <li>元のQ値の更新式は、次のとおり。
                    <div class="scroll">
                        \begin{align}
                        Q^\pi (s_t,a_t)\leftarrow Q^\pi (s_t,a_t)+\alpha(r_{t+1}+\gamma \max_a Q^\pi (s_{t+1},a')-Q^\pi (s_t,a_t))
                        \end{align}
                    </div>
                </li>
                <li>ターゲット（普通の深層学習における正解データ）を以下のように定義する。
                    <div class="scroll">
                        \begin{align}
                        Y_t = r_{t+1}+\gamma \max_a Q(s_{t+1},a';\theta^-)
                        \end{align}
                    </div>
                </li>
                <li>そして、以下の二乗誤差の損失関数\(L(\theta)\)の最小化を行う。
                    <div class="scroll">
                        \begin{align}
                        L(\theta) = \mathbb{E}[(Y_t - Q(s_t,a_t;\theta))^2]
                        \end{align}
                    </div>
                </li>
                <li>あとはこの損失関数を\thetaで微分し、普通のニューラルネットと同じようにパラメータを更新する。
                    <div class="scroll">
                        \begin{align}
                        \nabla_\theta L(\theta) = \mathbb{E}[(Y_t - Q(s_t,a_t;\theta))\nabla_\theta Q(s_t,a_t;\theta)]
                        \end{align}
                    </div>
                </li>
                <li>Q学習の更新式では、状態遷移時に得た報酬\(r_t\)に、遷移先の状態で最も高い値を持つ行動のQ値をテーブルから選択し、ターゲットとした。</li>
                <li>テーブルのない深層強化学習では、次の状態\(s_{t+1}\)を深層ニューラルネットワークに入力し、最大Q値を持つ行動のQ値を選択する。</li>
                <li>欠点：状態行動価値\(Q(s_t,a_t;\theta)\)を求めるのに使う\(\theta\)をターゲットの出力にも使うと、予測値と正解データを同じモデルで出力することになり、学習が安定しない。</li>
                <li>そこで、ターゲットを求めるため、更新前のパラメータ\(\theta^-\)をコピーした<b>ターゲットネットワーク</b>を使用する。</li>
            </ul>
            <h3>DQNの工夫「経験再生（Experience Replay）」</h3>
            <ul>
                <li><b>リプレイバッファ</b>と呼ばれる機構に、環境内を探索して得た複数の経験{\(s_t, a_t, r_{t+1}, s_{t+1}\)}を保存し、深層ニューラルネットワークのバッチ学習を行なう際に、このバッファからランダムに複数の経験を選択してミニバッチ学習（＝経験再生）を行う。</li>
                <li>Q学習やSARSAなどの強化学習では、一度環境内で行動して得た経験を一度だけのQ値の更新に使っていたが、DQNではすぐにQ値の更新に使わず、保存しておいてから後から使用する。</li>
                <li>今までの強化学習における経験の使い方の問題
                    <ul>
                        <li>一般的な確率的勾配降下法(SGD)で前提としているサンプル間の独立性が、時系列的な相関によって壊れる。</li>
                        <li>極めてレアな経験があったとしても、それを後で使えず捨ててしまう。</li>
                    </ul>
                </li>
            </ul>
            <h3>DQNの工夫「畳み込みニューラルネットワーク（CNN）の利用」</h3>
            <ul>
                <li>人間の視覚に着想を得た畳み込み処理の機構をニューラルネットワークに使うことで、画像認識でのブレークスルーのきっかけとなった畳み込みニューラルネットワーク（CNN）を使用する。</li>
                <li>機械にとっては、画像はRGBの数字の羅列だが、CNNにより強化学習の制御に必要な画像上の重要特徴（敵、障害物の位置など）を捉えることができるようになった。</li>
                <li>従来のニューラルネットを使う強化学習では、画像をそのまま入力してもうまくいかず、人が入力する特徴量を設計したり、別の機械学習モデル（オートエンコーダ）で得た低次元特徴を入力して無理やり成功させていた。</li>
            </ul>
            <h3>報酬のクリッピング</h3>
            <ul>
                <li>強化学習の課題となるゲームなどでは、アイテム獲得、敵を倒す、ゴールするなどで得る様々なスコアを報酬とする。</li>
                <li>このスコアの値をそのまま報酬にした場合、課題によって他の学習機構をゲームにあわせて毎回調整する必要が出てくるため、すべての報酬を-1~1の範囲におさめる。</li>
            </ul>
            <h3>Double DQN</h3>
            <ul>
                <li>Q学習やDQNなど、予測値の教師信号であるターゲットを求めて、予測値との誤差を小さくするように学習するアルゴリズムでは、価値の過大評価による学習阻害の問題がある。</li>
                <li>これに対処するため、二つの関数近似器を用いて、「行動選択」と「価値評価」を行う部分に分ける手法（Double Q-Learning）をDQNにも適用する。</li>
                <li>直感的には、２つのネットワークがどちらも価値評価を（同時に）大きく間違える可能性は低いため、仮に片方が価値の過大評価をしたとしても、違うパラメータを持つもう片方が正しい評価をすれば、それを抑制できるということ。</li>
            </ul>
            <h3>Prioritized Experience Replay</h3>
            <ul>
                <li>リプレイバッファに内の経験に優先度をつけ、学習効果が高い経験再生を行う。</li>
                <li>元のDQNでは、経験再生時にリプレイバッファにためた過去の経験からランダムサンプリングして学習する。</li>
                <li>しかし、実際には経験データの中には学習効果が高い経験とそうでない経験が混ざっているはずであり、学習効果が高い経験を優先的に学習させたい。</li>
                <li>Prioritized Experience Replayでは、各経験データに対して、TD誤差が高いもの（＝損失が大きくなるデータ）に対し大きい優先度を割り当てるよう、以下のように優先度\(p_i\)をつける。
                    <div class="scroll">
                        \begin{align}
                        p_i = \|r_t+\gamma \max_a Q_{target} (s_{t+1},a)-Q(s_t,a_t)\|
                        \end{align}
                    </div>
                </li>
                <li>そして、\(i\)番目の経験に対して、優先度が高いものが優先的に選択されるよう、選択確率を以下のように計算する。
                    <div class="scroll">
                        \begin{align}
                        p_i = \frac{p_i^\alpha}{\sum_j p_i^\alpha}
                        \end{align}
                    </div>
                </li>
                <li>優先度をつけて経験をサンプリングすることで、経験データの分布に偏りを与えることによるバイアスが発生する。</li>
                <li>このバイアスにより、学習の収束解に変化が生じてしまうため、バイアスへの対処として、<b>重点サンプリング</b>を導入する。</li>
            </ul>
            <h3>Dueling Network</h3>
            <ul>
                <li>状態行動価値を状態のみに依存する部分とそうでない部分に分離する。</li>
                <li>深層強化学習でネットワークが出力する状態行動価値は、状態のみに依存する部分と、それ以外の行動で決まる部分に分解できると考えられる。</li>
                <li>障害物を避けるゲームで、障害物が全くないところを移動しているときは、どう動いても大して結果は変わらないため、このような時の状態行動価値はほぼ状態に依存する価値で構成したい。</li>
                <li>このような場合、行動で決まる部分を学習してもあまり意味がなく、従来のネットワークの構造では行動の候補数が多いほど無駄な学習が発生してしまう。</li>
                <li>加えて、状態行動価値の絶対値が大きく、各行動の価値の差は小さい場合には、少量のノイズで最大の行動価値が入れ替わってしまう。</li>
                <li>Dueling Networkでは、ネットワークの出力部分を以下のように状態のみに依存する価値\(V(s)\)を出力する部分と、行動で決まる価値\(A(s, a)\)（アドバンテージ）を出力する部分を分けて学習の効率化を図る。
                    <div class="scroll">
                        \begin{align}
                        Q(s, a) = V(s) + A(s, a)
                        \end{align}
                    </div>
                </li>
            </ul>
            <h3>Categorical DQN</h3>
            <ul>
                <li>ただの期待収益ではなく、収益の分布を予測する。</li>
                <li>今までの手法では、ネットワークの予測はただ一つの未来の期待報酬を予測していた。</li>
                <li>しかし、ある行動をしたときにどれくらいの確率でどれくらいの報酬を得られるかを示す分布をモデル化することで、学習に有用な表現を学習できると考えられる。</li>
                <li>従来の研究では、収益を分布で予測することで危険な行動選択を避けるリスク考慮型強化学習という手法で、<b>分布型強化学習</b>として用いられてきた。</li>
            </ul>
            <h3>Noisy Network</h3>
            <ul>
                <li>探索を促進するためネットワークに学習可能なノイズを与え、効率的に探索方向を変える。</li>
                <li>DQNでは各ステップで状態を入力したときの最大価値行動またはε-greedy法のように確率εでランダムな行動を選択して探索し、経験データを集める。</li>
                <li>しかし、このやり方では各行動ステップで独立なランダムの摂動に依存していて、本当に大規模な探索をするには単調すぎる。</li>
                <li>そこで、一貫して探索を偏った方向に変えるため、深層ニューラルネットワークのパラメータそのものに対してノイズを与えることで、出力に対して長期の影響を与える。</li>
                <li>探索時に状態行動価値を出力するネットワークには、各ステップごとにノイズを与える。</li>
                <li>損失の計算時には、ターゲットネットワークにもノイズを加えて出力する。</li>
            </ul>
            <h3>Rainbow</h3>
            <ul>
                <li>すべてのDQN拡張手法を組み合わせた全部混ぜ深層強化学習。</li>
                <li>Double DQN, Prioritized Experience Replay, Dueling Networks, Categorical DQN, Noisy Network, マルチステップ学習を取り入れた。</li>
                <li>マルチステップ学習とPrioritized Experience Replayはオフにするとパフォーマンスが大きく低下するため、貢献が大きい。</li>
                <li>Double DQNやDueling Networkは影響が小さい。</li>
                <li>Noisy NetworkやDistributional (Categorical DQN)をオフにすると、学習前半はあまり影響がないが、後半にパフォーマンスが低下する。</li>
                <li>「全部組み合わせたら最強のアルゴリズムになった」というシンプルな結論。</li>
            </ul>
            <h3>分散型深層強化学習（Distributed Deep Reinforcement Learning）</h3>
            <ul>
                <li>１つの深層ニューラルネットワークに一つのエージェントが対応し、一つの環境から経験を集めるのではなく、各機構を複数計算機に分散させて学習を効率化する。</li>
                <li><b>Gorila (General Reinforcement Learning Architecture)</b></li>
                <ul>
                    <li>DQNの登場後、すぐに考案された最初期の分散型深層強化学習アーキテクチャ。</li>
                    <li>DQNを拡張し、並列環境から経験を集めるActorと、経験から勾配を計算するLearner、パラメータと勾配の情報を保存するParameter Serverに分けて学習する。</li>
                    <li>コンピュータないの計算資源をフルに使い、学習を行なう（損失を計算する）部分と並列で環境から経験を集める部分を分け、それぞれ複数並列実行したら早く学習できる、という発想。</li>
                </ul>
            </ul>
            <h3>A3C (Asynchronous Advantage Actor-Critic)</h3>
            <ul>
                <li>DQNのような価値ベースではなく、方策ベースのActor-Criticを使った分散型強化学習の手法。</li>
                <li>"A3"の意味</li>
                <ul>
                    <li>Asyncronous: 複数エージェント（Actor）を分散させて並列に探索させ、非同期に方策パラメータを更新する。</li>
                    <ul>
                        <li>各環境で並列して動き、特有のパラメータ\(\theta'\)（方策パラメータ）、\(\theta_v '\)（価値パラメータ）を持つエージェントが、自らの環境で集めた経験を使って勾配を計算し、計算した勾配を中央のLearnerに送る。</li>
                        <li>定期的に、中央のLearnerのグローバルパラメータ\(\theta\)を各エージェントのパラメータにコピーする。</li>
                        <li>Experience Replayによる経験サンプルを行わない手法であるため、安定したOn-Policyアルゴリズム（SARSAなど）を使用でき、RNNやLSTMを方策ネットワークとして採用可能である。</li>
                        <li>A3Cの後継手法A2Cでは、このAsyncronous部分が廃止された。</li>
                    </ul>
                    <li>Advantage: 方策勾配を計算する際にベースラインとして状態価値を使ったアドバンテージを使う。</li>
                    <li>Actor-critic: 方策勾配計算のアーキテクチャとしてActor-Criticを採用する。</li>
                </ul>
                <li>Experience Replayを廃止し、各環境で並列に動く各Actorは個別に多様な探索を行う。</li>
            </ul>
            <h3>Ape-X</h3>
            <ul>
                <li>環境から経験を集めるActorと、Actorが集めた経験を蓄積するリプレイバッファ、リプレイバッファからサンプルした経験から勾配を計算して学習するLearner(GPU上)に分けた分散型深層強化学習。</li>
                <li>リプレイバッファからの経験再生には、優先度をつけるPrioritized Experience Replayを使う。</li>
            </ul>
            <h3>R2D2 (Recurrent Replay Distributed DQN)</h3>
            <ul>
                <li>Ape-Xの方策ネットワークにRNN(LSTM)を組み込み、その他様々な工夫を行った分散型深層強化学習の最高パフォーマンスアルゴリズム。</li>
                <li>通常、経験再生を使った深層強化学習ではネットワークの勾配計算時に経験を保存したときのRNNの隠れ状態を再現できないため、RNNを方策ネットワークで表現するのは難しかったが、本手法では経験を得る時の状態の保存(Stored state)と経験再生に経験データをいくつか流して隠れ状態を再現(Burn-in)してこれを解決。</li>
                <li>深層強化学習におけるAtariなどでは、環境内でMDPが実質的に成立していない(POMDP)と仮説を立て、これに対してRNNを使う先行研究DRQN(Deep RecurrentQ-Networks)があったが、あまり成功しなかった。</li>
                <li>経験再生時には、経験を保存したタイミングのRNNの隠れ状態と、再生する時のネットワークの隠れ状態が違うため、学習が困難だった。</li>
                <li>これを回避するため、以下の二つの工夫を提案した。</li>
                <ul>
                    <li>Stored State: 経験を保存する時の隠れ状態も保存し、経験再生時の初期状態とする。</li>
                    <li>Burn-in: 経験再生時に時系列的に連続した経験列を一定時間流し、続く経験を使って損失を計算する。</li>
                </ul>
                <li>今までの経験再生は、経験の時系列的な並びを無視していた。</li>
                <li>R2D2ではBurn-inを行う場合に時系列に沿って経験を入力するため、経験再生時には時系列的に連続した経験列をサンプリングする。</li>
                <li>Burn-inの時には、サンプリングした経験列の最初の経験を得たときに保存されたRNNの隠れ状態をセットし、決められた分だけのBurn-inを行って隠れ状態の更新をした後、損失を計算する。</li>
            </ul>
            <h3>その他の分散型深層強化学習アルゴリズム</h3>
            <ul>
                <li>ACER:A3Cを方策オフ型で経験再生を利用する。</li>
                <li>IMPALA:V-traceと呼ばれる方策オフ化手法を使い、マルチタスク強化学習を行なう。</li>
                <li>UNREAL:複数の補助タスクを導入して、報酬以外の教師信号も利用する。</li>
                <li>R2D3:R2D2をベースにエキスパートのデモデータも効果的に使用し高難度タスク攻略する。</li>
                <li>現在の深層強化学習の最先端アルゴリズム(いわゆるState of the Artなアルゴリズム)はこの分散型深層強化学習がベースになっているものが多い。</li>
            </ul>
            <h3>深層強化学習の新たなパラダイム/ 内発的報酬による探索</h3>
            <ul>
                <li>"Montezuma's Revenge"というゲームがなかなかクリアできなかった。</li>
                <li>攻略できない課題の特徴として、以下のようなものがあった。</li>
                <ul>
                    <li>探索空間が大きすぎ、可能な状態が桁違い。</li>
                    <li>ゲームの特性上、意味のある状態に到達するのが難しく、特殊な状態遷移が必要。</li>
                    <li>上記をまとめて、強化学習に必要な報酬の獲得機会が極めて少ない報酬がスパースな課題。</li>
                </ul>
                <li>報酬がスパースな課題では、報酬の獲得が難しすぎることで価値関数がほとんど更新されず、意味のない探索が繰り返される。</li>
                <li>発想「<b>内発的報酬の導入</b>」</li>
                <ul>
                    <li>今までの強化学習では、報酬は環境から与えられるものだったが、報酬を環境が出してくれないのであれば、勝手に何かの基準で生成すればいいのではないか。</li>
                    <li>この基準は環境の意味のある場所の探索を促進するようなもの。</li>
                </ul>
                <li><b>外発的報酬(Extrinsic Reward)</b></li>
                <ul>
                    <li>環境で本来の目的を達成した時に獲得できる報酬\(R_e\)</li>
                </ul>
                <li><b>内発的報酬(Intrinsic Reward)</b></li>
                <ul>
                    <li>環境内で何らかの基準で、エージェント自身が勝手に生み出す報酬\(R_i\)。探索ボーナスとも。</li>
                </ul>
                <li>最終的に受け取る報酬: \(R - R_e + R_i\)</li>
                <li>普通の強化学習：外的な報酬だけに頼った場合でも、ランダム探索で現実的な時間で学習可能だが、報酬がスパースな場合、ランダム探索ではいつまでたっても報酬に到達しない。</li>
                <li>そこで、何らかの基準で内発的報酬を生成し、探索に方向性、偏りを与える。</li>
                <li>例）行ったことのない状態に到達すると報酬を生成する。</li>
            </ul>
            <h3>内発的報酬の生成方式</h3>
            <ul>
                <li>状態への訪問回数のカウントベース手法</li>
                <li>関節的に状態への訪問回数を計算する擬似カウントベース手法</li>
                <li>予測器の予測誤差を用いた内発的報酬生成</li>
                <li>その他
                    <ul>
                        <li>遷移確率の変化 = 環境に対する情報量の改善とみなして内発的報酬</li>
                        <li>２つのネットワーク間の蒸留と出力予測により内発的報酬生成</li>
                    </ul>
                </li>
                <li>人間が、ある種の探索課題をクリアする場合も内発的報酬を用いた探索に似たようなことをしている。</li>
                <li>欠点
                    <ul>
                        <li>状態数があまりにも多すぎたり、状態空間が連続の場合は、ほどんどの状態カウントがゼロになり、意味をなさない。</li>
                        <li>この性質上、最近の強化学習主要ベンチマークである物体制御タスクやAtariゲームではほとんど意味がない。</li>
                    </ul>
                </li>
                <li>改良
                    <ul>
                        <li>擬似カウントベース手法：一つ一つの状態の訪問回数を直接カウントせず、画像ピクセル単位でみた状態の類似度、発生確率から、擬似的にカウントする。</li>
                        <li>状態のハッシュ化を用いたカウントベース手法：状態をハッシュを用いて変換して抽象化し、似た状態をまとめてカウントする。</li>
                    </ul>
                </li>
            </ul>
            <h3>予測誤差による内発的報酬生成</h3>
            <ul>
                <li>観測\(x_t\)とその時選択する行動\(a_t\)から、次の観測\(x_{t+1}\)がどうなるか予測するモデル\(f(x_t, a_t)\)を考える。</li>
                <li>モデルの出力と実際に\(x_t\)で\(a_t\)を選択した場合の次の観測\(x_{t+1}\)を用いて二乗誤差を計算し、NNを学習する。</li>
                <li>既に観測した遷移は予測精度が高くなり（二乗誤差が小さくなる）、観測が少ない遷移は予測精度が低くなる（二乗誤差が大きい）。</li>
            </ul>
            <h3>NoisyTV problem</h3>
            <ul>
                <li>Unity (ゲームエンジン)で作った強化学習環境に、次々と違う映像が映し出されるテレビを設置し、内発的報酬による探索をさせてみたところ、エージェントの動きが止まってしまった。</li>
                <li>理由：常に観測がランダムに変化することで、予測器の学習が意味をなさず、TVを観測する状態に高い報酬が生成されるため。</li>
                <li>環境内の無意味な情報を無視できる特徴抽出が必要。</li>
            </ul>
            <h3>ICM (Intrinsic Curiosity Module)</h3>
            <ul>
                <li>エージェントの行動に関係があるもののみに注目する特徴抽出器を逆モデルを用いて学習し、予測誤差により内発的報酬を生成する機構をつける。</li>
            </ul>
            <h3>VIME</h3>
            <ul>
                <li>環境に対する情報量の改善 = 好奇心とし、情報量が改善されるような状態遷移に対して多くの内発的報酬を付与する。</li>
                <li>情報量の改善は、状態遷移前後の遷移確率の分布の分布間のKLダイバージェンス。</li>
            </ul>
            <h3>RND (Random Network Distillation)</h3>
            <ul>
                <li>二つのネットワーク、ターゲットネットワーク、予測ネットワークを用意する。</li>
                <li>両ネットワークに、評価したい状態を入力する。</li>
                <li>予測ネットワークはターゲットネットワークの出力をまねるように学習を行ない、両出力の二乗誤差が内発的報酬になる。</li>
                <li>つまり、新しく観測した状態に対しては内発的報酬が大きくなる。</li>
            </ul>
            <h3>世界モデル (World Models)</h3>
            <ul>
                <li>以下の３要素から構成される。
                    <ul>
                        <li>Vモデル：環境から得た観測をVAEで圧縮した潜在表現を得る。</li>
                        <li>Mモデル：Vモデルの潜在変数とCモデルの行動を入力としたMDN-RNNで未来を予測する。</li>
                        <li>Cモデル：V, Mモデルの入力をもとに行動し、環境からの報酬でCMA-ESにより方策を最適化する。</li>
                    </ul>
                </li>
                <li>エージェント内部で世界をモデル化し、空間情報の圧縮とそれをもとに時間方向の予測、つまり世界のシミュレーションを行っていると見なせる。</li>
            </ul>
            </section>
        <section id="cas">
            <h2>Deep Reinforcement Learning in Continuous Action Spaces</h2>
            <h3>よく使われるアルゴリズム</h3>
            <ul>
                <li>DQN, DDPG, SAC, PPO, AlphaGoのどれか、もしくはそれによく似たアルゴリズムが（ほぼ確実に）使用される。</li>
                <li>なぜ色々なアルゴリズムが出てくるのか？</li>
                <li>強化学習の目標は逐次意思決定問題を解くことであり、そして現実には様々な種類の問題が存在する。</li>
                <li>ゲーム、医療、ロボットの制御、工場の管理などが代表例。</li>
                <li>それぞれが特殊な難しさを抱えている。</li>
                <li>ゲームは自由に探索できるが、状態空間が巨大。</li>
                <li>ロボットは行動空間が無限集合な場合が多く、DQNだと対処しづらい。</li>
                <li>医療では探索ができない。工場の管理では方策の性能が保証されなければ、大きな損失が出る。</li>
                <li>様々なアルゴリズムを学ぶと、「特殊な難しさ」を抱えた問題に直面した時に役に立つ。</li>
                <li>DDPG, SAC, PPOなどは、ロボットのように行動空間が無限集合の問題に対処できる。</li>
            </ul>
            <h3>連続値制御と離散値制御</h3>
            <ul>
                <li>離散値制御：行動集合が有限なので\(arg \max_{\pi \in _Pi} \sum_{a \in A} \pi(s, a) Q_k (s, a)\)が簡単に計算可能（貪欲方策）　→　方策更新ステップで近似が必要ない。</li>
                <li>連続値制御：行動集合が無限なので\(arg \max_{\pi \in _Pi} \sum_{a \in A} \pi(s, a) Q_k (s, a)\)の計算が難しい　→　方策ステップを勾配法で近似する</li>
            </ul>
            <h3>DDPG</h3>
            <ul>
                <li>Q学習を連続行動に対応させる。</li>
                <li>Q学習：方策更新を\(\mu_k : S \rightarrow A\)を使った等価な形で書き換えたもの</li>
                <li>DQN：Q学習＋方策評価をNNで近似</li>
                <li>DDPG：Q学習＋方策評価をNNで近似＋方策更新をNNで近似</li>
                <li>DDPGは（他のアルゴリズムと比較すると）学習が不安定であり、あまり性能がよくない。</li>
                <li>その後登場したTD3やSACには安定性を向上させるための様々な工夫が盛り込まれた。</li>
            </ul>
            <h3>TD3: Twin-Delayed DDPG</h3>
            <ul>
                <li>Double Q-learningを導入し、Q値の過大評価を低減</li>
                <li>方策の更新頻度を少なくする（2回に1回）</li>
                <ul>
                    <li>アクターの更新が速すぎるとクリティックが追いつけず、収束性が保証できない場合がある</li>
                </ul>
                <li>TD学習の\(arg \max_a\)にノイズをのせる</li>
                <ul>
                    <li>\(max_a Q(s, a)\)は誤差に弱い。\(\max\)を弱めて過適合を防ぐ。</li>
                </ul>
                <li>TD3は、Q学習＋方策評価をNNで近似＋方策更新をNNで近似＋工夫3つ</li>
            </ul>
            <h3>SAC: Soft Actor-Critic</h3>
            <ul>
                <li>モチベーション：エントロピー正則化RLは誤差に強い。</li>
                <li>やり方：Soft Q学習を連続値行動に対応させたい。</li>
                <li>Soft Q学習を連続値行動に対応させたい。</li>
                <li>Soft Q学習：Q学習＋エントロピー正則化</li>
                <li>方策更新、方策評価の式に方策のエントロピーの項を加える。</li>
                <li>実はエントロピー正則化された\(arg \max\)の解はsoftmax方策と同じ</li>
                <li>行動集合が離散だと簡単に実現可能（Soft-DQN）だが、連続だと難しい。</li>
                <li>SAC: softmax方策を連続行動用に修正する。</li>
                <li>モチベーション：softmax方策を連続分布で近似しよう。</li>
                <li>やり方：KLダイバージェンスの最小化で分布を近づける（勾配法で更新）</li>
                <li>SAC: Q学習＋方策評価をNNで近似＋方策更新をNNで近似</li>
            </ul>
            <h3>マルチステップRLとPPO</h3>
            <ul>
                <li>ホライゾン：エピソードの長さ。割引ありの設定を考えているので、Ｔ＝∞で導出するが、Ｔが有限＆割引なしでも似たような結果がでる。</li>
                <li>軌跡\(\tau = {s_1, a_1, \cdots, s_T, a_T}\)：方策\(\pi\)が辿る状態と行動の軌跡。</li>
                <li>割引訪問頻度\(d_{h}^{\pi} (s) = \sum_{t=1}^{h} \gamma^{t-1} p(s_t = s | \pi)\)：方策\(\pi\)がステップhまでに状態sに訪れる総回数の期待値。</li>
                <li>Q学習、DQNでは、\(s_1, a_1\)についての情報しか使ってない。もう少し未来の情報を使ってもよさそう。</li>
            </ul>
            <h3>貪欲方策は良い方策なのか？</h3>
            <ul>
                <li>貪欲方策の利点は、その計算効率の良さにある。</li>
                <li>欠点：貪欲方策の期待収益は、引数のQの精度次第で大きく変化してしまう。</li>
                <li>1手先だけではなく、数手先まで読めば良さそう。</li>
            </ul>
            <h3>マルチステップ強化学習</h3>
            <ul>
                <li>マルチステップ方策更新とマルチステップ方策評価＋関数近似</li>
            </ul>
            <h3>方策勾配法ベース：REINFORCE</h3>
            <ul>
                <li>REINFORCE: Tステップ方策勾配＆Tステップ方策評価をモンテカルロ近似</li>
                <li>Q学習は1ステップ先しかみない。REINFORCEはTステップ先まですべてみる。（どちらも極端なケース）</li>
                <li>中間のアルゴリズムは？</li>
            </ul>
            <h3>ステップ数はいくつに設定するべき？</h3>
            <ul>
                <li>REINFORCE: Tステップ方策勾配＆Tステップ方策評価をモンテカルロ近似</li>
                <ul>
                    <li>\(h = T\)なので真の期待値を計算できれば最適方策に収束する。しかし、真の期待値計算には無限個のサンプルが必要になってしまう。（勾配のバリアンスが大きい）</li>
                </ul>
                <li>DDPG: Q学習＋方策評価をNNで近似＋方策更新をNNで近似</li>
                <ul>
                    <li>\(h = 1\)なのでQのネットワークは手元にあるのでサンプル近似をせずに方策更新ができる。しかし、Qが間違っている場合は誤った方策に収束してしまう。（勾配のバイアスが大きい）</li>
                </ul>
            </ul>
            <h3>バイアス・バリアンスのトレードオフ</h3>
            <ul>
                <li>適当に\(n\)手先まで見る、と決めてもいいけど、1~Tステップ評価の結果全部を混ぜてもよさそう　→　λ方策評価</li>
            </ul>
            <h3>λ方策評価</h3>
            <ul>
                <li>nが大きいほどバリアンスが大きいので、大きいnの重みは小さくして、\(\lambda \in [0, 1]\)の重みですべての評価を混ぜてみよう、</li>
                <li>\(\lambda \in [0,1]\)でバイアスとバリアンスのトレードオフが調整できる。（\(\lambda= 0\)で1ステップ評価、\(\lambda = 1\)でモンテカルロ評価）</li>
            </ul>
            <h3>GAE: Generalized Advantage Estimation</h3>
            <ul>
                <li>\(Q(s, a)\)の代わりにアドバンテージ関数\(A(s, a) = Q(s, a) - V(s)\)を\(\lambda\)推定しよう。</li>
            </ul>
            <h3>方策勾配と単調性能向上</h3>
            <ul>
                <li>TRPOは実装が面倒くさいので、代わりにPPOが用いられる。</li>
            </ul>
            </section>
        <section id="demo">
            <h2>Methods using demonstrations</h2>
            <h3>なぜデモンストレーションを利用するのか？</h3>
            <ul>
                <li>強化学習の課題：</li>
                <ul>
                    <li>複雑なタスクのそれぞれに対して逐一報酬う\(r_t\)を設計するのは大変</li>
                    <li>何度も環境-エージェント間で相互作用するのが現実的ではない場合がある</li>
                </ul>
                <li>デモンストレーションを何らかの方策（他の機械学習エージェント、人間による操作、ヒューリスティックなど）で収集</li>
                <li>報酬の設計をせずに複雑なタスクをデモデータによる教示で学習させる（模倣学習）</li>
                <li>環境-エージェント間の相互作用なしで強化学習を行なう（オフライン強化学習）</li>
            </ul>
            <h3>デモを利用した強化学習：OT-Opt</h3>
            <ul>
                <li>画像入力で、未知物体のGraspingを学習するOT-Optを提案</li>
                <li>Graspingは、ランダムに行動した場合、成功することは極めて稀</li>
                <li>→ある程度（15~30%）成功するようなデモを取れる方策からデータ収集</li>
                <li>58万回の試行データを事前に集めてOff-Policy RLで学習する</li>
                <ul>
                    <li>Off-Policy: データ収集に用いた方策と、更新時の方策が異なる</li>
                    <li>On-Policy: 現在の方策でデータを集めて、方策や価値関数を更新</li>
                </ul>
            </ul>
            <h3>デモを利用した大規模な模倣学習：RT-1</h3>
            <ul>
                <li>画像と言語指示を入力に、様々なGraspingタスクを大規模なデモンストレーションの模倣学習で学習する</li>
                <li>Robotics Transformer 1の</li>
            </ul>
            <h3>模倣学習のアプローチ</h3>
            <ul>
                <li>大きくわけて二つのアプローチがある。</li>
                <ul>
                    <li>Behavior Cloning: デモデータの行動をそのまま真似るように方策を学習する。教師あり学習の問題になる。</li>
                    <li>逆強化学習：デモデータで選択された行動の裏側にある報酬関数を推定する。推定した報酬関数を用いた強化学習で方策を学習する。深層学習以降は、敵対的学習を用いて、陽に報酬関数を推定せずに強化学習を行なう手法が主流。</li>
                </ul>
            </ul>
            <h3>Behavior Cloning</h3>
            <ul>
                <li>デモデータに含まれる、状態と行動について、方策を教師あり学習。</li>
            </ul>
            <h3>Distribution Shiftの問題</h3>
            <ul>
                <li>Behavior Clonningは簡単な手法だが、特にデモデータが少ない場合には、パラメータの過剰適合などによりパフォーマンスが出ない場合がある。</li>
            </ul>
            <h3>逆強化学習</h3>
            <ul>
                <li>デモデータから報酬関数を推定し、その報酬関数を用いて強化学習する</li>
                <ul>
                    <li>逆強化学習は解が一意に定まらない問題</li>
                    <li>多くの報酬関数が同じ行動で説明できる</li>
                </ul>
                <li>手順は以下のとおり。</li>
                <ol>
                    <li>最適方策からサンプル</li>
                    <li>報酬関数を学習</li>
                    <li>その報酬関数のもとで方策を学習</li>
                </ol>
                <li>報酬関数が一意に定まらない中で、どのように強化学習するのが望ましいか</li>
                <li>デモデータから推定できる軌道の分布全体をカバーするような方策が学習できると望ましい</li>
                <li>軌道の模倣ではなく、軌道の分布の模倣と考えるとよい</li>
                <li>方策から生じる軌道は、デモデータのサポートの中で、できる限り多様な方が望ましい</li>
                <li>→方策から生じる軌道の分布のエントロピーを最大化する</li>
                <li>強化学習の文脈のMaximum Entropy RLをもとに考える。</li>
            </ul>
            <h3>Max-Ent RL</h3>
            <ul>
                <li>方策から生成される軌道の分布、報酬関数から定義されるターゲット分布を考えると、Max-Ent RLの目的関数はKL Divergenceの最小化とみなせる</li>
                <li>方策から生じる軌道の分布のエントロピーを最大化する逆強化学習</li>
            </ul>
            <h3>Generative Adversarial Networks</h3>
            <ul>
                <li>Generator（生成器）が潜在変数から画像などを生成するように学習</li>
                <li>Discriminator（識別器）は学習データと生成されたデータを分類するように学習</li>
                <li>逆強化学習はGANとして解釈できる</li>
            </ul>
            <h3>模倣学習　まとめ</h3>
            <ul>
                <li>模倣学習にはBehavior CloningとInverse RLがある。</li>
                <li>Behavior Cloningは教師あり学習</li>
                <li>学習した方策を実行て新たにデータを加えるDAggerは、BCのDistiribution Shiftの問題を抑えることができる</li>
                <li>Inverse RLはデモから報酬関数を推定する問題設定</li>
                <li>深層学習以降では、敵対的強化学習を用いて陽に報酬関数を推定せずに強化学習を行なう手法が主流</li>
                <li>エントロピー最大化の観点からIRLを考えると、敵対的学習(GAN)との共通点が見える。</li>
            </ul>
            <h3>オフライン強化学習</h3>
            <ul>
                <li>何らかの方策で収集されたデモデータから、環境-エージェント間の相互なしで強化学習を行なう。</li>
                <li>模倣学習では、環境-エージェント間の相互作用を利用するものも多かった。</li>
                <li>ロボット学習、医療、対話エージェントなど、現実世界の応用には必要不可欠。</li>
                <li>環境-エージェント間の相互作用コストが高い分野。</li>
                <li>On-Policy: 学習方策でデータを収集し、そのデータで方策を更新する。</li>
                <li>Off-Policy: 学習方策でデータを収集し、これまでに集めたデータで方策を更新する</li>
                <li>Off: 任意の挙動方策で事前にデータを収集し、そのデータで方策を更新する</li>
            </ul>
            <h3>Off-Policy Evaluation</h3>
            <ul>
                <li>与えられた方策の価値関数の値を、環境で実行せずに推定する問題設定</li>
                <li>オフライン強化学習の文脈では、どの方策が良いか選ぶModel Selectionへの活用が考えられる。</li>
            </ul>
            <h3>オフライン強化学習におけるDistribution Shift</h3>
            <ul>
                <li>オフライン強化学習では、挙動方策によるデータセットの分布と学習方策による行動の分布が学習中に離れてしまう。</li>
                <li>→Distribution Shiftと呼ばれる問題</li>
            </ul>
            <h3>Q関数の過大評価</h3>
            <ul>
                <li>Q学習ではターゲットの計算の際に最大値を取るが、学習途中の不正確なQ関数では、正しい最大値が取れない可能性がある。</li>
                <li>オンライン強化学習では探索が促され、新たなサンプルにより、過大評価は解消されうる</li>
                <li>オフライン強化学習ではデータセット外のQ関数はいくら学習しても不正確なまま</li>
            </ul>
            <h3>オフライン強化学習のアプローチ</h3>
            <ul>
                <li>Distiributional ShiftによるQ関数の過大評価にどのように対応するか?</li>
                <li>大きく分けて３つのアプローチがある。</li>
                <li>①学習方策に制約をかける</li>
                <li>②価値関数に制約をかける</li>
                <li>③価値関数を不確実性で重みづけする</li>
            </ul>
            <h3>Distribution Matching</h3>
            <ul>
                <li>挙動方策をBCで推定し、推定した挙動方策からのサンプリングを学習方策として用いる</li>
                <li>Distributional Shiftを大幅に抑えることができる</li>
                <li>挙動方策の性能が悪い場合（ランダム方策など）、学習方策も悪くなってしまう。</li>
            </ul>
            <h3>BCQ: Batch-Constrained deep Q-Learning</h3>
            <ul>
                <li>方策が出力する行動がデータセットから大きく外れないようにする</li>
                <li>Conditional VAEでBC</li>
                <li>CVAEのデコーダー（状態で条件付け）からn個行動をサンプルし、摂動を加えた行動からQ関数が最大になるものを選択する</li>
                <li>摂動でQ関数のサポートを広げ、価値の高い行動を探索</li>
                <li>DDPGと同様な方策勾配法で摂動モデルを学習</li>
                <li>行動に摂動を加えてQ関数の学習に利用</li>
                <li>Q関数の学習を安定化させる。</li>
                <li>Clipped Double Q-Learningのｙ変形版を用いる。</li>
                <li></li>
            </ul>
            <h3>Support Matching</h3>
            <ul>
                <li>挙動方策をBCで推定し、学習方策を学習する際の正則化項として用いる</li>
                <li>Distribution matchingと比べて制約は緩い</li>
            </ul>
            <h3>学習方策に制約をかける手法の問題点</h3>
            <ul>
                <li>挙動方策の推定が必要である。</li>
                <li>挙動方策の推定がうまくいかないと全く性能がでない</li>
                <li>制約が保守的すぎる場合がある</li>
                <li>例えばどんな行動を取っても報酬が0となるような状態にいることを考えると、学習方策に制約をかける必要がない。</li>
            </ul>
            <h3>CQL: Conservative Q-Learning</h3>
            <ul>
                <li>真の価値関数の値の下界となるように目的関数を構成し、Q関数の過大評価を抑える。</li>
                <li>SACやDQNQ関数の目的関数をわずかに修正するだけ</li>
                <li>パフォーマンス：BCや学習方策に直接制約をかける手法と比べて圧倒的に良い。</li>
            </ul>
            <h3>Implicit Q-Learning</h3>
            <ul>
                <li>オフライン強化学習でQ関数に過大評価の影響がみられるのは、データセット外の行動を用いてTD学習するため</li>
                <li>データセット外の行動を用いずにQ関数を学習したい→SARSAの目的関数を利用</li>
                <li>複雑なタスクでは性能がでない。</li>
                <li>挙動方策の期待収益であって、最適な期待収益ではないため</li>
                <li>次の目的関数で価値関数を学習することを目指す。</li>
                <li>Expectile Regression（分位点回帰に類似した手法）を用いて状態価値関数を学習</li>
            </ul>
            <h3>LLMに着想を得た研究</h3>
            <ul>
                <li>大規模言語モデルが様々な分野で成功を収めている　→　強化学習にも同様の考え方が適用できないか？</li>
                <li>強化学習にTransformerを取り入れる</li>
                <li>Internet-scaleの大規模データを用いて方策の事前学習をする</li>
            </ul>
            <h3>Dicision Transformer</h3>
            <ul>
                <li>Decoder-onlyのTransformerを用いて条件付き模倣学習を行なう</li>
                <li>GPT-2のアーキテクチャを利用</li>
                <li>過去Tステップのreturn-to-go（達成すべき累積報酬の目標値）、状態、行動を系列データとみなす。</li>
                <li>各ステップの行動を予測して教師あり学習</li>
                <li>軌道の時刻tをposition embeddingのように扱う。</li>
                <li>(R,s,a,R,s,a,...)の順番に並べてCausal Transformer（自身より前ステップのトークンにしかアテンションを掛けないTransformer）に入力する</li>
                <li>BCやTD学習と比較しても遜色ない性能</li>
            </ul>
            <h3>拡散モデルを用いた研究</h3>
            <ul>
                <li>拡散モデルの高い表現力により、画像生成タスクをはじめとした様々な生成タスクで成功を収めている。→強化学習においては連続値行動の方策関数として利用可能</li>
                <li>拡散モデルを使用する利点としては、様々な条件付き生成を容易に実現できることや、表現力が高いためデータの多様性を捉えることができることなどがある。</li>
            </ul>
            <h3>オフライン強化学習　まとめ</h3>
            <ul>
                <li>オフライン強化学習は強化学習と実際の応用をつなぐ可能性がある研究分野。</li>
                <li>大きくわけて３つのアプローチがある</li>
                <ul>
                    <li>学習方策に直接制約をかける(Policy constraint methods) ← 簡単</li>
                    <li>価値関数の下界を用いる (Learning lower-bounded policy-values) ← 性能がいい</li>
                    <li>価値関数を不確実性で重み付けする (Uncertainty-based methods)</li>
                </ul>
                <li>Large Language Models(GPT-3, PaLM, ChatGPTなど)の成功から着想を得た研究</li>
                <ul>
                    <li>Decoder-onlyのTransformerを用いて条件付き模倣学習</li>
                    <li>Internet-scaleの学習データを用いた事前学習</li>
                </ul>
                <li>拡散モデルを用いた研究</li>
            </ul>
        </section>
        <section id="modelbase">
            <h2>Control as Inference</h2>
            <h3>強化学習</h3>
            <ul>
                <li>問題設定：エージェントは状態\(s_t\)を受け取り、方策\(\pi\)に基づいて行動\(a_t\)を選択し、次の状態\(s_{t+1}\)と報酬\(r(s_t, a_t)\)を受け取る。状態はマルコフ決定過程に従う（直前の状態と行動のみに依存）。</li>
                <li>目的：収益\(\sum_{t=1}^{\infty}r(s_t, a_t)\)を最大化する方策\(\pi\)の学習</li>
                <li>行動価値関数：状態\(s_t\)において行動\(a_t\)を取り、それ以降は方策\(\pi\)に従って行動した場合に得られる収益の期待値</li>
                <li>最適行動価値関数：状態\(s_t\)において行動\(a_t\)を取り、それ以降は最適な方策\(\pi\)を取り続けた場合に得られる収益の期待値</li>
                <li>状態価値関数：状態\(s_t\)から常に方策\(\pi\)に従って行動し続けた場合に得られる収益の期待値</li>
                <li>最適状態価値関数：状態\(s_t\)から常に最適な方策に従って行動し続けた場合に得られる収益の期待値</li>
                <li>ベルマン方程式：価値関数が満たす方程式</li>
                <li>ベルマン最適方程式：最適価値関数が満たす方程式</li>
                <li>Q学習：各状態と行動のペアの価値を記録するテーブルを用意。訪れた状態行動の価値をベルマン最適方程式に基づいて更新。方策は価値関数を最大化する行動を選択。</li>
                <li>Q学習＋関数近似：状態空間が膨大な場合（状態が連続値など）、テーブル法は使えない。代わりに、状態と行動を入力に価値を出力する関数\(Q_{\theta}\)を最適化して近似する。深層強化学習では、\(Q_{\theta}\)にDNNを用いる。</li>
                <li>方策勾配法：状態から行動を選ぶ方策の分布\(\pi_{\varphi}(a|s)\)を用意。収益を最大化する方向に方策のパラメータ\(\theta\)を更新</li>
                <li>Actor-Critic：行動価値関数と方策を両方推定する。</li>
                <li>挙動方策：学習データを得るのに用いた方策</li>
                <li>推定方策：学習対象の方策</li>
                <li>方策オン：挙動方策と推定方策が同じ（方策勾配法など）</li>
                <li>方策オフ：挙動方策と推定方策が異なる（Q学習など）</li>
            </ul>
            <h3>最大エントロピー強化学習</h3>
            <ul>
                <li>方策にエントロピー正則化をつけた強化学習手法</li>
                <li>エージェントは収益を最大化しながら、なるべく多様な行動を選択するように方策を学習する。</li>
                <li>性能が向上する、探索が促される、方策がロバストになるなどの利点から近年注目されている。</li>
                <li>Soft Acotr-Critic：通常のActor-Criticはon-policy。違う方策で集めたデータは使えない。これに対して、SACは挙動方策に依存しない　→　SACはoff-policy</li>
            </ul>
            <h3>Control as Inference</h3>
            <ul>
                <li>最適軌道と最適方策：最適性変数を用いると、強化学習は2通りの定式化ができる</li>
                <ol>
                    <li>最適軌道の推論：常に最適な行動を取り続けるような軌道の事後分布</li>
                    <li>最適方策の推論：現在以降の行動がすべて最適であるという条件下での現在の行動の事後分布</li>
                </ol>
                <li>強化学習とベイズ統計：強化学習で統計モデルに相当するもの　→　マルコフ決定過程に最適性変数を加えたもの</li>
                <li>強化学習で推論する事後分布は何か　→　最適軌道 or 最適方策</li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
            <h3></h3>
            <ul>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
                <li></li>
            </ul>
        </section>
    </main>
</body>
</html>
